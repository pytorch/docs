python.builtin
==================
dynamic_shape_round
^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`python.builtin <python.builtin>`, :doc:`torch.dynamic-shape <torch.dynamic-shape>`

    Support Level: NOT_SUPPORTED_YET

Original source code:

.. code-block:: python

    # mypy: allow-untyped-defs
    import torch
    
    from torch._export.db.case import SupportLevel
    from torch.export import Dim
    
    class DynamicShapeRound(torch.nn.Module):
        """
        Calling round on dynamic shapes is not supported.
        """
    
        def forward(self, x):
            return x[: round(x.shape[0] / 2)]
    
    x = torch.randn(3, 2)
    dim0_x = Dim("dim0_x")
    example_inputs = (x,)
    tags = {"torch.dynamic-shape", "python.builtin"}
    support_level = SupportLevel.NOT_SUPPORTED_YET
    dynamic_shapes = {"x": {0: dim0_x}}
    model = DynamicShapeRound()
    

Result:

.. code-block::

    AssertionError: RoundToInt(IntTrueDiv(dim0_x, 2)) <= dim0_x


tensor_setattr
^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`python.builtin <python.builtin>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    # mypy: allow-untyped-defs
    import torch
    
    
    class TensorSetattr(torch.nn.Module):
        """
        setattr() call onto tensors is not supported.
        """
        def forward(self, x, attr):
            setattr(x, attr, torch.randn(3, 2))
            return x + 4
    
    example_inputs = (torch.randn(3, 2), "attr")
    tags = {"python.builtin"}
    model = TensorSetattr()
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m", attr):
                 [2m# File: /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_export/db/examples/tensor_setattr.py:11 in forward, code: return x + 4[0m
                add: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m" = torch.ops.aten.add.Tensor(x, [34m4[0m);  [2mx = None[0m
                return (add,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=ConstantArgument(name='attr', value='attr'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None)])
    Range constraints: {}
    


type_reflection_method
^^^^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`python.builtin <python.builtin>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    # mypy: allow-untyped-defs
    import torch
    
    class A:
        @classmethod
        def func(cls, x):
            return 1 + x
    
    class TypeReflectionMethod(torch.nn.Module):
        """
        type() calls on custom objects followed by attribute accesses are not allowed
        due to its overly dynamic nature.
        """
    
        def forward(self, x):
            a = A()
            return type(a).func(x)
    
    
    example_inputs = (torch.randn(3, 4),)
    tags = {"python.builtin"}
    model = TypeReflectionMethod()
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "[31mf32[0m[34m[3, 4][0m[2m[34m[0m[2m[32m[0m"):
                 [2m# File: /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_export/db/examples/type_reflection_method.py:7 in func, code: return 1 + x[0m
                add: "[31mf32[0m[34m[3, 4][0m[2m[34m[0m[2m[32m[0m" = torch.ops.aten.add.Tensor(x, [34m1[0m);  [2mx = None[0m
                return (add,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None)])
    Range constraints: {}
    
