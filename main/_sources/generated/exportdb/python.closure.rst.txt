python.closure
==================
cond_closed_over_variable
^^^^^^^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`torch.cond <torch.cond>`, :doc:`python.closure <python.closure>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    # mypy: allow-untyped-defs
    import torch
    
    from functorch.experimental.control_flow import cond
    
    
    class CondClosedOverVariable(torch.nn.Module):
        """
        torch.cond() supports branches closed over arbitrary variables.
        """
    
        def forward(self, pred, x):
            def true_fn(val):
                return x * 2
    
            def false_fn(val):
                return x - 2
    
            return cond(pred, true_fn, false_fn, [x + 1])
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, pred: "[31mb8[0m[34m[][0m[2m[34m[0m[2m[32m[0m", x: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m"):
                 [2m# File: /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_higher_order_ops/cond.py:108 in cond, code: return cond_op(pred, true_fn, false_fn, operands)[0m
                true_graph_0 = self.true_graph_0
                false_graph_0 = self.false_graph_0
                conditional = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, [34m[x][0m);  [2mpred = true_graph_0 = false_graph_0 = x = None[0m
                getitem: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m" = conditional[[34m0[0m];  [2mconditional = None[0m
                return (getitem,)
                
            class <lambda>(torch.nn.Module):
                def forward(self, x: "f32[3, 2]"):
                             mul: "f32[3, 2]" = torch.ops.aten.mul.Tensor(x, 2);  x = None
                    return (mul,)
                    
            class <lambda>(torch.nn.Module):
                def forward(self, x: "f32[3, 2]"):
                             sub: "f32[3, 2]" = torch.ops.aten.sub.Tensor(x, 2);  x = None
                    return (sub,)
                    
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='pred'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='getitem'), target=None)])
    Range constraints: {}
    


nested_function
^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`python.closure <python.closure>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    # mypy: allow-untyped-defs
    import torch
    
    
    
    class NestedFunction(torch.nn.Module):
        """
        Nested functions are traced through. Side effects on global captures
        are not supported though.
        """
        def __init__(self):
            super().__init__()
    
        def forward(self, a, b):
            x = a + b
            z = a - b
    
            def closure(y):
                nonlocal x
                x += 1
                return x * y + z
    
            return closure(x)
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, a: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m", b: "[31mf32[0m[34m[2][0m[2m[34m[0m[2m[32m[0m"):
                 [2m# File: /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_export/db/examples/nested_function.py:20 in forward, code: x = a + b[0m
                add: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m" = torch.ops.aten.add.Tensor(a, b)
                
                 [2m# File: /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_export/db/examples/nested_function.py:21 in forward, code: z = a - b[0m
                sub: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m" = torch.ops.aten.sub.Tensor(a, b);  [2ma = b = None[0m
                
                 [2m# File: /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_export/db/examples/nested_function.py:25 in closure, code: x += 1[0m
                add_1: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m" = torch.ops.aten.add.Tensor(add, [34m1[0m);  [2madd = None[0m
                
                 [2m# File: /opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/_export/db/examples/nested_function.py:26 in closure, code: return x * y + z[0m
                mul: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m" = torch.ops.aten.mul.Tensor(add_1, add_1);  [2madd_1 = None[0m
                add_2: "[31mf32[0m[34m[3, 2][0m[2m[34m[0m[2m[32m[0m" = torch.ops.aten.add.Tensor(mul, sub);  [2mmul = sub = None[0m
                return (add_2,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='a'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='b'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_2'), target=None)])
    Range constraints: {}
    
