torch.escape-hatch
======================
assume_constant_result
^^^^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`torch.escape-hatch <torch.escape-hatch>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    import torch
    import torch._dynamo as torchdynamo
    
    
    
    class AssumeConstantResult(torch.nn.Module):
        """
        Applying `assume_constant_result` decorator to burn make non-tracable code as constant.
        """
    
        def __init__(self):
            super().__init__()
    
        @torchdynamo.assume_constant_result
        def get_item(self, y):
            return y.int().item()
    
        def forward(self, x, y):
            return x[: self.get_item(y)]
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[3, 2]", y: "i64[]"):
                    slice_1: "f32[3, 2]" = torch.ops.aten.slice.Tensor(x, 0, 0, 4);  x = None
                return (slice_1,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='slice_1'), target=None)])
    Range constraints: {}
    


constrain_as_size_example
^^^^^^^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`torch.dynamic-value <torch.dynamic-value>`, :doc:`torch.escape-hatch <torch.escape-hatch>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    import torch
    
    
    
    class ConstrainAsSizeExample(torch.nn.Module):
        """
        If the value is not known at tracing time, you can provide hint so that we
        can trace further. Please look at torch._check and torch._check_is_size APIs.
        torch._check_is_size is used for values that NEED to be used for constructing
        tensor.
        """
    
        def __init__(self):
            super().__init__()
    
        def forward(self, x):
            a = x.item()
            torch._check_is_size(a)
            torch._check(a <= 5)
            return torch.zeros((a, 5))
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "i64[]"):
                    _local_scalar_dense: "Sym(u0)" = torch.ops.aten._local_scalar_dense.default(x);  x = None
                
                    ge_1: "Sym(u0 >= 0)" = _local_scalar_dense >= 0
                scalar_tensor_default: "f32[]" = torch.ops.aten.scalar_tensor.default(ge_1);  ge_1 = None
                _assert_async_msg = torch.ops.aten._assert_async.msg(scalar_tensor_default, '_local_scalar_dense is outside of inline constraint [0, 5].');  scalar_tensor_default = None
                le_3: "Sym(u0 <= 0)" = _local_scalar_dense <= 5
                scalar_tensor_default_1: "f32[]" = torch.ops.aten.scalar_tensor.default(le_3);  le_3 = None
                _assert_async_msg_1 = torch.ops.aten._assert_async.msg(scalar_tensor_default_1, '_local_scalar_dense is outside of inline constraint [0, 5].');  scalar_tensor_default_1 = None
                
                    ge: "Sym(u0 >= 0)" = _local_scalar_dense >= 0
                _assert_scalar = torch.ops.aten._assert_scalar.default(ge, 'Runtime assertion failed for item >= 0');  ge = None
                le: "Sym(u0 <= 5)" = _local_scalar_dense <= 5
                _assert_scalar_1 = torch.ops.aten._assert_scalar.default(le, 'Runtime assertion failed for item <= 5');  le = None
                mul: "Sym(-u0)" = -1 * _local_scalar_dense
                le_1: "Sym(-u0 <= 0)" = mul <= 0;  mul = None
                _assert_scalar_2 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression -u0 <= 0 on node 'le_2'\nMore context: %mul : [num_users=1] = call_function[target=operator.mul](args = (-1, %item), kwargs = {})\n%le_2 : [num_users=0] = call_function[target=operator.le](args = (%mul, 0), kwargs = {})");  le_1 = None
                le_2: "Sym(u0 <= 5)" = _local_scalar_dense <= 5
                _assert_scalar_3 = torch.ops.aten._assert_scalar.default(le_2, "Runtime assertion failed for expression u0 <= 5 on node 'le_3'\nMore context: %_assert_scalar_default_2 : [num_users=0] = call_function[target=torch.ops.aten._assert_scalar.default](args = (%le_2, Runtime assertion failed for expression -u0 <= 0 on node 'le_2'\nMore context: %mul : [num_users=1] = call_function[target=operator.mul](args = (-1, %item), kwargs = {})\n%le_2 : [num_users=0] = call_function[target=operator.le](args = (%mul, 0), kwargs = {})), kwargs = {})\n%le_3 : [num_users=0] = call_function[target=operator.le](args = (%item, 5), kwargs = {})");  le_2 = None
                
                    zeros: "f32[u0, 5]" = torch.ops.aten.zeros.default([_local_scalar_dense, 5], device = device(type='cpu'), pin_memory = False);  _local_scalar_dense = None
                return (zeros,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='zeros'), target=None)])
    Range constraints: {u0: ValueRanges(lower=0, upper=5, is_bool=False), u1: ValueRanges(lower=0, upper=5, is_bool=False), u2: ValueRanges(lower=0, upper=5, is_bool=False), u3: ValueRanges(lower=0, upper=5, is_bool=False)}
    


constrain_as_value_example
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`torch.dynamic-value <torch.dynamic-value>`, :doc:`torch.escape-hatch <torch.escape-hatch>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    import torch
    
    
    
    class ConstrainAsValueExample(torch.nn.Module):
        """
        If the value is not known at tracing time, you can provide hint so that we
        can trace further. Please look at torch._check and torch._check_is_size APIs.
        torch._check is used for values that don't need to be used for constructing
        tensor.
        """
    
        def __init__(self):
            super().__init__()
    
        def forward(self, x, y):
            a = x.item()
            torch._check(a >= 0)
            torch._check(a <= 5)
    
            if a < 6:
                return y.sin()
            return y.cos()
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "i64[]", y: "f32[5, 5]"):
                    _local_scalar_dense: "Sym(u0)" = torch.ops.aten._local_scalar_dense.default(x);  x = None
                
                    ge_1: "Sym(u0 >= 0)" = _local_scalar_dense >= 0
                scalar_tensor_default: "f32[]" = torch.ops.aten.scalar_tensor.default(ge_1);  ge_1 = None
                _assert_async_msg = torch.ops.aten._assert_async.msg(scalar_tensor_default, '_local_scalar_dense is outside of inline constraint [0, 5].');  scalar_tensor_default = None
                le_3: "Sym(u0 <= 0)" = _local_scalar_dense <= 5
                scalar_tensor_default_1: "f32[]" = torch.ops.aten.scalar_tensor.default(le_3);  le_3 = None
                _assert_async_msg_1 = torch.ops.aten._assert_async.msg(scalar_tensor_default_1, '_local_scalar_dense is outside of inline constraint [0, 5].');  scalar_tensor_default_1 = None
                
                    ge: "Sym(u0 >= 0)" = _local_scalar_dense >= 0
                _assert_scalar = torch.ops.aten._assert_scalar.default(ge, 'Runtime assertion failed for item >= 0');  ge = None
                le: "Sym(u0 <= 5)" = _local_scalar_dense <= 5
                _assert_scalar_1 = torch.ops.aten._assert_scalar.default(le, 'Runtime assertion failed for item <= 5');  le = None
                mul: "Sym(-u0)" = -1 * _local_scalar_dense
                le_1: "Sym(-u0 <= 0)" = mul <= 0;  mul = None
                _assert_scalar_2 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression -u0 <= 0 on node 'le_2'\nMore context: %mul : [num_users=1] = call_function[target=operator.mul](args = (-1, %item), kwargs = {})\n%le_2 : [num_users=0] = call_function[target=operator.le](args = (%mul, 0), kwargs = {})");  le_1 = None
                le_2: "Sym(u0 <= 5)" = _local_scalar_dense <= 5;  _local_scalar_dense = None
                _assert_scalar_3 = torch.ops.aten._assert_scalar.default(le_2, "Runtime assertion failed for expression u0 <= 5 on node 'le_3'\nMore context: %_assert_scalar_default_2 : [num_users=0] = call_function[target=torch.ops.aten._assert_scalar.default](args = (%le_2, Runtime assertion failed for expression -u0 <= 0 on node 'le_2'\nMore context: %mul : [num_users=1] = call_function[target=operator.mul](args = (-1, %item), kwargs = {})\n%le_2 : [num_users=0] = call_function[target=operator.le](args = (%mul, 0), kwargs = {})), kwargs = {})\n%le_3 : [num_users=0] = call_function[target=operator.le](args = (%item, 5), kwargs = {})");  le_2 = None
                
                    sin: "f32[5, 5]" = torch.ops.aten.sin.default(y);  y = None
                return (sin,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='y'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='sin'), target=None)])
    Range constraints: {u0: ValueRanges(lower=0, upper=5, is_bool=False), u1: ValueRanges(lower=0, upper=5, is_bool=False), u2: ValueRanges(lower=0, upper=5, is_bool=False), u3: ValueRanges(lower=0, upper=5, is_bool=False)}
    
