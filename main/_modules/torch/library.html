


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.library &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/library.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.4.0a0+gitb42cfca ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/library.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_environment_variables.html">Torch Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch.library</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.library</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">._ops</span> <span class="kn">import</span> <span class="n">OpOverload</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">weakref</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">torch._library.custom_ops</span> <span class="kn">import</span> <span class="n">custom_op</span><span class="p">,</span> <span class="n">_maybe_get_opdef</span><span class="p">,</span> <span class="n">device_types_t</span><span class="p">,</span> <span class="n">CustomOpDef</span>
<span class="kn">import</span> <span class="nn">torch._library</span> <span class="k">as</span> <span class="nn">_library</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;Library&#39;</span><span class="p">,</span>
    <span class="s1">&#39;impl&#39;</span><span class="p">,</span>
    <span class="s1">&#39;define&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fallthrough_kernel&#39;</span><span class="p">,</span>
    <span class="s1">&#39;impl_abstract&#39;</span><span class="p">,</span>
    <span class="s1">&#39;register_fake&#39;</span><span class="p">,</span>
    <span class="s1">&#39;get_ctx&#39;</span><span class="p">,</span>
    <span class="s1">&#39;custom_op&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Set containing the combination of (namespace, operator, DispatchKey) for which a new kernel has been registered</span>
<span class="c1"># The keys in the set are of the form `namespace + &quot;/&quot; + op_name + &quot;/&quot; + dispatch_key`.</span>
<span class="c1"># This set is maintained to ensure that two libraries don&#39;t try to override the exact same functionality to avoid</span>
<span class="c1"># libraries calling into kernels not intended to be called.</span>
<span class="n">_impls</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">_defs</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="c1"># prim is reserved by TorchScript interpreter</span>
<span class="n">_reserved_namespaces</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;prim&#39;</span><span class="p">]</span>

<div class="viewcode-block" id="fallthrough_kernel"><a class="viewcode-back" href="../../library.html#torch.library.fallthrough_kernel">[docs]</a><span class="k">def</span> <span class="nf">fallthrough_kernel</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A dummy function to pass to ``Library.impl`` in order to register a fallthrough.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;fallthrough_kernel() should never be called.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Library"><a class="viewcode-back" href="../../library.html#torch.library.Library">[docs]</a><span class="k">class</span> <span class="nc">Library</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class to create libraries that can be used to register new operators or</span>
<span class="sd">    override operators in existing libraries from Python.</span>
<span class="sd">    A user can optionally pass in a dispatch keyname if they only want to register</span>
<span class="sd">    kernels corresponding to only one specific dispatch key.</span>

<span class="sd">    To create a library to override operators in an existing library (with name ns), set the kind to &quot;IMPL&quot;.</span>
<span class="sd">    To create a new library (with name ns) to register new operators, set the kind to &quot;DEF&quot;.</span>
<span class="sd">    To create a fragment of a possibly existing library to register operators (and bypass</span>
<span class="sd">    the limitation that there is only one library for a given namespace), set the kind to</span>
<span class="sd">    &quot;FRAGMENT&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        ns: library name</span>
<span class="sd">        kind: &quot;DEF&quot;, &quot;IMPL&quot; (default: &quot;IMPL&quot;), &quot;FRAGMENT&quot;</span>
<span class="sd">        dispatch_key: PyTorch dispatch key (default: &quot;&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">kind</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">kind</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;IMPL&#39;</span><span class="p">,</span> <span class="s1">&#39;DEF&#39;</span><span class="p">,</span> <span class="s1">&#39;FRAGMENT&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unsupported kind: &quot;</span><span class="p">,</span> <span class="n">kind</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ns</span> <span class="ow">in</span> <span class="n">_reserved_namespaces</span> <span class="ow">and</span> <span class="p">(</span><span class="n">kind</span> <span class="o">==</span> <span class="s2">&quot;DEF&quot;</span> <span class="ow">or</span> <span class="n">kind</span> <span class="o">==</span> <span class="s1">&#39;FRAGMENT&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="s2">&quot; is a reserved namespace. Please try creating a library with another name.&quot;</span><span class="p">)</span>

        <span class="n">frame</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">filename</span><span class="p">,</span> <span class="n">lineno</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">filename</span><span class="p">,</span> <span class="n">frame</span><span class="o">.</span><span class="n">lineno</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dispatch_library</span><span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">lineno</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ns</span> <span class="o">=</span> <span class="n">ns</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op_defs</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op_impls</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_registration_handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">RegistrationHandle</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kind</span> <span class="o">=</span> <span class="n">kind</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dispatch_key</span> <span class="o">=</span> <span class="n">dispatch_key</span>
        <span class="c1"># Use a finalizer to setup the &quot;destructor&quot; instead of __del__.</span>
        <span class="c1"># Python __del__ can lead to weird things (globals and locals may already</span>
        <span class="c1"># be gone when __del__ actually gets called!). finalizers help the</span>
        <span class="c1"># situation because it lets us capture references and keeps them alive</span>
        <span class="n">weakref</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_del_library</span><span class="p">,</span> <span class="n">_impls</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_impls</span><span class="p">,</span> <span class="n">_defs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_defs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_registration_handles</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Library(kind=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kind</span><span class="si">}</span><span class="s2">, ns=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ns</span><span class="si">}</span><span class="s2">, dispatch_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dispatch_key</span><span class="si">}</span><span class="s2">)&gt;&quot;</span>

<div class="viewcode-block" id="Library.define"><a class="viewcode-back" href="../../library.html#torch.library.Library.define">[docs]</a>    <span class="k">def</span> <span class="nf">define</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">alias_analysis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="p">()):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&#39;&#39;&#39;Defines a new operator and its semantics in the ns namespace.</span>

<span class="sd">        Args:</span>
<span class="sd">            schema: function schema to define a new operator.</span>
<span class="sd">            alias_analysis (optional): Indicates if the aliasing properties of the operator arguments can be</span>
<span class="sd">                                       inferred from the schema (default behavior) or not (&quot;CONSERVATIVE&quot;).</span>
<span class="sd">            tags (Tag | Sequence[Tag]): one or more torch.Tag to apply to this</span>
<span class="sd">                                       operator. Tagging an operator changes the operator&#39;s behavior</span>
<span class="sd">                                       under various PyTorch subsystems; please read the docs for the</span>
<span class="sd">                                       torch.Tag carefully before applying it.</span>

<span class="sd">        Returns:</span>
<span class="sd">            name of the operator as inferred from the schema.</span>

<span class="sd">        Example::</span>
<span class="sd">            &gt;&gt;&gt; my_lib = Library(&quot;mylib&quot;, &quot;DEF&quot;)</span>
<span class="sd">            &gt;&gt;&gt; my_lib.define(&quot;sum(Tensor self) -&gt; Tensor&quot;)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># This is added because we also want to disallow PURE_FUNCTION alias analysis which is a valid</span>
        <span class="c1"># AliasAnalysis type in C++</span>
        <span class="k">if</span> <span class="n">alias_analysis</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;FROM_SCHEMA&quot;</span><span class="p">,</span> <span class="s2">&quot;CONSERVATIVE&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid alias_analysis type </span><span class="si">{</span><span class="n">alias_analysis</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tags</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tag</span><span class="p">):</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">(</span><span class="n">tags</span><span class="p">,)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">alias_analysis</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tags</span><span class="p">))</span>
        <span class="n">qualname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ns</span> <span class="o">+</span> <span class="s2">&quot;::&quot;</span> <span class="o">+</span> <span class="n">schema</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;(&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op_defs</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
        <span class="n">_defs</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span></div>

    <span class="k">def</span> <span class="nf">_register_fake</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&#39;&#39;&#39;Registers the fake impl for an operator defined in the library.&#39;&#39;&#39;</span>
        <span class="n">source</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_source</span><span class="p">(</span><span class="n">_stacklevel</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">_getframe</span><span class="p">(</span><span class="n">_stacklevel</span><span class="p">)</span>
        <span class="n">caller_module</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getmodule</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
        <span class="c1"># Can be none if you call register_fake from somewhere there isn&#39;t a module</span>
        <span class="c1"># (e.g. __main__)</span>
        <span class="n">caller_module_name</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">caller_module</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">caller_module</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="c1"># TODO(rzou): We&#39;re gonna need to stage this change with torchvision,</span>
        <span class="c1"># since torchvision is github first.</span>
        <span class="k">if</span> <span class="n">caller_module_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">caller_module_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;torchvision.&quot;</span><span class="p">):</span>
            <span class="n">caller_module_name</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">qualname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ns</span><span class="si">}</span><span class="s2">::</span><span class="si">{</span><span class="n">op_name</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">entry</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">simple_registry</span><span class="o">.</span><span class="n">singleton</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">caller_module_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">func_to_register</span> <span class="o">=</span> <span class="n">_check_pystubs_once</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">qualname</span><span class="p">,</span> <span class="n">caller_module_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">func_to_register</span> <span class="o">=</span> <span class="n">fn</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">entry</span><span class="o">.</span><span class="n">abstract_impl</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">func_to_register</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_registration_handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_impl_with_aoti_compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&#39;&#39;&#39;Register the operator to use the AOTI-compiled implementation.</span>

<span class="sd">        Args:</span>
<span class="sd">            op_name: operator name (along with the overload) or OpOverload object.</span>
<span class="sd">            dispatch_key: dispatch key that the input function should be registered for. By default, it uses</span>
<span class="sd">                          the dispatch key that the library was created with.</span>

<span class="sd">        Example::</span>
<span class="sd">            &gt;&gt;&gt; my_lib = Library(&quot;aten&quot;, &quot;IMPL&quot;)</span>
<span class="sd">            &gt;&gt;&gt; my_lib._impl_with_aoti_compile(&quot;div.Tensor&quot;, &quot;CPU&quot;)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="n">dispatch_key</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
            <span class="n">dispatch_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dispatch_key</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">DispatchKeySet</span><span class="p">(</span><span class="n">dispatch_key</span><span class="p">)</span><span class="o">.</span><span class="n">has</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DispatchKey</span><span class="o">.</span><span class="n">Dense</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">op_name</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">OpOverload</span><span class="p">):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">op_name</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">name</span>
            <span class="n">overload_name</span> <span class="o">=</span> <span class="n">op_name</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">overload_name</span>
            <span class="k">if</span> <span class="n">overload_name</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">overload_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;_impl_with_aoti_compile should be passed either a name or an OpOverload object &quot;</span>
                               <span class="s2">&quot;as the first argument&quot;</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ns</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;::&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">dispatch_key</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">_impls</span><span class="p">:</span>
            <span class="c1"># TODO: in future, add more info about where the existing function is registered (this info is</span>
            <span class="c1"># today already returned by the C++ warning when _impl_with_aoti_compile is called but we error out before that)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;This is not allowed since there&#39;s already a kernel registered from python overriding </span><span class="si">{}</span><span class="s2">&quot;</span>
                               <span class="s2">&quot;&#39;s behavior for </span><span class="si">{}</span><span class="s2"> dispatch key and </span><span class="si">{}</span><span class="s2"> namespace.&quot;</span><span class="o">.</span>
                               <span class="nb">format</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;::&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dispatch_key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ns</span><span class="p">))</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">impl_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="o">.</span><span class="n">impl_with_aoti_compile</span>
        <span class="n">impl_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ns</span><span class="p">,</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;::&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dispatch_key</span><span class="p">)</span>

        <span class="n">_impls</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op_impls</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

<div class="viewcode-block" id="Library.impl"><a class="viewcode-back" href="../../library.html#torch.library.Library.impl">[docs]</a>    <span class="k">def</span> <span class="nf">impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">with_keyset</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&#39;&#39;&#39;Registers the function implementation for an operator defined in the library.</span>

<span class="sd">        Args:</span>
<span class="sd">            op_name: operator name (along with the overload) or OpOverload object.</span>
<span class="sd">            fn: function that&#39;s the operator implementation for the input dispatch key or :func:`~fallthrough_kernel`</span>
<span class="sd">                to register a fallthrough.</span>
<span class="sd">            dispatch_key: dispatch key that the input function should be registered for. By default, it uses</span>
<span class="sd">                          the dispatch key that the library was created with.</span>

<span class="sd">        Example::</span>
<span class="sd">            &gt;&gt;&gt; my_lib = Library(&quot;aten&quot;, &quot;IMPL&quot;)</span>
<span class="sd">            &gt;&gt;&gt; def div_cpu(self, other):</span>
<span class="sd">            &gt;&gt;&gt;     return self * (1 / other)</span>
<span class="sd">            &gt;&gt;&gt; my_lib.impl(&quot;div.Tensor&quot;, div_cpu, &quot;CPU&quot;)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input function is required to be a callable but found type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dispatch_key</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
            <span class="n">dispatch_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dispatch_key</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">op_name</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">OpOverload</span><span class="p">):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">op_name</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">name</span>
            <span class="n">overload_name</span> <span class="o">=</span> <span class="n">op_name</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">overload_name</span>
            <span class="k">if</span> <span class="n">overload_name</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">overload_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;impl should be passed either a name or an OpOverload object as the first argument&quot;</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ns</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;::&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">dispatch_key</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">_impls</span><span class="p">:</span>
            <span class="c1"># TODO: in future, add more info about where the existing function is registered (this info is</span>
            <span class="c1"># today already returned by the C++ warning when impl is called but we error out before that)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;This is not allowed since there&#39;s already a kernel registered from python overriding </span><span class="si">{}</span><span class="s2">&quot;</span>
                               <span class="s2">&quot;&#39;s behavior for </span><span class="si">{}</span><span class="s2"> dispatch key and </span><span class="si">{}</span><span class="s2"> namespace.&quot;</span><span class="o">.</span>
                               <span class="nb">format</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;::&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dispatch_key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ns</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">dispatch_key</span> <span class="o">==</span> <span class="s2">&quot;Meta&quot;</span><span class="p">:</span>
            <span class="n">dispatcher_op_name</span> <span class="o">=</span> <span class="n">name</span>
            <span class="k">if</span> <span class="s1">&#39;::&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dispatcher_op_name</span><span class="p">:</span>
                <span class="n">dispatcher_op_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ns</span><span class="si">}</span><span class="s1">::</span><span class="si">{</span><span class="n">dispatcher_op_name</span><span class="si">}</span><span class="s1">&#39;</span>

            <span class="c1"># Internally, we shouldn&#39;t be registering meta kernels for any operators that</span>
            <span class="c1"># have CompositeImplicitAutograd kernels.</span>
            <span class="c1"># Instead, we should be letting those decompositions run, and writing meta kernels</span>
            <span class="c1"># only for the base operators.</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dispatch_has_kernel_for_dispatch_key</span><span class="p">(</span><span class="n">dispatcher_op_name</span><span class="p">,</span> <span class="s2">&quot;CompositeImplicitAutograd&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;We should not register a meta kernel directly to the operator &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;,&quot;</span>
                    <span class="s2">&quot; because it has a CompositeImplicitAutograd kernel in core.&quot;</span>
                    <span class="s2">&quot; Instead we should let the operator decompose, and ensure that we have meta kernels&quot;</span>
                    <span class="s2">&quot; for the base ops that it decomposes into.&quot;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dispatch_key</span> <span class="k">if</span> <span class="n">dispatch_key</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span> <span class="k">else</span> <span class="s2">&quot;CompositeImplicitAutograd&quot;</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">with_keyset</span><span class="p">)</span>

        <span class="n">_impls</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op_impls</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_destroy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_registration_handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_registration_handles</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">global</span> <span class="n">_impls</span>
        <span class="n">_impls</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_impls</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_op_defs</span><span class="p">:</span>
            <span class="c1"># Delete the cached torch.ops.ns.foo if it was registered.</span>
            <span class="c1"># Otherwise, accessing it leads to a segfault.</span>
            <span class="c1"># It&#39;s possible that we only registered an overload in this Library</span>
            <span class="c1"># and another library owns an alive overload.</span>
            <span class="c1"># That&#39;s OK - the next time torch.ops.ns.foo gets called, it&#39;ll be</span>
            <span class="c1"># recomputed to point at the right collection of overloads.</span>
            <span class="n">ns</span><span class="p">,</span> <span class="n">name_with_overload</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;::&quot;</span><span class="p">)</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">name_with_overload</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="n">ns</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">namespace</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_del_library</span><span class="p">(</span><span class="n">captured_impls</span><span class="p">,</span> <span class="n">op_impls</span><span class="p">,</span> <span class="n">captured_defs</span><span class="p">,</span> <span class="n">op_defs</span><span class="p">,</span> <span class="n">registration_handles</span><span class="p">):</span>
    <span class="n">captured_impls</span> <span class="o">-=</span> <span class="n">op_impls</span>
    <span class="n">captured_defs</span> <span class="o">-=</span> <span class="n">op_defs</span>
    <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">registration_handles</span><span class="p">:</span>
        <span class="n">handle</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_scoped_library</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">lib</span> <span class="o">=</span> <span class="n">Library</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">lib</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">_destroy</span><span class="p">()</span>


<span class="n">_keep_alive</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Library</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>


<span class="n">NAMELESS_SCHEMA</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\(.*\) -&gt; .*&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="define"><a class="viewcode-back" href="../../library.html#torch.library.define">[docs]</a><span class="nd">@functools</span><span class="o">.</span><span class="n">singledispatch</span>
<span class="k">def</span> <span class="nf">define</span><span class="p">(</span><span class="n">qualname</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lib</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="p">()):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a new operator.</span>

<span class="sd">    In PyTorch, defining an op (short for &quot;operator&quot;) is a two step-process:</span>
<span class="sd">    - we need to define the op (by providing an operator name and schema)</span>
<span class="sd">    - we need to implement behavior for how the operator interacts with</span>
<span class="sd">    various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.</span>

<span class="sd">    This entrypoint defines the custom operator (the first step)</span>
<span class="sd">    you must then perform the second step by calling various</span>
<span class="sd">    ``impl_*`` APIs, like :func:`torch.library.impl` or</span>
<span class="sd">    :func:`torch.library.register_fake`.</span>

<span class="sd">    Args:</span>
<span class="sd">        qualname (str): The qualified name for the operator. Should be</span>
<span class="sd">            a string that looks like &quot;namespace::name&quot;, e.g. &quot;aten::sin&quot;.</span>
<span class="sd">            Operators in PyTorch need a namespace to</span>
<span class="sd">            avoid name collisions; a given operator may only be created once.</span>
<span class="sd">            If you are writing a Python library, we recommend the namespace to</span>
<span class="sd">            be the name of your top-level module.</span>
<span class="sd">        schema (str): The schema of the operator. E.g. &quot;(Tensor x) -&gt; Tensor&quot;</span>
<span class="sd">            for an op that accepts one Tensor and returns one Tensor. It does</span>
<span class="sd">            not contain the operator name (that is passed in ``qualname``).</span>
<span class="sd">        lib (Optional[Library]): If provided, the lifetime of this operator</span>
<span class="sd">            will be tied to the lifetime of the Library object.</span>
<span class="sd">        tags (Tag | Sequence[Tag]): one or more torch.Tag to apply to this</span>
<span class="sd">            operator. Tagging an operator changes the operator&#39;s behavior</span>
<span class="sd">            under various PyTorch subsystems; please read the docs for the</span>
<span class="sd">            torch.Tag carefully before applying it.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the operator</span>
<span class="sd">        &gt;&gt;&gt; torch.library.define(&quot;mylib::sin&quot;, &quot;(Tensor x) -&gt; Tensor&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Add implementations for the operator</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.impl(&quot;mylib::sin&quot;, &quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def f(x):</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(np.sin(x.numpy()))</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Call the new operator from torch.ops.</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt; y = torch.ops.mylib.sin(x)</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(y, x.sin())</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">qualname</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;define(qualname, schema): expected qualname &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;to be instance of str, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">namespace</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">parse_namespace</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">lib</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lib</span> <span class="o">=</span> <span class="n">Library</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;FRAGMENT&quot;</span><span class="p">)</span>
        <span class="n">_keep_alive</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lib</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">NAMELESS_SCHEMA</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;define(qualname, schema, ...): expected schema &quot;</span>
            <span class="sa">f</span><span class="s1">&#39;to look like e.g. &quot;(Tensor x) -&gt; Tensor&quot; but &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;got &quot;</span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
    <span class="n">lib</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="n">schema</span><span class="p">,</span> <span class="n">alias_analysis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="n">tags</span><span class="p">)</span></div>


<span class="nd">@define</span><span class="o">.</span><span class="n">register</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">lib</span><span class="p">:</span> <span class="n">Library</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">alias_analysis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The old torch.library.define.</span>
<span class="sd">    We&#39;re keeping this around for BC reasons</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">wrap</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">alias_analysis</span><span class="p">)</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">f</span>
    <span class="k">return</span> <span class="n">wrap</span>


<div class="viewcode-block" id="impl"><a class="viewcode-back" href="../../library.html#torch.library.impl">[docs]</a><span class="nd">@functools</span><span class="o">.</span><span class="n">singledispatch</span>
<span class="k">def</span> <span class="nf">impl</span><span class="p">(</span><span class="n">qualname</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lib</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register an implementation for a device type for this operator.</span>

<span class="sd">    You may pass &quot;default&quot; for ``types`` to register this implementation as the</span>
<span class="sd">    default implementation for ALL device types.</span>
<span class="sd">    Please only use this if the implementation truly supports all device types;</span>
<span class="sd">    for example, this is true if it is a composition of built-in PyTorch operators.</span>

<span class="sd">    Some valid types are: &quot;cpu&quot;, &quot;cuda&quot;, &quot;xla&quot;, &quot;mps&quot;, &quot;ipu&quot;, &quot;xpu&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        qualname (str): Should be a string that looks like &quot;namespace::operator_name&quot;.</span>
<span class="sd">        types (str | Sequence[str]): The device types to register an impl to.</span>
<span class="sd">        lib (Optional[Library]): If provided, the lifetime of this registration</span>
<span class="sd">            will be tied to the lifetime of the Library object.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the operator</span>
<span class="sd">        &gt;&gt;&gt; torch.library.define(&quot;mylib::mysin&quot;, &quot;(Tensor x) -&gt; Tensor&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Add implementations for the cpu device</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.impl(&quot;mylib::mysin&quot;, &quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def f(x):</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(np.sin(x.numpy()))</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt; y = torch.ops.mylib.mysin(x)</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(y, x.sin())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">types</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">types</span> <span class="o">=</span> <span class="p">(</span><span class="n">types</span><span class="p">,)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">({})</span>
    <span class="k">for</span> <span class="n">typ</span> <span class="ow">in</span> <span class="n">types</span><span class="p">:</span>
        <span class="n">is_dispatch_key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_parse_dispatch_key</span><span class="p">(</span><span class="n">typ</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dispatch_key</span><span class="p">:</span>
            <span class="c1"># We also support passing a DispatchKey to impl. Please prefer using</span>
            <span class="c1"># the higher-level torch.library APIs and only pass DispatchKey to</span>
            <span class="c1"># torch.library.impl with caution (or even better, don&#39;t use this</span>
            <span class="c1"># option and file an issue on GitHub for what you need).</span>
            <span class="c1"># We don&#39;t advertise this to users because</span>
            <span class="c1"># it is very easy to shoot yourself in the foot.</span>
            <span class="n">keys</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">typ</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">keys</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">_device_type_to_key</span><span class="p">(</span><span class="n">typ</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="n">namespace</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">parse_namespace</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lib</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_lib</span> <span class="o">=</span> <span class="n">Library</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;FRAGMENT&quot;</span><span class="p">)</span>
            <span class="n">_keep_alive</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">use_lib</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_lib</span> <span class="o">=</span> <span class="n">lib</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
            <span class="n">use_lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">qualname</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">register</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">register</span><span class="p">(</span><span class="n">func</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_device_type_to_key</span><span class="p">(</span><span class="n">device_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
        <span class="c1"># This is technically not correct, because although all device_type</span>
        <span class="c1"># DispatchKeys are included in CompositeExplicitAutograd,</span>
        <span class="c1"># not everything in CompositeExplicitAutograd is associated with a</span>
        <span class="c1"># device_type. I don&#39;t really care that much about the difference.</span>
        <span class="k">return</span> <span class="s2">&quot;CompositeExplicitAutograd&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dispatch_key_for_device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>


<span class="nd">@impl</span><span class="o">.</span><span class="n">register</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">lib</span><span class="p">:</span> <span class="n">Library</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Legacy torch.library.impl API. Kept around for BC&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">wrap</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">f</span>
    <span class="k">return</span> <span class="n">wrap</span>


<div class="viewcode-block" id="impl_abstract"><a class="viewcode-back" href="../../library.html#torch.library.impl_abstract">[docs]</a><span class="nd">@deprecated</span><span class="p">(</span>
    <span class="s2">&quot;`torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that &quot;</span>
    <span class="s2">&quot;instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.&quot;</span><span class="p">,</span>
    <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">impl_abstract</span><span class="p">(</span><span class="n">qualname</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lib</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This API was renamed to :func:`torch.library.register_fake` in PyTorch 2.4.</span>
<span class="sd">    Please use that instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stacklevel</span> <span class="o">=</span> <span class="n">_stacklevel</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">register_fake</span><span class="p">(</span><span class="n">qualname</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">lib</span><span class="o">=</span><span class="n">lib</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="n">_stacklevel</span><span class="p">)</span></div>


<span class="n">_op_identifier</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;torch._ops.OpOverload&quot;</span><span class="p">,</span> <span class="s2">&quot;torch._library.custom_ops.CustomOpDef&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="register_kernel"><a class="viewcode-back" href="../../library.html#torch.library.register_kernel">[docs]</a><span class="k">def</span> <span class="nf">register_kernel</span><span class="p">(</span>
        <span class="n">op</span><span class="p">:</span> <span class="n">_op_identifier</span><span class="p">,</span>
        <span class="n">device_types</span><span class="p">:</span> <span class="n">device_types_t</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">/</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">lib</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Library</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register an implementation for a device type for this operator.</span>

<span class="sd">    Some valid device_types are: &quot;cpu&quot;, &quot;cuda&quot;, &quot;xla&quot;, &quot;mps&quot;, &quot;ipu&quot;, &quot;xpu&quot;.</span>
<span class="sd">    This API may be used as a decorator.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Callable): The function to register as the implementation for</span>
<span class="sd">            the given device types.</span>
<span class="sd">        device_types (None | str | Sequence[str]): The device_types to register an impl to.</span>
<span class="sd">            If None, we will register to all device types -- please only use</span>
<span class="sd">            this option if your implementation is truly device-type-agnostic.</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from torch.library import custom_op</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create a custom op that works on cpu</span>
<span class="sd">        &gt;&gt;&gt; @custom_op(&quot;mylib::numpy_sin&quot;, mutates_args=(), device_types=&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def numpy_sin(x: Tensor) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.numpy()</span>
<span class="sd">        &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(y_np)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Add implementations for the cuda device</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.register_kernel(&quot;mylib::numpy_sin&quot;, &quot;cuda&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def _(x):</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">        &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(y_np).to(device=x.device)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x_cpu = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt; x_cuda = x_cpu.cuda()</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">custom_ops</span><span class="o">.</span><span class="n">CustomOpDef</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;register_kernel(op): got unexpected type for op: {type(op)}&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_name</span>
    <span class="n">opdef</span> <span class="o">=</span> <span class="n">_maybe_get_opdef</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">opdef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">opdef</span><span class="o">.</span><span class="n">register_kernel</span><span class="p">(</span><span class="n">device_types</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device_types</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">device_types</span> <span class="o">=</span> <span class="s2">&quot;CompositeExplicitAutograd&quot;</span>
    <span class="k">return</span> <span class="n">impl</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">device_types</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">lib</span><span class="o">=</span><span class="n">lib</span><span class="p">)</span></div>


<div class="viewcode-block" id="register_fake"><a class="viewcode-back" href="../../library.html#torch.library.register_fake">[docs]</a><span class="k">def</span> <span class="nf">register_fake</span><span class="p">(</span>
        <span class="n">op</span><span class="p">:</span> <span class="n">_op_identifier</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">/</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">lib</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Library</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">_stacklevel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a FakeTensor implementation (&quot;fake impl&quot;) for this operator.</span>

<span class="sd">    Also sometimes known as a &quot;meta kernel&quot;, &quot;abstract impl&quot;.</span>

<span class="sd">    An &quot;FakeTensor implementation&quot; specifies the behavior of this operator on</span>
<span class="sd">    Tensors that carry no data (&quot;FakeTensor&quot;). Given some input Tensors with</span>
<span class="sd">    certain properties (sizes/strides/storage_offset/device), it specifies</span>
<span class="sd">    what the properties of the output Tensors are.</span>

<span class="sd">    The FakeTensor implementation has the same signature as the operator.</span>
<span class="sd">    It is run for both FakeTensors and meta tensors. To write a FakeTensor</span>
<span class="sd">    implementation, assume that all Tensor inputs to the operator are</span>
<span class="sd">    regular CPU/CUDA/Meta tensors, but they do not have storage, and</span>
<span class="sd">    you are trying to return regular CPU/CUDA/Meta tensor(s) as output.</span>
<span class="sd">    The FakeTensor implementation must consist of only PyTorch operations</span>
<span class="sd">    (and may not directly access the storage or data of any input or</span>
<span class="sd">    intermediate Tensors).</span>

<span class="sd">    This API may be used as a decorator (see examples).</span>

<span class="sd">    For a detailed guide on custom ops, please see</span>
<span class="sd">    https://pytorch.org/docs/main/notes/custom_operators.html</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example 1: an operator without data-dependent output shape</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::custom_linear&quot;, mutates_args=())</span>
<span class="sd">        &gt;&gt;&gt; def custom_linear(x: Tensor, weight: Tensor, bias: Tensor) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     raise NotImplementedError(&quot;Implementation goes here&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.register_fake(&quot;mylib::custom_linear&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def _(x, weight, bias):</span>
<span class="sd">        &gt;&gt;&gt;     assert x.dim() == 2</span>
<span class="sd">        &gt;&gt;&gt;     assert weight.dim() == 2</span>
<span class="sd">        &gt;&gt;&gt;     assert bias.dim() == 1</span>
<span class="sd">        &gt;&gt;&gt;     assert x.shape[1] == weight.shape[1]</span>
<span class="sd">        &gt;&gt;&gt;     assert weight.shape[0] == bias.shape[0]</span>
<span class="sd">        &gt;&gt;&gt;     assert x.device == weight.device</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     return (x @ weight.t()) + bias</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; with torch._subclasses.fake_tensor.FakeTensorMode():</span>
<span class="sd">        &gt;&gt;&gt;     x = torch.randn(2, 3)</span>
<span class="sd">        &gt;&gt;&gt;     w = torch.randn(3, 3)</span>
<span class="sd">        &gt;&gt;&gt;     b = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt;     y = torch.ops.mylib.custom_linear(x, w, b)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; assert y.shape == (2, 3)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example 2: an operator with data-dependent output shape</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::custom_nonzero&quot;, mutates_args=())</span>
<span class="sd">        &gt;&gt;&gt; def custom_nonzero(x: Tensor) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.numpy(force=True)</span>
<span class="sd">        &gt;&gt;&gt;     res = np.stack(np.nonzero(x_np), axis=1)</span>
<span class="sd">        &gt;&gt;&gt;     return torch.tensor(res, device=x.device)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.register_fake(&quot;mylib::custom_nonzero&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def _(x):</span>
<span class="sd">        &gt;&gt;&gt;     # Number of nonzero-elements is data-dependent.</span>
<span class="sd">        &gt;&gt;&gt;     # Since we cannot peek at the data in an fake impl,</span>
<span class="sd">        &gt;&gt;&gt;     # we use the ctx object to construct a new symint that</span>
<span class="sd">        &gt;&gt;&gt;     # represents the data-dependent size.</span>
<span class="sd">        &gt;&gt;&gt;     ctx = torch.library.get_ctx()</span>
<span class="sd">        &gt;&gt;&gt;     nnz = ctx.new_dynamic_size()</span>
<span class="sd">        &gt;&gt;&gt;     shape = [nnz, x.dim()]</span>
<span class="sd">        &gt;&gt;&gt;     result = x.new_empty(shape, dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt;     return result</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torch.fx.experimental.proxy_tensor import make_fx</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4, 0])</span>
<span class="sd">        &gt;&gt;&gt; trace = make_fx(torch.ops.mylib.custom_nonzero, tracing_mode=&quot;symbolic&quot;)(x)</span>
<span class="sd">        &gt;&gt;&gt; trace.print_readable()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(trace(x), torch.ops.mylib.custom_nonzero(x))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">custom_ops</span><span class="o">.</span><span class="n">CustomOpDef</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;register_fake(op): got unexpected type for op: {type(op)}&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_name</span>
    <span class="n">opdef</span> <span class="o">=</span> <span class="n">_maybe_get_opdef</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">opdef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">opdef</span><span class="o">.</span><span class="n">register_fake</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">opdef</span><span class="o">.</span><span class="n">register_fake</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>

    <span class="n">stacklevel</span> <span class="o">=</span> <span class="n">_stacklevel</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="n">namespace</span><span class="p">,</span> <span class="n">op_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">parse_namespace</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lib</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_lib</span> <span class="o">=</span> <span class="n">Library</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;FRAGMENT&quot;</span><span class="p">)</span>
            <span class="n">_keep_alive</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">use_lib</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_lib</span> <span class="o">=</span> <span class="n">lib</span>
        <span class="n">use_lib</span><span class="o">.</span><span class="n">_register_fake</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="n">stacklevel</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">func</span>

    <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">register</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">stacklevel</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">register</span><span class="p">(</span><span class="n">func</span><span class="p">)</span></div>


<div class="viewcode-block" id="register_autograd"><a class="viewcode-back" href="../../library.html#torch.library.register_autograd">[docs]</a><span class="k">def</span> <span class="nf">register_autograd</span><span class="p">(</span><span class="n">op</span><span class="p">:</span> <span class="n">_op_identifier</span><span class="p">,</span> <span class="n">backward</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">/</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">setup_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">lib</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a backward formula for this custom op.</span>

<span class="sd">    In order for an operator to work with autograd, you need to register</span>
<span class="sd">    a backward formula:</span>
<span class="sd">    1. You must tell us how to compute gradients during the backward pass</span>
<span class="sd">    by providing us a &quot;backward&quot; function.</span>
<span class="sd">    2. If you need any values from the forward to compute gradients, you can</span>
<span class="sd">    use `setup_context` to save values for backward.</span>

<span class="sd">    ``backward`` runs during the backward pass. It accepts ``(ctx, *grads)``:</span>
<span class="sd">    - ``grads`` is one or more gradients. The number of gradients matches</span>
<span class="sd">    the number of outputs of the operator.</span>
<span class="sd">    The ``ctx`` object is `the same ctx object &lt;context_method_mixins&gt;`_ used by</span>
<span class="sd">    :class:`torch.autograd.Function`. The semantics of ``backward_fn`` are the</span>
<span class="sd">    same as :meth:`torch.autograd.Function.backward`.</span>

<span class="sd">    ``setup_context(ctx, inputs, output)`` runs during the forward pass.</span>
<span class="sd">    Please save quantities needed for backward onto the ``ctx`` object via</span>
<span class="sd">    either :meth:`torch.autograd.function.FunctionCtx.save_for_backward`</span>
<span class="sd">    or assigning them as attributes of ``ctx``. If your custom op has</span>
<span class="sd">    kwarg-only arguments, we expect the signature of ``setup_context``</span>
<span class="sd">    to be ``setup_context(ctx, inputs, keyword_only_inputs, output)``.</span>

<span class="sd">    Both ``setup_context_fn`` and ``backward_fn`` must be traceable. That is,</span>
<span class="sd">    they may not directly access :meth:`torch.Tensor.data_ptr` and they must</span>
<span class="sd">    not depend on or mutate global state. If you need a non-traceable backward,</span>
<span class="sd">    you can make it a separate custom_op that you call inside ``backward_fn``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::numpy_sin&quot;, mutates_args=())</span>
<span class="sd">        &gt;&gt;&gt; def numpy_sin(x: Tensor) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">        &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(y_np).to(device=x.device)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def setup_context(ctx, inputs, output) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x, = inputs</span>
<span class="sd">        &gt;&gt;&gt;     ctx.save_for_backward(x)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def backward(ctx, grad):</span>
<span class="sd">        &gt;&gt;&gt;     x, = ctx.saved_tensors</span>
<span class="sd">        &gt;&gt;&gt;     return grad * x.cos()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.library.register_autograd(&quot;mylib::numpy_sin&quot;, backward, setup_context=setup_context)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; y = numpy_sin(x)</span>
<span class="sd">        &gt;&gt;&gt; grad_x, = torch.autograd.grad(y, x, torch.ones_like(y))</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(grad_x, x.cos())</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example with a keyword-only arg</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::numpy_mul&quot;, mutates_args=())</span>
<span class="sd">        &gt;&gt;&gt; def numpy_mul(x: Tensor, *, val: float) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">        &gt;&gt;&gt;     y_np = x_np * val</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(y_np).to(device=x.device)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def setup_context(ctx, inputs, keyword_only_inputs, output) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     ctx.val = keyword_only_inputs[&quot;val&quot;]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def backward(ctx, grad):</span>
<span class="sd">        &gt;&gt;&gt;     return grad * ctx.val</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.library.register_autograd(&quot;mylib::numpy_mul&quot;, backward, setup_context=setup_context)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; y = numpy_mul(x, val=3.14)</span>
<span class="sd">        &gt;&gt;&gt; grad_x, = torch.autograd.grad(y, x, torch.ones_like(y))</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(grad_x, torch.full_like(x, 3.14))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">custom_ops</span><span class="o">.</span><span class="n">CustomOpDef</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;register_autograd(op): got unexpected type for op: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">op</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_name</span>
    <span class="n">opdef</span> <span class="o">=</span> <span class="n">_maybe_get_opdef</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">opdef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">opdef</span><span class="o">.</span><span class="n">register_autograd</span><span class="p">(</span><span class="n">backward</span><span class="p">,</span> <span class="n">setup_context</span><span class="o">=</span><span class="n">setup_context</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
    <span class="n">qualname</span> <span class="o">=</span> <span class="n">op</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">lookup_op</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
    <span class="n">schema</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_schema</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">is_functional_schema</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Cannot register autograd formula for non-functional operator &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">op</span><span class="si">}</span><span class="s2"> with schema </span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">. Please create &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;a functional operator and register an autograd formula for that.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">has_kwarg_only_tensors</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;register_autograd with kwarg-only Tensor args. In the original &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;definition of the op, please make your tensors not kwarg-only. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Got: </span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">info</span> <span class="o">=</span> <span class="n">_library</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Info</span><span class="p">(</span><span class="n">backward</span><span class="p">,</span> <span class="n">setup_context</span><span class="p">)</span>
    <span class="n">autograd_kernel</span> <span class="o">=</span> <span class="n">_library</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">make_autograd_impl</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
    <span class="n">namespace</span><span class="p">,</span> <span class="n">opname</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">parse_namespace</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">lib</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lib</span> <span class="o">=</span> <span class="n">Library</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;FRAGMENT&quot;</span><span class="p">)</span>
        <span class="n">_keep_alive</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lib</span><span class="p">)</span>
    <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">opname</span><span class="p">,</span> <span class="n">autograd_kernel</span><span class="p">,</span> <span class="s2">&quot;Autograd&quot;</span><span class="p">,</span> <span class="n">with_keyset</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<span class="c1"># If the op was defined in C++, then we want to make sure there was an</span>
<span class="c1"># m.set_python_module(module, ...) call and that the module is the</span>
<span class="c1"># same as the module that called torch.library.register_fake.</span>
<span class="k">def</span> <span class="nf">_check_pystubs_once</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">qualname</span><span class="p">,</span> <span class="n">actual_module_name</span><span class="p">):</span>
    <span class="n">checked</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">checked</span>
        <span class="k">if</span> <span class="n">checked</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">lookup_op</span><span class="p">(</span><span class="n">qualname</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">_defined_in_python</span><span class="p">:</span>
            <span class="n">checked</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">maybe_pystub</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dispatch_pystub</span><span class="p">(</span>
            <span class="n">op</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="n">op</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">overload_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">maybe_pystub</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">requires_set_python_module</span><span class="p">():</span>
                <span class="n">namespace</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">namespace</span>
                <span class="n">cpp_filename</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">debug</span><span class="p">()</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Operator &#39;</span><span class="si">{</span><span class="n">qualname</span><span class="si">}</span><span class="s2">&#39; was defined in C++ and has a Python &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;fake impl. In this situation, we require there to also be a &quot;</span>
                    <span class="sa">f</span><span class="s1">&#39;companion C++ `m.set_python_module(&quot;</span><span class="si">{</span><span class="n">actual_module_name</span><span class="si">}</span><span class="s1">&quot;)` &#39;</span>
                    <span class="sa">f</span><span class="s2">&quot;call, but we could not find one. Please add that to &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;to the top of the C++ TORCH_LIBRARY(</span><span class="si">{</span><span class="n">namespace</span><span class="si">}</span><span class="s2">, ...) block the &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;operator was registered in (</span><span class="si">{</span><span class="n">cpp_filename</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pystub_module</span> <span class="o">=</span> <span class="n">maybe_pystub</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">actual_module_name</span> <span class="o">!=</span> <span class="n">pystub_module</span><span class="p">:</span>
                <span class="n">cpp_filename</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">debug</span><span class="p">()</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Operator &#39;</span><span class="si">{</span><span class="n">qualname</span><span class="si">}</span><span class="s2">&#39; specified that its python fake impl &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;is in the Python module &#39;</span><span class="si">{</span><span class="n">pystub_module</span><span class="si">}</span><span class="s2">&#39; but it was actually found &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;in &#39;</span><span class="si">{</span><span class="n">actual_module_name</span><span class="si">}</span><span class="s2">&#39;. Please either move the fake impl &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;or correct the m.set_python_module call (</span><span class="si">{</span><span class="n">cpp_filename</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="n">checked</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inner</span>


<span class="c1"># NOTE [ctx inside the fake implementation]</span>
<span class="c1"># If a user has an operator with data-dependent output shape, then when writing</span>
<span class="c1"># a fake implementation they must query the current ctx and use methods on the</span>
<span class="c1"># ctx to construct a new unbacked symint.</span>
<span class="c1">#</span>
<span class="c1"># This is done via us setting the global_ctx_getter function every time a fake</span>
<span class="c1"># implementation is invoked.</span>
<div class="viewcode-block" id="get_ctx"><a class="viewcode-back" href="../../library.html#torch.library.get_ctx">[docs]</a><span class="k">def</span> <span class="nf">get_ctx</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="s2">&quot;torch._library.abstract_impl.AbstractImplCtx&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;get_ctx() returns the current AbstractImplCtx object.</span>

<span class="sd">    Calling ``get_ctx()`` is only valid inside of an fake impl</span>
<span class="sd">    (see :func:`torch.library.register_fake` for more usage details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">abstract_impl</span><span class="o">.</span><span class="n">global_ctx_getter</span><span class="p">()</span></div>


<span class="n">_OPCHECK_DEFAULT_UTILS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;test_schema&quot;</span><span class="p">,</span>
    <span class="s2">&quot;test_autograd_registration&quot;</span><span class="p">,</span>
    <span class="s2">&quot;test_faketensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;test_aot_dispatch_dynamic&quot;</span><span class="p">,</span>
<span class="p">)</span>


<div class="viewcode-block" id="opcheck"><a class="viewcode-back" href="../../library.html#torch.library.opcheck">[docs]</a><span class="k">def</span> <span class="nf">opcheck</span><span class="p">(</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverloadPacket</span><span class="p">,</span> <span class="n">CustomOpDef</span><span class="p">],</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">test_utils</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">_OPCHECK_DEFAULT_UTILS</span><span class="p">,</span>
    <span class="n">raise_exception</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given an operator and some sample arguments, tests if the operator is</span>
<span class="sd">    registered correctly.</span>

<span class="sd">    That is, when you use the torch.library/TORCH_LIBRARY APIs to create a</span>
<span class="sd">    custom op, you specified metadata (e.g. mutability info) about the custom op</span>
<span class="sd">    and these APIs require that the functions you pass them satisfy certain</span>
<span class="sd">    properties (e.g. no data pointer access in the fake/meta/abstract kernel)</span>
<span class="sd">    ``opcheck`` tests these metadata and properties.</span>

<span class="sd">    Concretely, we test the following:</span>
<span class="sd">    - test_schema: if the operator&#39;s schema is correct.</span>
<span class="sd">    - test_autograd_registration: if autograd was registered correctly.</span>
<span class="sd">    - test_faketensor: If the operator has a FakeTensor kernel</span>
<span class="sd">    (and if it is correct). The FakeTensor kernel is necessary (</span>
<span class="sd">    but not sufficient) for the operator to work with PyTorch compilation</span>
<span class="sd">    APIs (torch.compile/export/FX).</span>
<span class="sd">    - test_aot_dispatch_dynamic: If the operator has correct behavior</span>
<span class="sd">    with PyTorch compilation APIs (torch.compile/export/FX).</span>
<span class="sd">    This checks that the outputs (and gradients, if applicable) are the</span>
<span class="sd">    same under eager-mode PyTorch and torch.compile.</span>
<span class="sd">    This test is a superset of ``test_faketensor``.</span>

<span class="sd">    For best results, please call ``opcheck`` multiple times with a</span>
<span class="sd">    representative set of inputs. If your operator supports</span>
<span class="sd">    autograd, please use ``opcheck`` with inputs with ``requires_grad = True``;</span>
<span class="sd">    if your operator supports multiple devices (e.g. CPU and CUDA), please</span>
<span class="sd">    use ``opcheck`` with inputs on all supported devices.</span>

<span class="sd">    Args:</span>
<span class="sd">        op: The operator. Must either be a function decorated with</span>
<span class="sd">            :func:`torch.library.custom_op` or an OpOverload/OpOverloadPacket</span>
<span class="sd">            found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)</span>
<span class="sd">        args: The args to the operator</span>
<span class="sd">        kwargs: The kwargs to the operator</span>
<span class="sd">        test_utils: Tests that we should run. Default: all of them.</span>
<span class="sd">            Example: (&quot;test_schema&quot;, &quot;test_faketensor&quot;)</span>
<span class="sd">        raise_exception: If we should raise an exception on the first</span>
<span class="sd">            error. If False, we will return a dict with information</span>
<span class="sd">            on if each test passed or not.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        opcheck and :func:`torch.autograd.gradcheck` test different things;</span>
<span class="sd">        opcheck tests if your usage of torch.library APIs is correct while</span>
<span class="sd">        :func:`torch.autograd.gradcheck` tests if your autograd formula is</span>
<span class="sd">        mathematically correct. Use both to test custom ops that support</span>
<span class="sd">        gradient computation.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::numpy_mul&quot;, mutates_args=())</span>
<span class="sd">        &gt;&gt;&gt; def numpy_add(x: Tensor, y: float) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.numpy(force=True)</span>
<span class="sd">        &gt;&gt;&gt;     z_np = x_np + y</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(z_np).to(x.device)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; @numpy_sin.register_fake</span>
<span class="sd">        &gt;&gt;&gt; def _(x, y):</span>
<span class="sd">        &gt;&gt;&gt;     return torch.empty_like(x)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def setup_context(ctx, inputs, output):</span>
<span class="sd">        &gt;&gt;&gt;     y, = inputs</span>
<span class="sd">        &gt;&gt;&gt;     ctx.y = y</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def backward(ctx, grad):</span>
<span class="sd">        &gt;&gt;&gt;     return grad * ctx.y, None</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; numpy_sin.register_autograd(backward, setup_context=setup_context)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; sample_inputs = [</span>
<span class="sd">        &gt;&gt;&gt;     (torch.randn(3), 3.14),</span>
<span class="sd">        &gt;&gt;&gt;     (torch.randn(2, 3, device=&#39;cuda&#39;), 2.718),</span>
<span class="sd">        &gt;&gt;&gt;     (torch.randn(1, 10, requires_grad=True), 1.234),</span>
<span class="sd">        &gt;&gt;&gt;     (torch.randn(64, 64, device=&#39;cuda&#39;, requires_grad=True), 90.18),</span>
<span class="sd">        &gt;&gt;&gt; ]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; for args in sample_inputs:</span>
<span class="sd">        &gt;&gt;&gt;     torch.library.opcheck(foo, args)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">torch.testing._internal.optests</span> <span class="k">as</span> <span class="nn">optests</span>
    <span class="k">return</span> <span class="n">optests</span><span class="o">.</span><span class="n">opcheck</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">test_utils</span><span class="o">=</span><span class="n">test_utils</span><span class="p">,</span> <span class="n">raise_exception</span><span class="o">=</span><span class="n">raise_exception</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/sphinx_highlight.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>