


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.fx.experimental.proxy_tensor &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/fx/experimental/proxy_tensor.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.5.0a0+git4574301 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/fx/experimental/proxy_tensor.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/get_start_xpu.html">Pytorch 2.4: Getting Started on Intel GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_environment_variables.html">Torch Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.fx.experimental.proxy_tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.fx.experimental.proxy_tensor</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-decorators</span>
<span class="c1"># Copyright (c) Facebook, Inc. and its affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">typing</span>
<span class="kn">import</span> <span class="nn">typing_extensions</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">weakref</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">ExitStack</span><span class="p">,</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Generator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">overload</span><span class="p">,</span>
    <span class="n">Protocol</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Concatenate</span><span class="p">,</span> <span class="n">ParamSpec</span><span class="p">,</span> <span class="n">Self</span>
<span class="kn">from</span> <span class="nn">weakref</span> <span class="kn">import</span> <span class="n">WeakKeyDictionary</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch._ops</span>
<span class="kn">import</span> <span class="nn">torch.fx</span> <span class="k">as</span> <span class="nn">fx</span>
<span class="kn">import</span> <span class="nn">torch.fx.traceback</span> <span class="k">as</span> <span class="nn">fx_traceback</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">SymBool</span><span class="p">,</span> <span class="n">SymInt</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._dispatch.python</span> <span class="kn">import</span> <span class="n">enable_python_dispatcher</span>
<span class="kn">from</span> <span class="nn">torch._library.fake_class_registry</span> <span class="kn">import</span> <span class="n">FakeScriptObject</span>
<span class="kn">from</span> <span class="nn">torch._subclasses.fake_impls</span> <span class="kn">import</span> <span class="n">fast_detach</span>
<span class="kn">from</span> <span class="nn">torch._subclasses.fake_tensor</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FakeTensor</span><span class="p">,</span>
    <span class="n">FakeTensorMode</span><span class="p">,</span>
    <span class="n">is_fake</span><span class="p">,</span>
    <span class="n">unset_fake_temporarily</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch._subclasses.meta_utils</span> <span class="kn">import</span> <span class="n">is_sparse_any</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">GraphModule</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">,</span> <span class="n">Tracer</span>
<span class="kn">from</span> <span class="nn">torch.fx.graph_module</span> <span class="kn">import</span> <span class="n">_assign_attr</span>
<span class="kn">from</span> <span class="nn">torch.fx.node</span> <span class="kn">import</span> <span class="n">_side_effectful_need_to_be_preserved_pre_dispatch</span>
<span class="kn">from</span> <span class="nn">torch.fx.passes.shape_prop</span> <span class="kn">import</span> <span class="n">_extract_tensor_metadata</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">TorchFunctionMode</span>
<span class="kn">from</span> <span class="nn">torch.utils._python_dispatch</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_disable_infra_mode</span><span class="p">,</span>
    <span class="n">_push_mode</span><span class="p">,</span>
    <span class="n">_unset_infra_mode</span><span class="p">,</span>
    <span class="n">TorchDispatchMode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.utils._stats</span> <span class="kn">import</span> <span class="n">count</span>
<span class="kn">from</span> <span class="nn">torch.utils._thunk</span> <span class="kn">import</span> <span class="n">Thunk</span>
<span class="kn">from</span> <span class="nn">torch.utils._traceback</span> <span class="kn">import</span> <span class="n">CapturedTraceback</span>
<span class="kn">from</span> <span class="nn">torch.utils.weak</span> <span class="kn">import</span> <span class="n">_WeakHashRef</span><span class="p">,</span> <span class="n">WeakIdKeyDictionary</span><span class="p">,</span> <span class="n">WeakTensorKeyDictionary</span>

<span class="kn">from</span> <span class="nn">._backward_state</span> <span class="kn">import</span> <span class="n">BackwardState</span>
<span class="kn">from</span> <span class="nn">.sym_node</span> <span class="kn">import</span> <span class="n">SymNode</span>


<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">types</span>
    <span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">MutableMapping</span>

    <span class="kn">import</span> <span class="nn">sympy</span>

    <span class="kn">from</span> <span class="nn">torch._ops</span> <span class="kn">import</span> <span class="n">OpOverload</span>
    <span class="kn">from</span> <span class="nn">torch.fx._symbolic_trace</span> <span class="kn">import</span> <span class="n">PHBase</span>
    <span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">IntLikeType</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;PythonKeyTracer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dispatch_trace&quot;</span><span class="p">,</span>
    <span class="s2">&quot;make_fx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DecompositionInterpreter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;py_sym_types&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_innermost_proxy_mode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_proxy_mode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;handle_sym_dispatch&quot;</span><span class="p">,</span>
    <span class="s2">&quot;maybe_enable_thunkify&quot;</span><span class="p">,</span>
    <span class="s2">&quot;maybe_disable_thunkify&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">_ProxyTracer</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;PythonKeyTracer&quot;</span><span class="p">,</span> <span class="s2">&quot;_GraphAppendingTracerEx&quot;</span><span class="p">]</span>

<span class="n">_AnyScriptObject</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ScriptObject</span><span class="p">,</span> <span class="n">FakeScriptObject</span><span class="p">)</span>
<span class="n">_AnyScriptObjectType</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ScriptObject</span><span class="p">,</span> <span class="n">FakeScriptObject</span><span class="p">]</span>

<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>
<span class="n">prim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">prim</span>

<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">not_implemented_log</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">getArtifactLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;not_implemented&quot;</span><span class="p">)</span>

<span class="n">CURRENT_DECOMPOSITION_TABLE</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">CONSTANT_NUMEL_LIMIT</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">)</span>
<span class="n">_P</span> <span class="o">=</span> <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;_P&quot;</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">)</span>

<span class="n">null_ctx_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">nullcontext</span><span class="p">)</span>
<span class="c1"># We currently convert all SymInt to proxies before we use them.</span>
<span class="c1"># This could plausibly be handled at the Dynamo level.</span>
<span class="n">pytree</span><span class="o">.</span><span class="n">register_pytree_node</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">xs</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
    <span class="k">lambda</span> <span class="n">xs</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span>
    <span class="n">flatten_with_keys_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">xs</span><span class="p">:</span> <span class="p">(</span>
        <span class="p">[(</span><span class="n">pytree</span><span class="o">.</span><span class="n">SequenceKey</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">xs</span><span class="p">)],</span>
        <span class="kc">None</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">fake_signature</span><span class="p">(</span><span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">],</span> <span class="n">nargs</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;FX gets confused by varargs, de-confuse it&quot;&quot;&quot;</span>
    <span class="n">argnames</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;arg</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nargs</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">eval</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;lambda </span><span class="si">{</span><span class="n">argnames</span><span class="si">}</span><span class="s2">: fn(</span><span class="si">{</span><span class="n">argnames</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;fn&quot;</span><span class="p">:</span> <span class="n">fn</span><span class="p">})</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">decompose</span><span class="p">(</span>
    <span class="n">decomposition_table</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="k">global</span> <span class="n">CURRENT_DECOMPOSITION_TABLE</span>
    <span class="n">old_decomposition_table</span> <span class="o">=</span> <span class="n">CURRENT_DECOMPOSITION_TABLE</span>
    <span class="n">CURRENT_DECOMPOSITION_TABLE</span> <span class="o">=</span> <span class="n">decomposition_table</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">CURRENT_DECOMPOSITION_TABLE</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">CURRENT_DECOMPOSITION_TABLE</span> <span class="o">=</span> <span class="n">old_decomposition_table</span>


<span class="c1"># ensure we cannot collide with other properties</span>
<span class="n">proxy_slot</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_NoDefault</span><span class="p">:</span>
    <span class="k">pass</span>


<span class="n">no_default</span> <span class="o">=</span> <span class="n">_NoDefault</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">py_sym_types</span><span class="p">,</span> <span class="n">PySymType</span>


<span class="k">class</span> <span class="nc">_HasMeta</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PySymType</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">is_sym_node</span><span class="p">(</span><span class="n">node</span><span class="p">:</span> <span class="n">_HasMeta</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">),</span> <span class="s2">&quot;All nodes traced with proxy_tensor should have meta&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;val&quot;</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">],</span> <span class="n">py_sym_types</span><span class="p">)</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">set_proxy_slot</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">proxy</span><span class="p">:</span> <span class="n">_ProxyTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">set_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">_AnyScriptObjectType</span><span class="p">,</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">proxy</span><span class="p">:</span> <span class="n">Proxy</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">set_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">,</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">proxy</span><span class="p">:</span> <span class="n">_PySymProxyType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="o">...</span>


<span class="k">def</span> <span class="nf">set_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PySymType</span><span class="p">,</span> <span class="n">_AnyScriptObjectType</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">proxy</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;set_proxy_slot </span><span class="si">%s</span><span class="s2"> (</span><span class="si">%s</span><span class="s2">) </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="nb">id</span><span class="p">(</span><span class="n">obj</span><span class="p">),</span> <span class="n">proxy</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># We DO want to clobber proxies whenever we run an inplace operation</span>
        <span class="c1"># on a tensor, and it affects the metadata on the proxy.</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">)</span>
        <span class="n">tracer</span><span class="o">.</span><span class="n">tensor_tracker</span><span class="p">[</span><span class="n">obj</span><span class="p">]</span> <span class="o">=</span> <span class="n">proxy</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="n">_AnyScriptObject</span><span class="p">)):</span>
        <span class="c1"># We DO want to clobber proxies, with a similar rationale as for tensors.</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">)</span>
        <span class="n">tracer</span><span class="o">.</span><span class="n">script_object_tracker</span><span class="p">[</span><span class="n">obj</span><span class="p">]</span> <span class="o">=</span> <span class="n">proxy</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># NB: Never clobber pre-existing proxy.  Although the proxies</span>
        <span class="c1"># are in principle equivalent, when we do graph partitioning</span>
        <span class="c1"># we need there not to be spurious dependencies on tangent inputs.</span>
        <span class="c1"># This works because primals get their SymInts set first, and</span>
        <span class="c1"># THEN later we allocate tangent inputs.  Make sure if a SymInt</span>
        <span class="c1"># is derivable from a primal that we use that.</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">obj</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tracer</span><span class="o">.</span><span class="n">symnode_tracker</span><span class="p">:</span>
            <span class="n">tracer</span><span class="o">.</span><span class="n">symnode_tracker</span><span class="p">[</span><span class="n">obj</span><span class="p">]</span> <span class="o">=</span> <span class="n">typing</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">_PySymProxyType</span><span class="p">,</span> <span class="n">proxy</span><span class="p">)</span>

            <span class="c1"># WAR: python test/dynamo/test_subclasses.py</span>
            <span class="c1"># TestNestedTensor.test_basic_autograd</span>
            <span class="c1">#</span>
            <span class="c1"># AOTAutograd doesn&#39;t pass the &quot;outer sizes&quot; as an actual argument</span>
            <span class="c1"># to make_fx, but it is made use of internally in AOTAutograd&#39;s</span>
            <span class="c1"># call to tensor unflatten.  Because the outer sizes isn&#39;t passed</span>
            <span class="c1"># as an argument, it is therefore untracked.  However, it turns</span>
            <span class="c1"># out you luck out, because *Dynamo* will manually add the outer</span>
            <span class="c1"># sizes as an argument so you can fix up the proxy&#39;ness.</span>
            <span class="c1">#</span>
            <span class="c1"># This is probably fixed in</span>
            <span class="c1"># https://github.com/pytorch/pytorch/pull/125941/</span>
            <span class="kn">import</span> <span class="nn">sympy</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="p">,</span> <span class="n">sympy</span><span class="o">.</span><span class="n">Symbol</span><span class="p">):</span>
                <span class="n">tracer</span><span class="o">.</span><span class="n">sympy_expr_tracker</span><span class="p">[</span><span class="n">obj</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="p">]</span> <span class="o">=</span> <span class="n">proxy</span>


<span class="k">def</span> <span class="nf">has_proxy_slot</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">SymNode</span><span class="p">)),</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">True</span><span class="p">))</span>


<span class="n">_PySymProxyType</span> <span class="o">=</span> <span class="n">Thunk</span><span class="p">[</span><span class="n">Proxy</span><span class="p">]</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_ProxyTensor</span><span class="p">:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">default</span><span class="p">:</span> <span class="n">U</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_ProxyTensor</span><span class="p">,</span> <span class="n">U</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">default</span><span class="p">:</span> <span class="n">U</span><span class="p">,</span>
    <span class="n">transform</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">_ProxyTensor</span><span class="p">],</span> <span class="n">R</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">R</span><span class="p">,</span> <span class="n">U</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">_AnyScriptObjectType</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Proxy</span><span class="p">:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">_AnyScriptObjectType</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">default</span><span class="p">:</span> <span class="n">U</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Proxy</span><span class="p">,</span> <span class="n">U</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">_AnyScriptObjectType</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">default</span><span class="p">:</span> <span class="n">U</span><span class="p">,</span>
    <span class="n">transform</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Proxy</span><span class="p">],</span> <span class="n">R</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">R</span><span class="p">,</span> <span class="n">U</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_PySymProxyType</span><span class="p">:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">default</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">_PySymProxyType</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">,</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">default</span><span class="p">:</span> <span class="n">U</span><span class="p">,</span>
    <span class="n">transform</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">_PySymProxyType</span><span class="p">],</span> <span class="n">R</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">R</span><span class="p">,</span> <span class="n">U</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="c1"># the default argument is what to return if the slot is not set.</span>
<span class="c1"># the transform argument is handy if you need to extract a subfield from</span>
<span class="c1"># the successfully looked up result (but NOT the default.)</span>
<span class="k">def</span> <span class="nf">get_proxy_slot</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_AnyScriptObjectType</span><span class="p">,</span> <span class="n">PySymType</span><span class="p">],</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
    <span class="n">default</span><span class="p">:</span> <span class="nb">object</span> <span class="o">=</span> <span class="n">no_default</span><span class="p">,</span>
    <span class="n">transform</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="n">tracker</span><span class="p">:</span> <span class="n">Any</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">tracker</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">tensor_tracker</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">_AnyScriptObject</span><span class="p">):</span>
        <span class="n">tracker</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">script_object_tracker</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
        <span class="n">tracker</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">symnode_tracker</span>

    <span class="k">if</span> <span class="n">obj</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tracker</span><span class="p">:</span>
        <span class="c1"># Last ditch</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span> <span class="ow">in</span> <span class="n">tracer</span><span class="o">.</span><span class="n">sympy_expr_tracker</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">sympy_expr_tracker</span><span class="p">[</span><span class="n">obj</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default</span><span class="p">,</span> <span class="n">_NoDefault</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">obj</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="nb">id</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="si">}</span><span class="s2">)is not tracked with proxy for </span><span class="si">{</span><span class="n">tracer</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">default</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">tracker</span><span class="p">[</span><span class="n">obj</span><span class="p">]</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">snapshot_fake</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="c1"># val.detach() will also eventually call fast_detach(),</span>
    <span class="c1"># but this saves us a full trip into __torch_dispatch__</span>
    <span class="c1"># (snapshot_fake is called a lot)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">fast_detach</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">fake_mode</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">val</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>


<span class="n">_ExtractValType</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span>
    <span class="n">Union</span><span class="p">[</span>
        <span class="n">PySymType</span><span class="p">,</span>
        <span class="n">_AnyScriptObjectType</span><span class="p">,</span>
        <span class="n">BackwardState</span><span class="p">,</span>
        <span class="n">List</span><span class="p">[</span><span class="s2">&quot;_ExtractValType&quot;</span><span class="p">],</span>
        <span class="n">Tuple</span><span class="p">[</span><span class="s2">&quot;_ExtractValType&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;_ExtractValType&quot;</span><span class="p">],</span>
        <span class="n">Tensor</span><span class="p">,</span>
        <span class="nb">int</span><span class="p">,</span>
        <span class="nb">float</span><span class="p">,</span>
        <span class="nb">bool</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">]</span>


<span class="k">def</span> <span class="nf">extract_val</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="n">_ExtractValType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_ExtractValType</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">is_fake</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">snapshot_fake</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">val</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">_AnyScriptObject</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">val</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">BackwardState</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">val</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">val</span><span class="o">.</span><span class="vm">__class__</span><span class="p">([</span><span class="n">extract_val</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">val</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">extract_val</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">val</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">val</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="c1"># NB: Kinda hacky, but we should try to get val as the metadata</span>
            <span class="c1"># everywhere</span>
            <span class="c1"># TODO: This doesn&#39;t properly track storages.  A more robust</span>
            <span class="c1"># approach would be to maintain a per-trace FakeTensorMode and</span>
            <span class="c1"># from_real_tensor to create fake values (don&#39;t forget to</span>
            <span class="c1"># snapshot_fake)</span>
            <span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">allow_fallback_kernels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">fake_tensor_mode</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_strided</span><span class="p">(</span>
                    <span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">val</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">val</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">val</span><span class="o">.</span><span class="n">dtype</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">val</span>
    <span class="k">elif</span> <span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">typing_extensions</span><span class="o">.</span><span class="n">assert_never</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">_enable_thunkify</span><span class="p">(</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">enable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enable thunkification inside the context manager.  Thunkification prevents</span>
<span class="sd">    SymNode computation from directly being traced into an FX graph; instead,</span>
<span class="sd">    the compute is only added to the graph if it is actually used.  This helps</span>
<span class="sd">    us track SymNode compute when it is computed (since we need /something/</span>
<span class="sd">    to put in the tracker) even if it is unlikely to be used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">old</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">enable_thunkify</span>
    <span class="n">tracer</span><span class="o">.</span><span class="n">enable_thunkify</span> <span class="o">=</span> <span class="n">enable</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">tracer</span><span class="o">.</span><span class="n">enable_thunkify</span> <span class="o">=</span> <span class="n">old</span>


<div class="viewcode-block" id="maybe_disable_thunkify"><a class="viewcode-back" href="../../../../generated/torch.fx.experimental.proxy_tensor.maybe_disable_thunkify.html#torch.fx.experimental.proxy_tensor.maybe_disable_thunkify">[docs]</a><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">maybe_disable_thunkify</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Within a context, disable thunkification.  See :func:`maybe_enable_thunkify`</span>
<span class="sd">    for more details.  This is helpful if you have a wrapper function which</span>
<span class="sd">    you want to enable thunkification on, but in some segment on the inside (say,</span>
<span class="sd">    the original user function), you want to disable thunkification as you know</span>
<span class="sd">    it is not needed there.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">proxy_mode</span> <span class="o">=</span> <span class="n">get_proxy_mode</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">proxy_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">_enable_thunkify</span><span class="p">(</span><span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="p">,</span> <span class="n">enable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="k">yield</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span></div>


<div class="viewcode-block" id="maybe_enable_thunkify"><a class="viewcode-back" href="../../../../generated/torch.fx.experimental.proxy_tensor.maybe_enable_thunkify.html#torch.fx.experimental.proxy_tensor.maybe_enable_thunkify">[docs]</a><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">maybe_enable_thunkify</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Within this context manager, if you are doing make_fx tracing, we will thunkify</span>
<span class="sd">    all SymNode compute and avoid tracing it into the graph unless it is actually needed.</span>
<span class="sd">    You should prefer to avoid using this as much as possible, as lazy evaluation of</span>
<span class="sd">    SymNode tracing can lead to long chains of thunks which will stack overflow</span>
<span class="sd">    if you evaluate them.  However, this is currently sometimes necessary as there</span>
<span class="sd">    are buggy parts of PT2 which will fail with &quot;s0 is not tracked with proxy&quot; error</span>
<span class="sd">    due to insufficient tracing of SymNode computation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">proxy_mode</span> <span class="o">=</span> <span class="n">get_proxy_mode</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">proxy_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">_enable_thunkify</span><span class="p">(</span><span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="p">):</span>
            <span class="k">yield</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span></div>


<span class="c1"># Note [invariants for node meta &#39;val&#39;]</span>
<span class="c1"># What invariants do we have for the &#39;val&#39; set on the FX node?  It has accurate</span>
<span class="c1"># metadata... but only for metadata that exists &quot;below&quot; all other subsystems</span>
<span class="c1"># (most notably autograd, but also vmap, functorch transforms, etc).  This means</span>
<span class="c1"># you can get the dtype, shape, stride, storage, but you CANNOT get requires_grad,</span>
<span class="c1"># grad_fn, _base (_base actually may be set due to recursive call to</span>
<span class="c1"># ADInplaceOrView, but you shouldn&#39;t rely on it.)</span>
<span class="k">def</span> <span class="nf">set_meta</span><span class="p">(</span><span class="n">proxy</span><span class="p">:</span> <span class="n">Proxy</span><span class="p">,</span> <span class="n">val</span><span class="p">:</span> <span class="n">_ExtractValType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Proxy</span><span class="p">:</span>
    <span class="n">proxy</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">extract_val</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">_enable_thunkify</span><span class="p">(</span><span class="n">proxy</span><span class="o">.</span><span class="n">tracer</span><span class="p">):</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="c1"># Best effort tensor_meta setting; prefer using val!</span>
        <span class="k">if</span> <span class="n">is_fake</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
            <span class="n">proxy</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;tensor_meta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_extract_tensor_metadata</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">val</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">proxy</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;tensor_meta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_extract_tensor_metadata</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">proxy</span>


<span class="k">def</span> <span class="nf">thunkify</span><span class="p">(</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Thunk</span><span class="p">[</span><span class="n">R</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Delays computation of f until it&#39;s called again</span>
<span class="sd">    Also caches the result</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">tracer</span><span class="o">.</span><span class="n">enable_thunkify</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Thunk</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Thunk</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">r</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">track_tensor</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">proxy</span><span class="p">:</span> <span class="n">Proxy</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">try_set_proxy_slot</span><span class="p">(</span>
        <span class="n">outer_s</span><span class="p">:</span> <span class="n">IntLikeType</span><span class="p">,</span>
        <span class="n">proxy_callable</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">Concatenate</span><span class="p">[</span><span class="n">PySymType</span><span class="p">,</span> <span class="n">_P</span><span class="p">],</span> <span class="n">Proxy</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">callable</span><span class="p">(</span><span class="n">proxy_callable</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outer_s</span><span class="p">,</span> <span class="n">SymInt</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">_enable_thunkify</span><span class="p">(</span><span class="n">tracer</span><span class="p">):</span>
                <span class="n">set_proxy_slot</span><span class="p">(</span>
                    <span class="n">outer_s</span><span class="p">,</span>
                    <span class="n">tracer</span><span class="p">,</span>
                    <span class="n">thunkify</span><span class="p">(</span><span class="n">tracer</span><span class="p">,</span> <span class="n">proxy_callable</span><span class="p">,</span> <span class="n">outer_s</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
                <span class="p">)</span>

    <span class="c1"># The basic idea is that we need to associate each tensor/SymInt</span>
    <span class="c1"># with a Proxy.  How do we setup this association?  We just store</span>
    <span class="c1"># the proxy on the proxy slot of the object, keyed on the tracer</span>
    <span class="c1"># (so that if we have multiple tracers at the same time, they</span>
    <span class="c1"># don&#39;t clobber each other.)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">try_set_proxy_slot</span><span class="p">(</span>
            <span class="n">s</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">set_meta</span><span class="p">(</span>
                <span class="n">tracer</span><span class="o">.</span><span class="n">create_proxy</span><span class="p">(</span>
                    <span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sym_size</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="p">{}</span>
                <span class="p">),</span>
                <span class="n">x</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">i</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_sparse_any</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">()):</span>
            <span class="n">try_set_proxy_slot</span><span class="p">(</span>
                <span class="n">s</span><span class="p">,</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">set_meta</span><span class="p">(</span>
                    <span class="n">tracer</span><span class="o">.</span><span class="n">create_proxy</span><span class="p">(</span>
                        <span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sym_stride</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="p">{}</span>
                    <span class="p">),</span>
                    <span class="n">x</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="n">i</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="n">try_set_proxy_slot</span><span class="p">(</span>
        <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">set_meta</span><span class="p">(</span>
            <span class="n">tracer</span><span class="o">.</span><span class="n">create_proxy</span><span class="p">(</span>
                <span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sym_numel</span><span class="o">.</span><span class="n">default</span><span class="p">,</span> <span class="p">(</span><span class="n">proxy</span><span class="p">,),</span> <span class="p">{}</span>
            <span class="p">),</span>
            <span class="n">x</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_sparse_any</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
        <span class="n">try_set_proxy_slot</span><span class="p">(</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">set_meta</span><span class="p">(</span>
                <span class="n">tracer</span><span class="o">.</span><span class="n">create_proxy</span><span class="p">(</span>
                    <span class="s2">&quot;call_function&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sym_storage_offset</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">proxy</span><span class="p">,),</span>
                    <span class="p">{},</span>
                <span class="p">),</span>
                <span class="n">x</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="n">set_proxy_slot</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">constant</span><span class="p">))</span>


<span class="n">_NestedProxys</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span>
    <span class="n">Proxy</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="s2">&quot;_NestedProxys&quot;</span><span class="p">],</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="s2">&quot;_NestedProxys&quot;</span><span class="p">]</span>
<span class="p">]</span>
<span class="n">_NestedTensors</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span>
    <span class="n">Tensor</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="s2">&quot;_NestedTensors&quot;</span><span class="p">],</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="s2">&quot;_NestedTensors&quot;</span><span class="p">]</span>
<span class="p">]</span>


<span class="k">def</span> <span class="nf">track_tensor_tree</span><span class="p">(</span>
    <span class="n">inner_res</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span>
    <span class="n">proxy_res</span><span class="p">:</span> <span class="n">_NestedProxys</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_NestedTensors</span><span class="p">],</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">wrap_with_proxy</span><span class="p">(</span>
        <span class="n">e</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">proxy</span><span class="p">:</span> <span class="n">_NestedProxys</span><span class="p">,</span> <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_NestedTensors</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">constant</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
            <span class="n">track_tensor</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">proxy</span><span class="p">,</span> <span class="n">tracer</span><span class="o">=</span><span class="n">tracer</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">)</span>
            <span class="n">set_meta</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
            <span class="n">_set_unbacked_bindings</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">proxy</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">)</span>
            <span class="c1"># NB: eagerly set meta here, so that the numbering is in order</span>
            <span class="n">set_meta</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
            <span class="n">set_proxy_slot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="n">thunkify</span><span class="p">(</span><span class="n">tracer</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">proxy</span><span class="p">))</span>
            <span class="n">_set_unbacked_bindings</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">proxy</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">_AnyScriptObject</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">)</span>
            <span class="n">set_proxy_slot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="n">proxy</span><span class="p">)</span>
            <span class="n">set_meta</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="c1"># example use case: allreduce_ returns ([tensor], work)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">):</span>
                <span class="n">set_meta</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">get_constant</span><span class="p">(</span>
                <span class="n">c</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_NestedTensors</span><span class="p">],</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span>
            <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_NestedTensors</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                    <span class="k">return</span> <span class="n">c</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ee</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="c1"># Use an indexer here - if proxy is a List then it will unwrap</span>
                <span class="c1"># it. If it&#39;s a Proxy then it will proxy the getelem.</span>
                <span class="n">wrap_with_proxy</span><span class="p">(</span><span class="n">ee</span><span class="p">,</span> <span class="n">proxy</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">get_constant</span><span class="p">(</span><span class="n">constant</span><span class="p">,</span> <span class="n">idx</span><span class="p">))</span>  <span class="c1"># type: ignore[index]</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="c1"># example use case: triton_kernel_wrapper takes arguments as kwargs</span>

            <span class="c1"># In theory we could support const-prop when proxy-tensor-tracing</span>
            <span class="c1"># operators that returns dicts of tensors, but we have no use case</span>
            <span class="c1"># for it today (since the only op we currently trace that can</span>
            <span class="c1"># return a dict is triton_kernel_wrapper_functional/mutation,</span>
            <span class="c1"># which does not participate in const-prop)</span>
            <span class="k">assert</span> <span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">):</span>
                <span class="n">set_meta</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">wrap_with_proxy</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">proxy</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore[index]</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">BackwardState</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">)</span>
            <span class="n">set_meta</span><span class="p">(</span><span class="n">proxy</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
            <span class="n">e</span><span class="o">.</span><span class="n">proxy</span> <span class="o">=</span> <span class="n">proxy</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># intentionally pass on primitives</span>
            <span class="k">pass</span>

    <span class="n">wrap_with_proxy</span><span class="p">(</span><span class="n">inner_res</span><span class="p">,</span> <span class="n">proxy_res</span><span class="p">,</span> <span class="n">constant</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inner_res</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">_ProxyTensor</span><span class="p">:</span>
    <span class="n">proxy</span><span class="p">:</span> <span class="n">Proxy</span>
    <span class="n">constant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">fetch_sym_proxy</span><span class="p">(</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">PySymType</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">]]:</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="n">e</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">]:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">node</span>
        <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">constant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">n</span><span class="o">.</span><span class="n">constant</span>
        <span class="k">if</span> <span class="n">e</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="o">.</span><span class="n">is_number</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">SymBool</span><span class="p">):</span>
                <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">SymInt</span><span class="p">):</span>
                <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">expr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">)</span>
            <span class="c1"># NB: we REQUIRE all symints to be tracked</span>
            <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">tracer</span><span class="p">)</span><span class="o">.</span><span class="n">force</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">inner</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">fetch_object_proxy</span><span class="p">(</span><span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_ProxyTensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">fetch_object_proxy</span><span class="p">(</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">_AnyScriptObjectType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Proxy</span><span class="p">,</span> <span class="n">_AnyScriptObjectType</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">fetch_object_proxy</span><span class="p">(</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">PySymType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_PySymProxyType</span><span class="p">,</span> <span class="n">PySymType</span><span class="p">]:</span>
    <span class="o">...</span>


<span class="k">def</span> <span class="nf">fetch_object_proxy</span><span class="p">(</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_AnyScriptObjectType</span><span class="p">,</span> <span class="n">PySymType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>


<span class="n">HANDLED_TYPES</span> <span class="o">=</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_maybe_record_pointwise_barrier</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">proxy_mode</span><span class="p">:</span> <span class="n">ProxyTorchDispatchMode</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Records pointwise operators in user program (non decomposed) that were output in fp16/bf16</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">proxy_mode</span><span class="o">.</span><span class="n">decomp_layers</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">proxy_mode</span><span class="o">.</span><span class="n">emulate_precision_casts</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tag</span><span class="o">.</span><span class="n">pointwise</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">func</span><span class="o">.</span><span class="n">tags</span>
    <span class="p">):</span>
        <span class="k">return</span>

    <span class="n">last_node</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">)))</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">last_node</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">return</span>

    <span class="n">last_node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;low_precision_pointwise_barrier&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">proxy_call</span><span class="p">(</span>
    <span class="n">proxy_mode</span><span class="p">:</span> <span class="n">ProxyTorchDispatchMode</span><span class="p">,</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">,</span>
    <span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="n">unrecognized_types</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Type</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">flat_args_kwargs</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">can_handle_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="n">HANDLED_TYPES</span> <span class="ow">or</span> <span class="n">has_proxy_slot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">proxy_mode</span><span class="o">.</span><span class="n">_allow_fake_constant</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">,)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">r</span><span class="p">:</span>
            <span class="n">unrecognized_types</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">r</span>

    <span class="c1"># If there are any tensor subclasses, we need to handle those tensor subclasses first</span>
    <span class="c1"># TODO: we could use types to test this</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">can_handle_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args_kwargs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="n">not_implemented_log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;ProxyTensorMode tensors without proxy had unrecognized subclasses: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">unrecognized_types</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">NotImplemented</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">maybe_handle_decomp</span><span class="p">(</span><span class="n">proxy_mode</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">r</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">NotImplemented</span><span class="p">:</span>
        <span class="n">_maybe_record_pointwise_barrier</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">proxy_mode</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">r</span>

    <span class="c1"># For pre-autograd tracing, we do not want to run CompositeImplicit decomps.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">pre_dispatch</span> <span class="ow">and</span> <span class="n">func</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">size</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">stride</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">storage_offset</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="k">with</span> <span class="n">proxy_mode</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">decompose</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">r</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">NotImplemented</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">r</span>

    <span class="n">tracer</span> <span class="o">=</span> <span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span>
    <span class="n">f_flat_args_kwargs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span>
            <span class="n">fetch_object_proxy</span><span class="p">(</span><span class="n">tracer</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_AnyScriptObject</span><span class="p">))</span>
            <span class="k">else</span> <span class="n">x</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args_kwargs</span>
    <span class="p">]</span>

    <span class="c1"># If there are SymInts, we also should not consider this constant.</span>
    <span class="c1"># However, fake tensor handling of SymInts is sufficiently broken that</span>
    <span class="c1"># I couldn&#39;t write a test for this case</span>
    <span class="n">all_constant</span> <span class="o">=</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">t</span><span class="o">.</span><span class="n">constant</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">f_flat_args_kwargs</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># TODO: maybe constant SymInts should also be allowed?  Not sure if</span>
        <span class="c1"># this can happen</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args_kwargs</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tag</span><span class="o">.</span><span class="n">data_dependent_output</span> <span class="ow">in</span> <span class="n">func</span><span class="o">.</span><span class="n">tags</span><span class="p">:</span>
        <span class="c1"># Check if all of the Tensor inputs are constants</span>
        <span class="k">if</span> <span class="n">all_constant</span><span class="p">:</span>
            <span class="n">const_flat_args_kwargs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">t</span><span class="o">.</span><span class="n">constant</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">f_flat_args_kwargs</span>
            <span class="p">]</span>
            <span class="n">const_args</span><span class="p">,</span> <span class="n">const_kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span>
                <span class="n">const_flat_args_kwargs</span><span class="p">,</span> <span class="n">spec</span>
            <span class="p">)</span>
            <span class="k">with</span> <span class="n">unset_fake_temporarily</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">const_args</span><span class="p">,</span> <span class="o">**</span><span class="n">const_kwargs</span><span class="p">)</span>
        <span class="c1"># If any of the Tensor inputs are &quot;real&quot; (not FakeTensor), we may</span>
        <span class="c1"># incorrectly burn in constants by allowing this access.  Raise</span>
        <span class="c1"># an error in this case</span>
        <span class="k">if</span> <span class="n">proxy_mode</span><span class="o">.</span><span class="n">_error_on_data_dependent_ops</span> <span class="ow">and</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_all_only</span><span class="p">(</span>
            <span class="n">Tensor</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="ow">not</span> <span class="n">is_fake</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;It appears that you&#39;re trying to get value out of a tracing tensor with </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2"> - erroring out! &quot;</span>
                <span class="s2">&quot;It&#39;s likely that this is caused by data-dependent control flow or similar.  &quot;</span>
                <span class="s2">&quot;It may be possible to trace this with dynamic shapes; try setting tracing_mode=&#39;symbolic&#39; &quot;</span>
                <span class="s2">&quot;in your make_fx call.&quot;</span>
            <span class="p">)</span>

    <span class="n">proxy_flat_args_kwargs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">e</span><span class="o">.</span><span class="n">proxy</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">f_flat_args_kwargs</span>
    <span class="p">]</span>
    <span class="n">proxy_flat_args_kwargs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">fetch_sym_proxy</span><span class="p">(</span><span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="p">)(</span><span class="n">e</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">)</span> <span class="k">else</span> <span class="n">e</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">proxy_flat_args_kwargs</span>
    <span class="p">]</span>
    <span class="n">proxy_args</span><span class="p">,</span> <span class="n">proxy_kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">proxy_flat_args_kwargs</span><span class="p">,</span> <span class="n">spec</span><span class="p">)</span>

    <span class="c1"># When we trace through a torch.tensor invocation, you never actually</span>
    <span class="c1"># see a torch.ops.aten.tensor call. Instead, the way this function is</span>
    <span class="c1"># implemented internally is that we allocate a plain tensor (this is</span>
    <span class="c1"># *guaranteed* to be a plain tensor, we disable all modes when doing</span>
    <span class="c1"># so), and then call at::lift_fresh on it (to give modes a chance to do</span>
    <span class="c1"># their stuff).  Furthermore, the tensor argument to lift_fresh is guaranteed</span>
    <span class="c1"># to be freshly allocated, so we want lift_fresh to be a no-op (directly</span>
    <span class="c1"># returning the input argument).</span>
    <span class="c1">#</span>
    <span class="c1"># Here is the basic problem: when we trace this sequence of executions</span>
    <span class="c1"># into an FX graph, what happens to this call sequence?  Traditionally,</span>
    <span class="c1"># tensor constants get interned as buffers on the FX GraphModule.  But</span>
    <span class="c1"># this is dangerous.  Consider:</span>
    <span class="c1">#</span>
    <span class="c1">#       x = torch.tensor(1)</span>
    <span class="c1">#       x.add_(2)</span>
    <span class="c1">#</span>
    <span class="c1"># Naively, this traces into:</span>
    <span class="c1">#</span>
    <span class="c1">#       t = self._tensor_constant0  # initialized to torch.tensor(1)</span>
    <span class="c1">#       x = torch.ops.aten.lift_fresh(t)</span>
    <span class="c1">#       x.add_(2)</span>
    <span class="c1">#</span>
    <span class="c1"># If lift_fresh returns t directly, the subsequent add_ call will</span>
    <span class="c1"># modify the tensor constant. Really, the problem is we&#39;ve violated</span>
    <span class="c1"># the invariant the argument to lift is fresh.  So what we should</span>
    <span class="c1"># preserve the invariant by replacing lift_fresh with lift_fresh_copy:</span>
    <span class="c1">#</span>
    <span class="c1">#       t = self._tensor_constant0  # initialized to torch.tensor(1)</span>
    <span class="c1">#       x = torch.ops.aten.lift_fresh_copy(t)</span>
    <span class="c1">#       x.add_(2)</span>
    <span class="c1">#</span>
    <span class="c1"># This is what the overload modification does.</span>
    <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">lift_fresh</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
        <span class="n">func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">lift_fresh_copy</span><span class="o">.</span><span class="n">default</span>

    <span class="n">proxy_out</span> <span class="o">=</span> <span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">create_proxy</span><span class="p">(</span>
        <span class="s2">&quot;call_function&quot;</span><span class="p">,</span>
        <span class="n">func</span><span class="p">,</span>
        <span class="n">proxy_args</span><span class="p">,</span>
        <span class="n">proxy_kwargs</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_target_to_str</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">overloadpacket</span><span class="o">.</span><span class="vm">__name__</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">_enable_thunkify</span><span class="p">(</span><span class="n">proxy_mode</span><span class="o">.</span><span class="n">tracer</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># In some circumstances, we will be tracing in a situation where a tensor</span>
    <span class="c1"># is *statically* known to be a constant (currently, this only happens if</span>
    <span class="c1"># you run torch.tensor; deterministic factory functions like torch.arange</span>
    <span class="c1"># don&#39;t get this treatment).  When the tensor in question is small, it&#39;s</span>
    <span class="c1"># helpful to due constant propagation in case we call item() (in which</span>
    <span class="c1"># case we can return the constant value that is known, rather than give</span>
    <span class="c1"># an error.)  The logic here tests if constant propagation is possible</span>
    <span class="c1"># (because all of the inputs are constant).  If so, we disable fake tensor</span>
    <span class="c1"># mode (if it is on) and do true compute on the constant.</span>
    <span class="c1">#</span>
    <span class="c1"># It&#39;s worth highlighting that we&#39;re making a policy decision here.</span>
    <span class="c1"># There is a potential that the tensor is actually quite large, and we</span>
    <span class="c1"># don&#39;t actually want to run the compute.  The tensor being quite large</span>
    <span class="c1"># is one of the reasons why factory functions don&#39;t get this treatment</span>
    <span class="c1"># (since they can be quite large; if a parameter is initialized to a</span>
    <span class="c1"># constant value it will be!)  Similarly, there is also a potential</span>
    <span class="c1"># to run an operator that blows up the size of a small tensor; we don&#39;t</span>
    <span class="c1"># protect against this case, but we could force, e.g., only single</span>
    <span class="c1"># element constant computation by testing the numel of the result before</span>
    <span class="c1"># propagating const-ness.  Similarly, we don&#39;t require the constant to</span>
    <span class="c1"># live on CPU, but we could.</span>
    <span class="n">any_constant</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">t</span><span class="o">.</span><span class="n">constant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">f_flat_args_kwargs</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">constant</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">tensor_numel_in_limit</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">CONSTANT_NUMEL_LIMIT</span>

    <span class="c1"># If this is a lift, the input tensor is guaranteed to be a</span>
    <span class="c1"># constant, so we keep a copy of the original argument along so</span>
    <span class="c1"># we can query it if we&#39;re asked to item() it at some later point</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">func</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">lift_fresh_copy</span><span class="o">.</span><span class="n">default</span>
        <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">CONSTANT_NUMEL_LIMIT</span>
    <span class="p">):</span>
        <span class="k">with</span> <span class="n">unset_fake_temporarily</span><span class="p">():</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">Proxy</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)),</span> <span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">constant</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">elif</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">Tag</span><span class="o">.</span><span class="n">nondeterministic_seeded</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">func</span><span class="o">.</span><span class="n">tags</span>
        <span class="ow">and</span> <span class="n">all_constant</span>
        <span class="ow">and</span> <span class="n">any_constant</span>
        <span class="ow">and</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_all_only</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tensor_numel_in_limit</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="c1"># NB: do NOT include factories as constants</span>
        <span class="k">with</span> <span class="n">unset_fake_temporarily</span><span class="p">():</span>
            <span class="n">const_flat_args_kwargs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">t</span><span class="o">.</span><span class="n">constant</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">f_flat_args_kwargs</span>
            <span class="p">]</span>
            <span class="n">const_args</span><span class="p">,</span> <span class="n">const_kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span>
                <span class="n">const_flat_args_kwargs</span><span class="p">,</span> <span class="n">spec</span>
            <span class="p">)</span>
            <span class="n">constant</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">const_args</span><span class="p">,</span> <span class="o">**</span><span class="n">const_kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">constant</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">track_tensor_tree</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">proxy_out</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span> <span class="n">tracer</span><span class="o">=</span><span class="n">tracer</span><span class="p">)</span>
    <span class="n">_maybe_record_pointwise_barrier</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">proxy_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">_SymNodeDict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper around a dictionary that will hash SymInts with their nodes</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sym_node_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">PySymType</span><span class="p">,</span> <span class="n">_PySymProxyType</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">_PySymProxyType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sym_node_dict</span><span class="p">[</span><span class="n">key</span><span class="o">.</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_PySymProxyType</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sym_node_dict</span><span class="p">[</span><span class="n">key</span><span class="o">.</span><span class="n">node</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">key</span><span class="o">.</span><span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sym_node_dict</span>

    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">,</span> <span class="n">default</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_PySymProxyType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_PySymProxyType</span><span class="p">:</span>
        <span class="c1"># dict.get()&#39;s annotation doesn&#39;t accept `None` when the value type</span>
        <span class="c1"># isn&#39;t Optional.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sym_node_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">node</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sym_node_dict</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PythonKeyTracer</span><span class="p">(</span><span class="n">Tracer</span><span class="p">):</span>
    <span class="n">script_object_tracker</span><span class="p">:</span> <span class="n">MutableMapping</span><span class="p">[</span><span class="n">_AnyScriptObjectType</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">]</span>
    <span class="n">symnode_tracker</span><span class="p">:</span> <span class="n">_SymNodeDict</span>
    <span class="n">sympy_expr_tracker</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">sympy</span><span class="o">.</span><span class="n">Symbol</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span>
    <span class="n">tensor_tracker</span><span class="p">:</span> <span class="n">MutableMapping</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">]</span>
    <span class="n">torch_fn_counts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
    <span class="n">enable_thunkify</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">autowrap_modules</span><span class="o">=</span><span class="p">())</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_tracker</span> <span class="o">=</span> <span class="n">WeakTensorKeyDictionary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">symnode_tracker</span> <span class="o">=</span> <span class="n">_SymNodeDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">script_object_tracker</span> <span class="o">=</span> <span class="n">WeakIdKeyDictionary</span><span class="p">(</span>
            <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ref_type</span><span class="o">=</span><span class="n">_WeakHashRef</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sympy_expr_tracker</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="c1"># Stores the torch function that was called during tracing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Stores the counts for every torch function called. This is to help</span>
        <span class="c1"># distinguish between different calls to the same torch function.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_counts</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_thunkify</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># In general, we don&#39;t want to make modules leaves. In principle, users of</span>
    <span class="c1"># this tracer might want to override this in order to turn a couple specific</span>
    <span class="c1"># modules into leaves in the traced graph.</span>
    <span class="k">def</span> <span class="nf">call_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span>
        <span class="n">forward</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># We don&#39;t want to turn getattr calls into proxies. So we just return the actual value.</span>
    <span class="k">def</span> <span class="nf">getattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">attr_val</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">parameter_proxy_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">attr_val</span>

    <span class="k">def</span> <span class="nf">create_arg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">Node</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="n">p</span><span class="p">:</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span><span class="s2">&quot;get_attr&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="p">(),</span> <span class="p">{})</span>

            <span class="n">qualname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_fresh_qualname</span><span class="p">(</span><span class="s2">&quot;_param_constant&quot;</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">qualname</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span><span class="s2">&quot;get_attr&quot;</span><span class="p">,</span> <span class="n">qualname</span><span class="p">,</span> <span class="p">(),</span> <span class="p">{})</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">constant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">constant</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_arg</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[return-value]</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">unwrap_proxy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Proxy</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">unwrap_proxy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Proxy</span><span class="p">,</span> <span class="n">PySymType</span><span class="p">]:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">unwrap_proxy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span> <span class="n">_AnyScriptObjectType</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Proxy</span><span class="p">,</span> <span class="n">_AnyScriptObjectType</span><span class="p">]:</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">unwrap_proxy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">proxy</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="n">e</span><span class="o">.</span><span class="n">force</span><span class="p">())</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">_AnyScriptObject</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">e</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">_temp_remove_pre_dispatch_torch_function_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">_len_torch_function_stack</span><span class="p">,</span> <span class="n">_pop_mode</span><span class="p">,</span> <span class="n">_push_mode</span>

    <span class="n">temp_elements</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pre_dispatch_mode</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">while</span> <span class="n">_len_torch_function_stack</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">_pop_mode</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">PreDispatchTorchFunctionMode</span><span class="p">):</span>
            <span class="n">pre_dispatch_mode</span> <span class="o">=</span> <span class="n">mode</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">temp_elements</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">mode</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">temp_elements</span><span class="p">):</span>
        <span class="n">_push_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pre_dispatch_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp_elements</span><span class="p">)</span>
            <span class="k">while</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mode</span> <span class="o">=</span> <span class="n">_pop_mode</span><span class="p">()</span>
                <span class="n">count</span> <span class="o">-=</span> <span class="mi">1</span>

            <span class="n">temp_elements</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pre_dispatch_mode</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">mode</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">temp_elements</span><span class="p">):</span>
                <span class="n">_push_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
<span class="k">def</span> <span class="nf">dispatch_trace</span><span class="p">(</span>
    <span class="n">root</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Module</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
    <span class="n">tracer</span><span class="p">:</span> <span class="n">Tracer</span><span class="p">,</span>
    <span class="n">concrete_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">concrete_args</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

    <span class="c1"># NB: be careful not to DCE .item() calls</span>
    <span class="k">def</span> <span class="nf">impure_pred</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">.symbolic_shapes</span> <span class="kn">import</span> <span class="n">is_accessor_node</span>

        <span class="c1"># Always defer to the built-in notion of impure</span>
        <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">is_impure</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="c1"># Accessors always OK to DCE</span>
        <span class="k">if</span> <span class="n">is_accessor_node</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># If the operator in question takes SymInt args to SymInt output,</span>
        <span class="c1"># we assume it&#39;s pure and OK to DCE</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">),</span> <span class="n">py_sym_types</span><span class="p">)</span>
            <span class="ow">and</span>
            <span class="c1"># NB: constant args ok</span>
            <span class="nb">all</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">),</span> <span class="n">py_sym_types</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">n</span><span class="o">.</span><span class="n">args</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># No idea, just assume it&#39;s not OK</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">(</span><span class="n">impure_pred</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">torch._inductor.fx_passes.dedupe_symint_uses</span> <span class="kn">import</span> <span class="n">dedupe_symints</span>

    <span class="n">dedupe_symints</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">root</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="n">root</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">return</span> <span class="n">fx</span><span class="o">.</span><span class="n">_lazy_graph_module</span><span class="o">.</span><span class="n">_make_graph_module</span><span class="p">(</span><span class="n">tracer</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">wrap_key</span><span class="p">(</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">],</span> <span class="n">tensors</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">]:</span>
    <span class="n">flat_tensors</span><span class="p">,</span> <span class="n">tensors_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">proxies</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">_unused</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="p">:</span>
        <span class="n">flat_proxies</span><span class="p">,</span> <span class="n">proxies_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">proxies</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_proxies</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_tensors</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">disable_proxy_modes_tracing</span><span class="p">()</span> <span class="k">as</span> <span class="n">m</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ProxyTorchDispatchMode</span><span class="p">)</span>
            <span class="n">track_tensor_tree</span><span class="p">(</span><span class="n">flat_tensors</span><span class="p">,</span> <span class="n">flat_proxies</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tracer</span><span class="o">=</span><span class="n">tracer</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">get_tensor_proxy_slot</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">]:</span>
            <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">proxy</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map_only</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">get_tensor_proxy_slot</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map_only</span><span class="p">(</span>
            <span class="n">_AnyScriptObject</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">),</span> <span class="n">out</span>
        <span class="p">)</span>

        <span class="k">def</span> <span class="nf">get_sym_proxy_slot</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">PySymType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Proxy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tracer</span><span class="p">)</span><span class="o">.</span><span class="n">force</span><span class="p">()</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map_only</span><span class="p">(</span><span class="n">py_sym_types</span><span class="p">,</span> <span class="n">get_sym_proxy_slot</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">return</span> <span class="n">wrapped</span>


<span class="c1"># TODO: Make downstream users of this work with OperatorBase</span>
<span class="n">ORIGINAL_ATEN</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">object</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">set_original_aten_op</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="k">global</span> <span class="n">ORIGINAL_ATEN</span>
    <span class="k">if</span> <span class="n">ORIGINAL_ATEN</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">has_preserved_node_meta</span><span class="p">():</span>
        <span class="n">ORIGINAL_ATEN</span> <span class="o">=</span> <span class="n">func</span>
        <span class="n">fx_traceback</span><span class="o">.</span><span class="n">current_meta</span><span class="p">[</span><span class="s2">&quot;original_aten&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">ORIGINAL_ATEN</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">current_meta</span><span class="p">[</span><span class="s2">&quot;original_aten&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span>


<span class="k">class</span> <span class="nc">TorchFunctionMetadataMode</span><span class="p">(</span><span class="n">TorchFunctionMode</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span> <span class="o">=</span> <span class="n">tracer</span>

    <span class="k">def</span> <span class="nf">__torch_function__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">,</span>
        <span class="n">types</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorMeta</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">torch_fn_metadata</span> <span class="o">=</span> <span class="n">func</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">torch_fn_counts</span><span class="p">[</span><span class="n">func</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">torch_fn_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="c1"># This mode is **only** used for pre_dispatch tracing.</span>
<span class="c1"># In particular, we need to make sure that autograd/autocast API&#39;s</span>
<span class="c1"># that do not desugar into dispatcher operators stay in the graph.</span>
<span class="k">class</span> <span class="nc">PreDispatchTorchFunctionMode</span><span class="p">(</span><span class="n">TorchFunctionMode</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span> <span class="o">=</span> <span class="n">tracer</span>

    <span class="k">def</span> <span class="nf">__torch_function__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">,</span>
        <span class="n">types</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorMeta</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">_side_effectful_need_to_be_preserved_pre_dispatch</span><span class="p">:</span>
            <span class="c1"># It&#39;s for passing the export verifier which needs to verify the meta[&#39;val&#39;]</span>
            <span class="c1"># TODO(tmanlaibaatar): we should systematically couple it with expoert verifier,</span>
            <span class="c1"># instead of hardcoding it here.</span>
            <span class="n">node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span><span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="p">{})</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_set_grad_enabled</span><span class="p">:</span>
                <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="n">node</span>
            <span class="c1"># Don&#39;t actually run the function! We just want to trace the calls</span>
            <span class="c1"># into a graph. We don&#39;t actualy want to change global autograd state.</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ProxyTorchDispatchMode</span><span class="p">(</span><span class="n">TorchDispatchMode</span><span class="p">):</span>
    <span class="c1"># Ensure this is read-only; this exists only for legacy reasons</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_tracing</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">,</span>
        <span class="n">tracing_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">_allow_fake_constant</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">_error_on_data_dependent_ops</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DispatchKey</span><span class="o">.</span><span class="n">PreDispatch</span> <span class="k">if</span> <span class="n">pre_dispatch</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span> <span class="o">=</span> <span class="n">tracer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span> <span class="o">=</span> <span class="n">tracing_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span> <span class="o">=</span> <span class="n">pre_dispatch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_allow_fake_constant</span> <span class="o">=</span> <span class="n">_allow_fake_constant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_error_on_data_dependent_ops</span> <span class="o">=</span> <span class="n">_error_on_data_dependent_ops</span>
        <span class="c1"># Indicates to our torch_dispatch dispatching infra that</span>
        <span class="c1"># this is an &quot;infra&quot; mode with lower dispatching precedence.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mode_key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TorchDispatchModeKey</span><span class="o">.</span><span class="n">PROXY</span>
        <span class="c1"># Every time we enter a mode, we maintain a stack telling us what the previous</span>
        <span class="c1"># ProxyTorchDispatchMode state was (if there was any).</span>
        <span class="c1"># This lets us properly reset the state on exit.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enter_stack</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">ProxyTorchDispatchMode</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decomp_layers</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">emulate_precision_casts</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">emulate_precision_casts</span>

    <span class="nd">@count</span>
    <span class="k">def</span> <span class="nf">__torch_dispatch__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">,</span>
        <span class="n">types</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorMeta</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">set_original_aten_op</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="p">{}</span>

            <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="p">(</span><span class="n">prim</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">default</span><span class="p">,):</span>
                <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">proxy_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="c1"># Stash and store the previous proxy mode (there may or may not be one)</span>
        <span class="n">maybe_prev_proxy_mode</span> <span class="o">=</span> <span class="n">_unset_infra_mode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TorchDispatchModeKey</span><span class="o">.</span><span class="n">PROXY</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enter_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">maybe_prev_proxy_mode</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">exc_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="ne">BaseException</span><span class="p">]],</span>
        <span class="n">exc_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">BaseException</span><span class="p">],</span>
        <span class="n">traceback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">types</span><span class="o">.</span><span class="n">TracebackType</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">)</span>

        <span class="c1"># Re-enable the previous proxy mode, if there was one.</span>
        <span class="n">mb_previous_proxy_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enter_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">mb_previous_proxy_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_push_mode</span><span class="p">(</span><span class="n">mb_previous_proxy_mode</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">b</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">is_infra_mode</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_compute_proxy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">out</span><span class="p">:</span> <span class="n">PySymType</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Proxy</span><span class="p">:</span>
        <span class="n">n_args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">)</span><span class="o">.</span><span class="n">force</span><span class="p">()</span><span class="o">.</span><span class="n">node</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">a</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span>
        <span class="p">)</span>

        <span class="c1"># func doesn&#39;t have a __torch_function__ that Proxy can interpose, so</span>
        <span class="c1"># we gotta do it manually</span>
        <span class="n">n_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span><span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">n_args</span><span class="p">,</span> <span class="p">{})</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">p_out</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">(</span><span class="n">n_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">)</span>
        <span class="n">set_meta</span><span class="p">(</span><span class="n">p_out</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p_out</span>

    <span class="k">def</span> <span class="nf">__sym_dispatch__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">,</span>
        <span class="n">types</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorMeta</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="c1"># Peephole optimize multiply by one</span>
        <span class="c1"># NB: be careful not to trigger guards here!</span>
        <span class="k">if</span> <span class="n">func</span> <span class="o">==</span> <span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># For speed, we assume there are no nested data structures</span>
        <span class="c1"># (otherwise we could use tree_map)</span>
        <span class="c1"># We also assume there are no keyword arguments.</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">kwargs</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># If func returned a constant, we don&#39;t need to trace; we have</span>
        <span class="c1"># determined that the result is constant (no matter if the inputs</span>
        <span class="c1"># were symbolic) and it is no longer necessary to trace the</span>
        <span class="c1"># computation.  This could occur if func triggered some guards.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">py_sym_types</span><span class="p">):</span>
            <span class="n">p_out_thunk</span> <span class="o">=</span> <span class="n">thunkify</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_proxy</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span>
            <span class="p">)</span>
            <span class="n">set_proxy_slot</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">,</span> <span class="n">p_out_thunk</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">_GraphAppendingTracerEx</span><span class="p">(</span><span class="n">fx</span><span class="o">.</span><span class="n">proxy</span><span class="o">.</span><span class="n">GraphAppendingTracer</span><span class="p">):</span>
    <span class="n">script_object_tracker</span><span class="p">:</span> <span class="n">MutableMapping</span><span class="p">[</span><span class="n">_AnyScriptObjectType</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">]</span>
    <span class="n">symnode_tracker</span><span class="p">:</span> <span class="n">MutableMapping</span><span class="p">[</span><span class="n">PySymType</span><span class="p">,</span> <span class="n">_PySymProxyType</span><span class="p">]</span>
    <span class="n">tensor_tracker</span><span class="p">:</span> <span class="n">MutableMapping</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">_ProxyTensor</span><span class="p">]</span>
    <span class="n">sympy_expr_tracker</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">sympy</span><span class="o">.</span><span class="n">Symbol</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span>
    <span class="n">torch_fn_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">]</span>
    <span class="n">torch_fn_counts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
    <span class="n">enable_thunkify</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">Graph</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">symnode_tracker</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">WeakKeyDictionary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_tracker</span> <span class="o">=</span> <span class="n">WeakTensorKeyDictionary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sympy_expr_tracker</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">script_object_tracker</span> <span class="o">=</span> <span class="n">WeakIdKeyDictionary</span><span class="p">(</span>
            <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ref_type</span><span class="o">=</span><span class="n">_WeakHashRef</span>
        <span class="p">)</span>
        <span class="c1"># Stores the torch function that was called during tracing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Stores the counts for every torch function called. This is to help</span>
        <span class="c1"># distinguish between different calls to the same torch function.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_counts</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># TODO: I&#39;m not sure what the point of this class is; you can just</span>
<span class="c1"># make_fx through a regular Interpreter</span>
<span class="k">class</span> <span class="nc">DecompositionInterpreter</span><span class="p">(</span><span class="n">fx</span><span class="o">.</span><span class="n">Interpreter</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
        <span class="n">new_graph</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">,</span>
        <span class="n">decomposition_table</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">new_graph</span> <span class="o">=</span> <span class="n">new_graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span> <span class="o">=</span> <span class="n">_GraphAppendingTracerEx</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_graph</span><span class="p">)</span>
        <span class="c1"># Blegh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decomposition_table</span> <span class="o">=</span> <span class="n">decomposition_table</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">ProxyTorchDispatchMode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">,</span> <span class="n">tracing_mode</span><span class="o">=</span><span class="s2">&quot;real&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">placeholder</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span>  <span class="c1"># type: ignore[override]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">proxy</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_graph</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">)</span>
        <span class="n">track_tensor_tree</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">proxy</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tracer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">)</span>
        <span class="c1"># TODO handle case where the first character of target is &#39;*&#39;</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">get_attr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span>  <span class="c1"># type: ignore[override]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">proxy</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_graph</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">)</span>
        <span class="n">track_tensor_tree</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">proxy</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tracer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># call_function, call_method, call_module get traced automatically by the outer mode.</span>

    <span class="k">def</span> <span class="nf">output</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span>  <span class="c1"># type: ignore[override]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

        <span class="k">def</span> <span class="nf">get_proxy_node</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">_ProxyTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">Node</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">proxy</span><span class="o">.</span><span class="n">node</span>

        <span class="k">def</span> <span class="nf">unwrap</span><span class="p">(</span><span class="n">e</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">]:</span>
            <span class="k">return</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracer</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">get_proxy_node</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">new_graph</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">unwrap</span><span class="p">,</span> <span class="n">out</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="c1"># Should enter the mode at least once for being able to restore it later</span>
        <span class="c1"># See: https://github.com/pytorch/pytorch/pull/82549#discussion_r934782025</span>
        <span class="k">with</span> <span class="n">decompose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decomposition_table</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>


<span class="k">def</span> <span class="nf">wrapper_and_args_for_make_fx</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">R</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">List</span><span class="p">[</span><span class="nb">object</span><span class="p">]],</span> <span class="n">R</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">object</span><span class="p">]]:</span>
    <span class="c1"># make_fx doesn&#39;t support kwargs, so we need to do this flattening</span>
    <span class="c1"># and then unflatten the args before calling func</span>
    <span class="n">flat_args</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">object</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="p">:</span>
        <span class="n">fn_args</span><span class="p">,</span> <span class="n">fn_kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">spec</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">fn_args</span><span class="p">,</span> <span class="o">**</span><span class="n">fn_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrapped</span><span class="p">,</span> <span class="n">flat_args</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_autocast_cache</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="n">old_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_cache_enabled</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_autocast_cache_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_autocast_cache_enabled</span><span class="p">(</span><span class="n">old_value</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_ModuleNotInstalledAsSubmoduleError</span><span class="p">(</span><span class="ne">NameError</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="c1"># Base class for inline _ModuleStackTracer.__init__.AttrProxy</span>
<span class="k">class</span> <span class="nc">_AttrProxy</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">reset_proxy_mapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>


<span class="k">class</span> <span class="nc">_ModuleStackTracer</span><span class="p">(</span><span class="n">PythonKeyTracer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Customized version of PythonKeyTracer that retains module stack</span>
<span class="sd">    information in node.meta[&quot;nn_module_stack&quot;].</span>

<span class="sd">    FX symbolic trace actually does this already, but it relies on `self.root`</span>
<span class="sd">    being the actual module being traced. Since make_fx traces a lambda of our</span>
<span class="sd">    creation, things don&#39;t work properly.</span>

<span class="sd">    So for this version we hold onto a reference to the original module</span>
<span class="sd">    (scope_root) and use that to match the path. Also when we see,</span>
<span class="sd">            A</span>
<span class="sd">           / \</span>
<span class="sd">          B   C</span>
<span class="sd">           \ /</span>
<span class="sd">            D</span>
<span class="sd">    we want to record the path as A.B.D by recording only one path.</span>
<span class="sd">    See Note [Preserving the nn module stack metadata during export non-strict mode]  # noqa: W605</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope_root</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope_root</span> <span class="o">=</span> <span class="n">scope_root</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_paths</span><span class="p">:</span> <span class="n">WeakKeyDictionary</span><span class="p">[</span><span class="n">_AttrProxy</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeakKeyDictionary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">:</span> <span class="n">WeakKeyDictionary</span><span class="p">[</span><span class="n">Module</span><span class="p">,</span> <span class="n">_AttrProxy</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeakKeyDictionary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_modules</span><span class="p">:</span> <span class="n">WeakKeyDictionary</span><span class="p">[</span><span class="n">_AttrProxy</span><span class="p">,</span> <span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeakKeyDictionary</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">module_id_cache</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">mod</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope_root</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_id_cache</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">mod</span><span class="p">)]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># Build a wrapper around _AttrProxy to provide the tracer. We can&#39;t</span>
        <span class="c1"># store it on _AttrProxy itself beceause we mimic the underlying class</span>
        <span class="c1"># (including its attributes).</span>
        <span class="n">tracer</span> <span class="o">=</span> <span class="bp">self</span>

        <span class="k">class</span> <span class="nc">AttrProxy</span><span class="p">(</span><span class="n">_AttrProxy</span><span class="p">):</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Class is modified to be a subclass of torch.nn.Module</span>
                <span class="c1"># Warning: We blow away our own attributes here to mimic the base class</span>
                <span class="c1"># - so don&#39;t expect `self.x` to do anything useful.</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span>
                    <span class="n">base</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="vm">__class__</span><span class="p">),</span>
                    <span class="p">{},</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="vm">__dict__</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__qualname__</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__qualname__</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reset_proxy_mapping</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">reset_proxy_mapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tracer</span><span class="o">.</span><span class="n">proxy_paths</span><span class="p">[</span><span class="bp">self</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
                <span class="n">tracer</span><span class="o">.</span><span class="n">proxy_modules</span><span class="p">[</span><span class="bp">self</span><span class="p">]</span> <span class="o">=</span> <span class="n">base</span>

            <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AttrProxy</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span>
                <span class="c1"># Calling into torch.nn.Module.__getattr__ with super(),</span>
                <span class="c1"># That __getattr__ is patched to be module_getattr_wrapper in _symbolic_trace.py.</span>
                <span class="c1"># which then calls into _ModuleStackTracer.getattr</span>
                <span class="n">attr_val</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_val</span><span class="p">,</span> <span class="n">AttrProxy</span><span class="p">):</span>
                    <span class="n">attr_val</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">proxy_modules</span><span class="p">[</span><span class="n">attr_val</span><span class="p">]</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_val</span><span class="p">,</span> <span class="n">Module</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">attr_val</span>
                <span class="k">if</span> <span class="n">attr_val</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tracer</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">:</span>
                    <span class="n">tracer</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">[</span><span class="n">attr_val</span><span class="p">]</span> <span class="o">=</span> <span class="n">AttrProxy</span><span class="p">(</span>
                        <span class="n">attr_val</span><span class="p">,</span> <span class="n">tracer</span><span class="o">.</span><span class="n">proxy_paths</span><span class="p">[</span><span class="bp">self</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">name</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># NOTE [caching AttrProxy]. Caching ensures a 1-1 mapping between AttrProxy and the actual attr_val.</span>
                    <span class="c1"># 1. We reset the proxy_mapping to solve the diamond shape reference problem: we want to record the</span>
                    <span class="c1"># path as A.B.D instead of A.C.D (the purpose of _ModuleStackTracer).</span>
                    <span class="c1"># 2. Instead of creating a new AttrProxy, we just reset the proxy_mapping of existing one. This is to avoid</span>
                    <span class="c1"># dynamo creating multiple guards for the same attr_val but different AttrProxy when exporting</span>
                    <span class="c1"># a model that calls torch.compile (e.g when a model uses torch.cond.)</span>
                    <span class="n">tracer</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">[</span><span class="n">attr_val</span><span class="p">]</span><span class="o">.</span><span class="n">reset_proxy_mapping</span><span class="p">(</span>
                        <span class="n">attr_val</span><span class="p">,</span> <span class="n">tracer</span><span class="o">.</span><span class="n">proxy_paths</span><span class="p">[</span><span class="bp">self</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">name</span>
                    <span class="p">)</span>
                <span class="k">return</span> <span class="n">tracer</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">[</span><span class="n">attr_val</span><span class="p">]</span>

            <span class="k">def</span> <span class="nf">get_base</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">tracer</span><span class="o">.</span><span class="n">proxy_modules</span><span class="p">[</span><span class="bp">self</span><span class="p">]</span>

            <span class="nd">@property</span>
            <span class="k">def</span> <span class="nf">_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AttrProxy</span><span class="p">]:</span>
                <span class="k">assert</span> <span class="s2">&quot;_modules&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>
                <span class="n">submodules</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_modules&quot;</span><span class="p">]</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodules</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="n">key</span><span class="p">:</span> <span class="n">AttrProxy</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tracer</span><span class="o">.</span><span class="n">proxy_paths</span><span class="p">[</span><span class="bp">self</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">submodules</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_type</span> <span class="o">=</span> <span class="n">AttrProxy</span>

    <span class="k">def</span> <span class="nf">path_of_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">:</span> <span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Use tracked access path during tracing instead of the default BFS behavior.</span>
<span class="sd">        Still use all the possible module paths to verify the result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mod</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope_root</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">_AttrProxy</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">proxy_paths</span><span class="p">[</span><span class="n">mod</span><span class="p">]</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tracer</span><span class="o">.</span><span class="n">path_of_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">NameError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">_ModuleNotInstalledAsSubmoduleError</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="k">def</span> <span class="nf">getattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">attr_val</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">parameter_proxy_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_val</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_val</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">getattr</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">attr_val</span><span class="p">,</span> <span class="n">parameter_proxy_cache</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_val</span><span class="p">,</span> <span class="n">_AttrProxy</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">attr_val</span>

        <span class="c1"># See NOTE [caching AttrProxy].</span>
        <span class="k">if</span> <span class="n">attr_val</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">[</span><span class="n">attr_val</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proxy_type</span><span class="p">(</span><span class="n">attr_val</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">[</span><span class="n">attr_val</span><span class="p">]</span><span class="o">.</span><span class="n">reset_proxy_mapping</span><span class="p">(</span><span class="n">attr_val</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attr_proxy_map</span><span class="p">[</span><span class="n">attr_val</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">trace</span><span class="p">(</span>  <span class="c1"># type: ignore[override]</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">root</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Module</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span> <span class="n">concrete_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">concrete_args</span><span class="p">)</span>

        <span class="c1"># Since we are making _AttrProxy mimic the original</span>
        <span class="c1"># submodule, when someone registers a module directly</span>
        <span class="c1"># to the tracer while tracing, the proxy object gets registered</span>
        <span class="c1"># first. So we need to replace the proxy modules with the real ones</span>
        <span class="c1"># This can happen during HOO tracing</span>
        <span class="n">proxy_module_names_to_be_replaced</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_AttrProxy</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">proxy_modules</span><span class="p">:</span>
                <span class="n">proxy_module_names_to_be_replaced</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">))</span>

        <span class="k">def</span> <span class="nf">_delete_proxy_attr</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
            <span class="c1"># Copied from fx/graph_module.py</span>
            <span class="c1"># Customized it for proxy type</span>
            <span class="n">atoms</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="n">path</span><span class="p">,</span> <span class="n">target_submod</span> <span class="o">=</span> <span class="n">atoms</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span>
            <span class="n">mod</span> <span class="o">=</span> <span class="n">obj</span>

            <span class="c1"># Get the parent module</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
                    <span class="k">return</span> <span class="kc">False</span>

                <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="p">(</span><span class="n">_AttrProxy</span><span class="p">,</span> <span class="n">Module</span><span class="p">)):</span>
                    <span class="k">return</span> <span class="kc">False</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target_submod</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">False</span>

            <span class="c1"># At least the leaf module should be proxy type.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target_submod</span><span class="p">),</span> <span class="n">_AttrProxy</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">False</span>

            <span class="nb">delattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target_submod</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">for</span> <span class="n">proxy_module_name</span><span class="p">,</span> <span class="n">proxy_module</span> <span class="ow">in</span> <span class="n">proxy_module_names_to_be_replaced</span><span class="p">:</span>
            <span class="n">_delete_proxy_attr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">proxy_module_name</span><span class="p">)</span>
            <span class="n">actual_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proxy_modules</span><span class="p">[</span><span class="n">proxy_module</span><span class="p">]</span>
            <span class="n">_assign_attr</span><span class="p">(</span><span class="n">actual_module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="n">proxy_module_name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">call_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span>
        <span class="n">forward</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;PythonKeyTracer overrides call_module to avoid the scope handling,</span>
<span class="sd">        but we actually want it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">torch._dynamo</span> <span class="kn">import</span> <span class="n">OptimizedModule</span>

        <span class="c1"># FIXME (tmanlaibaatar)</span>
        <span class="c1"># When we call torch.compile inside HOO, we will end up</span>
        <span class="c1"># invoking a module that is not registered on the root. For</span>
        <span class="c1"># now, we just inline them. But once we start supporting</span>
        <span class="c1"># mark_strict in export, we do need to properly handle this.</span>
        <span class="c1"># Right now, it doesn&#39;t matter because current non-strict</span>
        <span class="c1"># use cases don&#39;t need to work with HOO.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="n">OptimizedModule</span><span class="p">,</span> <span class="n">GraphModule</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tracer</span><span class="o">.</span><span class="n">call_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">_ModuleNotInstalledAsSubmoduleError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unable to find the path of the module </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="s2">&quot;This might be because the module was not properly registered &quot;</span>
                <span class="s2">&quot;as a submodule, which is not good practice. We will trace &quot;</span>
                <span class="s2">&quot;through the module without recording stack information.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_leaf_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">module_qualified_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">create_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">Node</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create node and add on metadata.</span>
<span class="sd">        Add nn_module_stack here instead of TracerBase,</span>
<span class="sd">        since calls to make_fx() might not want to record module stack metadata.</span>
<span class="sd">        Add torch_fn by looking at torch_fn_metadata and torch_fn_counts.</span>
<span class="sd">        Add stack_trace by filtering out forward() stack frames.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">node</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

        <span class="c1"># nn_module_stack</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;placeholder&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="s2">&quot;nn_module_stack&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">:</span>
                <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;nn_module_stack&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_stack</span>
            <span class="c1"># convert nn_module_stack from Dict[key, (FQN, class)] -&gt; Dict[str, Tuple[str, str]]</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">fqn</span><span class="p">,</span> <span class="n">mod_cls</span><span class="p">)</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;nn_module_stack&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod_cls</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;nn_module_stack&quot;</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">fqn</span><span class="p">,</span>
                        <span class="n">mod_cls</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">mod_cls</span><span class="o">.</span><span class="vm">__qualname__</span><span class="p">,</span>
                    <span class="p">)</span>

        <span class="c1"># torch_fn</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="s2">&quot;torch_fn&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span>
        <span class="p">):</span>
            <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;torch_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_counts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># stack_trace</span>
        <span class="k">if</span> <span class="s2">&quot;stack_trace&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;placeholder&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">]:</span>
            <span class="n">user_frame_summary</span> <span class="o">=</span> <span class="n">CapturedTraceback</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">user_frame_summary</span><span class="p">:</span>
                <span class="c1"># we retain frames from forward() calls, or ops</span>
                <span class="c1"># located in torch/__init__.py (e.g. sym_int, sym_constrain_range, vmap)</span>
                <span class="n">stack_trace</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">frame</span>
                    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">user_frame_summary</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">frame</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;forward&quot;</span>
                        <span class="ow">or</span> <span class="n">frame</span><span class="o">.</span><span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;torch/__init__.py&quot;</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">]</span>
                <span class="c1"># filter out forward() frames from fx/_symbolic_trace.py, export/_trace.py</span>
                <span class="c1"># this is hardcoded, but leads to a much cleaner stack trace</span>
                <span class="n">stack_trace</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">frame</span>
                    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">stack_trace</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
                        <span class="n">frame</span><span class="o">.</span><span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;fx/_symbolic_trace.py&quot;</span><span class="p">)</span>
                        <span class="ow">or</span> <span class="n">frame</span><span class="o">.</span><span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;export/_trace.py&quot;</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">]</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">stack_trace</span>
                <span class="p">):</span>  <span class="c1"># empty list for strict mode, dynamo should handle stack_trace</span>
                    <span class="n">stack_trace</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">StackSummary</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">stack_trace</span><span class="p">)</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;stack_trace&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stack_trace</span><span class="o">.</span><span class="n">format</span><span class="p">())</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">node</span>


<span class="k">class</span> <span class="nc">_MakefxTracer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decomposition_table</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]],</span>
        <span class="n">tracing_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">_allow_non_fake_inputs</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">record_module_stack</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">_allow_fake_constant</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">_error_on_data_dependent_ops</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Configurations that are used to initialize the context managers and their states.</span>
        <span class="c1"># Should not modify them during tracing.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decomposition_table</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">decomposition_table</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decomposition_table</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sym_numel</span><span class="o">.</span><span class="n">default</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_decomp</span><span class="o">.</span><span class="n">decompositions</span><span class="o">.</span><span class="n">sym_numel</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">tracing_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_allow_non_fake_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">_allow_non_fake_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">pre_dispatch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_module_stack</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">record_module_stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_allow_fake_constant</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">_allow_fake_constant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_error_on_data_dependent_ops</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">_error_on_data_dependent_ops</span>

        <span class="c1"># All context managers and their states should be initialized before tracing based on the inputs</span>
        <span class="c1"># and configurations. After tracing, their states should be cleaned except for shape_env.</span>
        <span class="c1"># Remember to specify how to intialize it from user inputs and from parent tracer whenever</span>
        <span class="c1"># adding new modes in _MakefxTracer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeTensorMode</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nullcontext</span><span class="p">,</span> <span class="n">ProxyTorchDispatchMode</span><span class="p">]</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_function_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">nullcontext</span><span class="p">,</span> <span class="n">PreDispatchTorchFunctionMode</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PythonKeyTracer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">python_dispatcher_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nullcontext</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">nullcontext</span><span class="p">,</span> <span class="n">TorchFunctionMetadataMode</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_checkpoint_modes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proxy_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proxy_function_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">python_dispatcher_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata_mode</span><span class="p">,</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">_restore_modes</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prev_fake_tensor_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeTensorMode</span><span class="p">],</span>
        <span class="n">prev_proxy_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nullcontext</span><span class="p">,</span> <span class="n">ProxyTorchDispatchMode</span><span class="p">],</span>
        <span class="n">prev_proxy_function_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nullcontext</span><span class="p">,</span> <span class="n">PreDispatchTorchFunctionMode</span><span class="p">],</span>
        <span class="n">prev_fx_tracer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PythonKeyTracer</span><span class="p">],</span>
        <span class="n">prev_python_dispatcher_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nullcontext</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prev_torch_fn_metadata_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nullcontext</span><span class="p">,</span> <span class="n">TorchFunctionMetadataMode</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">prev_fake_tensor_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_mode</span> <span class="o">=</span> <span class="n">prev_proxy_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_function_mode</span> <span class="o">=</span> <span class="n">prev_proxy_function_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span> <span class="o">=</span> <span class="n">prev_fx_tracer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="n">prev_python_dispatcher_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata_mode</span> <span class="o">=</span> <span class="n">prev_torch_fn_metadata_mode</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_init_modes_from_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="n">prev_modes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_modes</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Avoid importing sympy at a module level</span>
            <span class="kn">from</span> <span class="nn">.symbolic_shapes</span> <span class="kn">import</span> <span class="n">ShapeEnv</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;_orig_mod&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_module_stack</span><span class="p">:</span>
                <span class="n">scope_root</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">_orig_mod</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span> <span class="o">=</span> <span class="n">_ModuleStackTracer</span><span class="p">(</span><span class="n">scope_root</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span> <span class="o">=</span> <span class="n">PythonKeyTracer</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span> <span class="o">==</span> <span class="s2">&quot;fake&quot;</span><span class="p">:</span>
                <span class="kn">import</span> <span class="nn">torch._dynamo</span>

                <span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">fake_tensor_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="kn">import</span> <span class="nn">torch._functorch.config</span> <span class="k">as</span> <span class="nn">_config</span>

                    <span class="k">with</span> <span class="n">_config</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="n">fake_tensor_allow_unsafe_data_ptr_access</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                        <span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span>
                            <span class="n">allow_fallback_kernels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">allow_non_fake_inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_allow_non_fake_inputs</span><span class="p">,</span>
                            <span class="n">shape_env</span><span class="o">=</span><span class="n">ShapeEnv</span><span class="p">(),</span>
                            <span class="n">static_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">fake_tensor_mode</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span> <span class="o">==</span> <span class="s2">&quot;symbolic&quot;</span><span class="p">:</span>
                <span class="kn">import</span> <span class="nn">torch._dynamo</span>

                <span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">fake_tensor_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">shape_env</span> <span class="o">=</span> <span class="n">ShapeEnv</span><span class="p">()</span>
                    <span class="kn">import</span> <span class="nn">torch._functorch.config</span> <span class="k">as</span> <span class="nn">_config</span>

                    <span class="k">with</span> <span class="n">_config</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="n">fake_tensor_allow_unsafe_data_ptr_access</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                        <span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span>
                            <span class="n">allow_fallback_kernels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">allow_non_fake_inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_allow_non_fake_inputs</span><span class="p">,</span>
                            <span class="n">shape_env</span><span class="o">=</span><span class="n">shape_env</span><span class="p">,</span>
                        <span class="p">)</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">fake_tensor_mode</span><span class="o">.</span><span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="p">),</span> <span class="s2">&quot;shape_env should be set if tracing with &#39;symbolic&#39;&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">fake_tensor_mode</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span> <span class="o">==</span> <span class="s2">&quot;real&quot;</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Unexpected tracing type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_construct_modes_with_fx_tracer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span><span class="p">)</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_restore_modes</span><span class="p">(</span><span class="o">*</span><span class="n">prev_modes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_construct_modes_with_fx_tracer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fx_tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proxy_mode</span> <span class="o">=</span> <span class="n">ProxyTorchDispatchMode</span><span class="p">(</span>
            <span class="n">fx_tracer</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span><span class="p">,</span>
            <span class="n">pre_dispatch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">,</span>
            <span class="n">_allow_fake_constant</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_allow_fake_constant</span><span class="p">,</span>
            <span class="n">_error_on_data_dependent_ops</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_error_on_data_dependent_ops</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proxy_function_mode</span> <span class="o">=</span> <span class="n">PreDispatchTorchFunctionMode</span><span class="p">(</span><span class="n">fx_tracer</span><span class="p">)</span>

        <span class="c1"># pre-autograd tracing uses per-dispatch-key modes,</span>
        <span class="c1"># which requires the python dispatcher</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span> <span class="o">==</span> <span class="s2">&quot;symbolic&quot;</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="n">enable_python_dispatcher</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata_mode</span> <span class="o">=</span> <span class="n">TorchFunctionMetadataMode</span><span class="p">(</span><span class="n">fx_tracer</span><span class="p">)</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_init_modes_from_parent</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">parent_tracer</span><span class="p">:</span> <span class="n">_MakefxTracer</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="c1"># By default, subtracer creates new modes based on parent tracer&#39;s config.</span>
        <span class="c1"># However, there are cases where we want to share the same modes with parent tracer</span>
        <span class="c1"># For example, fake_tensor_mode, we want the example value&#39;s fake_mode of parent graph and subgraphs to be the same.</span>
        <span class="n">prev_modes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_modes</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span> <span class="o">=</span> <span class="n">parent_tracer</span><span class="o">.</span><span class="n">fake_tensor_mode</span>

            <span class="k">def</span> <span class="nf">_create_sub_fx_tracer</span><span class="p">(</span><span class="n">parent_tracer</span><span class="p">:</span> <span class="n">_ProxyTracer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PythonKeyTracer</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">parent_tracer</span><span class="p">)</span> <span class="o">==</span> <span class="n">PythonKeyTracer</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">PythonKeyTracer</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">parent_tracer</span><span class="p">)</span> <span class="o">==</span> <span class="n">_ModuleStackTracer</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">_ModuleStackTracer</span><span class="p">(</span><span class="n">parent_tracer</span><span class="o">.</span><span class="n">scope_root</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Unexpected tracer type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">parent_tracer</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

            <span class="k">assert</span> <span class="n">parent_tracer</span><span class="o">.</span><span class="n">fx_tracer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span> <span class="o">=</span> <span class="n">_create_sub_fx_tracer</span><span class="p">(</span><span class="n">parent_tracer</span><span class="o">.</span><span class="n">fx_tracer</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_construct_modes_with_fx_tracer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span><span class="p">)</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_restore_modes</span><span class="p">(</span><span class="o">*</span><span class="n">prev_modes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_trace_inner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
        <span class="n">phs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">_symbolic_trace</span><span class="o">.</span><span class="n">PH</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_wrap_fake</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
            <span class="n">arg_count</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">def</span> <span class="nf">inner_wrap_fake</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
                <span class="k">nonlocal</span> <span class="n">arg_count</span>
                <span class="c1"># TODO: it would be nice to line these up with the names</span>
                <span class="c1"># FX will choose for the placeholders, but we don&#39;t</span>
                <span class="c1"># actually know what the names will be at this point yet</span>
                <span class="c1"># NB: the Source here is actually meaningless</span>
                <span class="kn">from</span> <span class="nn">torch._dynamo.source</span> <span class="kn">import</span> <span class="n">ConstantSource</span>

                <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">source</span> <span class="o">=</span> <span class="n">ConstantSource</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input</span><span class="si">{</span><span class="n">arg_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">arg_count</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>
                <span class="c1"># NB: don&#39;t match on bools</span>
                <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span> <span class="o">==</span> <span class="s2">&quot;symbolic&quot;</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="o">.</span><span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">),</span> <span class="s2">&quot;shape_env should be set if tracing with &#39;symbolic&#39;&quot;</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="o">.</span><span class="n">shape_env</span><span class="o">.</span><span class="n">create_symintnode</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="o">.</span><span class="n">shape_env</span><span class="o">.</span><span class="n">create_symbol</span><span class="p">(</span>
                            <span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="kc">None</span>
                        <span class="p">),</span>
                        <span class="n">hint</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                        <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ScriptObject</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">fake_class_registry</span><span class="o">.</span><span class="n">maybe_to_fake_obj</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="p">,</span> <span class="n">x</span>
                    <span class="p">)</span>

                <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">x</span><span class="p">,</span> <span class="n">FakeScriptObject</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;ScriptObject </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2"> has been fakified. Cannot wrap_fake it again.&quot;</span>
                <span class="k">return</span> <span class="n">x</span>

            <span class="n">wrap_fn_map</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;real&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
                <span class="s2">&quot;fake&quot;</span><span class="p">:</span> <span class="n">inner_wrap_fake</span><span class="p">,</span>
                <span class="s2">&quot;symbolic&quot;</span><span class="p">:</span> <span class="n">inner_wrap_fake</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">wrap_fn_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_wrap_func</span><span class="p">(</span><span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">],</span> <span class="n">phs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">PHBase</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">]:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">unwrap</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="s2">&quot;__code__&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">inspect</span><span class="o">.</span><span class="n">unwrap</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="vm">__code__</span><span class="o">.</span><span class="n">co_flags</span> <span class="o">&amp;</span> <span class="n">inspect</span><span class="o">.</span><span class="n">CO_VARARGS</span>
            <span class="p">):</span>
                <span class="c1"># FX doesn&#39;t support varargs, so we gotta fake up a wrapper</span>
                <span class="c1"># TODO: Would be nice to fix this at the source...</span>
                <span class="k">return</span> <span class="n">fake_signature</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">phs</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">f</span>

        <span class="n">args</span> <span class="o">=</span> <span class="n">_wrap_fake</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">func</span> <span class="o">=</span> <span class="n">_wrap_func</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">phs</span><span class="p">)</span>
        <span class="c1"># We disable the autocast cache as the autocast cache causes type conversions on parameters to</span>
        <span class="c1"># check a cache, which introduces untracked tensors into the graph</span>
        <span class="c1">#</span>
        <span class="c1"># We also disable tracing by any other tensor proxy-based tracers except the current. The</span>
        <span class="c1"># purpose of `make_fx` is to produce graphmodules as a side effect; its internal execution is</span>
        <span class="c1"># thus irrelevant to any external functional trace.</span>
        <span class="n">proxy_mode</span><span class="p">:</span> <span class="n">ProxyTorchDispatchMode</span> <span class="o">=</span> <span class="n">typing</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
            <span class="n">ProxyTorchDispatchMode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">proxy_mode</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
            <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">decompose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decomposition_table</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="p">:</span>
                <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="p">)</span>
            <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">python_dispatcher_mode</span><span class="p">)</span>
            <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proxy_function_mode</span><span class="p">)</span>
            <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_fn_metadata_mode</span><span class="p">)</span>
            <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">proxy_mode</span><span class="p">)</span>
            <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">disable_autocast_cache</span><span class="p">())</span>
            <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">_set_make_fx_tracer</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">dispatch_trace</span><span class="p">(</span>
                <span class="n">wrap_key</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">),</span>
                <span class="n">tracer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fx_tracer</span><span class="p">,</span>
                <span class="n">concrete_args</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">phs</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="c1"># TODO: kind of a bad way to do it, should maybe figure out a better way</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracing_mode</span> <span class="o">==</span> <span class="s2">&quot;symbolic&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">t</span><span class="o">.</span><span class="n">shape_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_tensor_mode</span><span class="o">.</span><span class="n">shape_env</span>
        <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_modes_from_inputs</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trace_inner</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">trace_subgraph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
        <span class="c1"># Create a new tracer based on parent&#39;s config</span>
        <span class="n">sub_tracer</span> <span class="o">=</span> <span class="n">_MakefxTracer</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decomposition_table</span><span class="p">,</span>
            <span class="s2">&quot;real&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_allow_non_fake_inputs</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">record_module_stack</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_allow_fake_constant</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_error_on_data_dependent_ops</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">sub_tracer</span><span class="o">.</span><span class="n">_init_modes_from_parent</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">sub_tracer</span><span class="o">.</span><span class="n">_trace_inner</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>


<span class="n">_CURRENT_MAKE_FX_TRACER</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_MakefxTracer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">_set_make_fx_tracer</span><span class="p">(</span><span class="n">tracer</span><span class="p">:</span> <span class="n">_MakefxTracer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="k">global</span> <span class="n">_CURRENT_MAKE_FX_TRACER</span>
    <span class="n">prev_tracer</span> <span class="o">=</span> <span class="n">_CURRENT_MAKE_FX_TRACER</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">_CURRENT_MAKE_FX_TRACER</span> <span class="o">=</span> <span class="n">tracer</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_CURRENT_MAKE_FX_TRACER</span> <span class="o">=</span> <span class="n">prev_tracer</span>


<div class="viewcode-block" id="make_fx"><a class="viewcode-back" href="../../../../generated/torch.fx.experimental.proxy_tensor.make_fx.html#torch.fx.experimental.proxy_tensor.make_fx">[docs]</a><span class="k">def</span> <span class="nf">make_fx</span><span class="p">(</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">decomposition_table</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tracing_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;real&quot;</span><span class="p">,</span>
    <span class="n">_allow_non_fake_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">record_module_stack</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">_allow_fake_constant</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">_error_on_data_dependent_ops</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">GraphModule</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a function f, return a new function which when executed with valid</span>
<span class="sd">    arguments to f, returns an FX GraphModule representing the set of operations that</span>
<span class="sd">    were executed during the course of execution.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="n">tracing_mode</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;real&quot;</span><span class="p">,</span> <span class="s2">&quot;fake&quot;</span><span class="p">,</span> <span class="s2">&quot;symbolic&quot;</span><span class="p">]</span>

    <span class="n">make_fx_tracer</span> <span class="o">=</span> <span class="n">_MakefxTracer</span><span class="p">(</span>
        <span class="n">decomposition_table</span><span class="p">,</span>
        <span class="n">tracing_mode</span><span class="p">,</span>
        <span class="n">_allow_non_fake_inputs</span><span class="p">,</span>
        <span class="n">pre_dispatch</span><span class="p">,</span>
        <span class="n">record_module_stack</span><span class="p">,</span>
        <span class="n">_allow_fake_constant</span><span class="p">,</span>
        <span class="n">_error_on_data_dependent_ops</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">make_fx_tracer</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrapped</span></div>


<span class="k">def</span> <span class="nf">get_torch_dispatch_modes</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">TorchDispatchMode</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_python_dispatch</span><span class="o">.</span><span class="n">_get_current_dispatch_mode_stack</span><span class="p">()</span>


<span class="c1"># TODO: this is a legacy name, there is only ever one proxy mode as it&#39;s an</span>
<span class="c1"># infra mode</span>
<span class="k">def</span> <span class="nf">get_innermost_proxy_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProxyTorchDispatchMode</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">get_proxy_mode</span><span class="p">()</span>


<div class="viewcode-block" id="get_proxy_mode"><a class="viewcode-back" href="../../../../generated/torch.fx.experimental.proxy_tensor.get_proxy_mode.html#torch.fx.experimental.proxy_tensor.get_proxy_mode">[docs]</a><span class="k">def</span> <span class="nf">get_proxy_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProxyTorchDispatchMode</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Current the currently active proxy tracing mode, or None if</span>
<span class="sd">    we are not currently tracing.  This includes pre-dispatch proxy</span>
<span class="sd">    tracing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pre_dispatch_mode</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">_get_dispatch_mode_pre_dispatch</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TorchDispatchModeKey</span><span class="o">.</span><span class="n">PROXY</span>
    <span class="p">)</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_dispatch_mode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TorchDispatchModeKey</span><span class="o">.</span><span class="n">PROXY</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">pre_dispatch_mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;pre_dispatch_mode=</span><span class="si">{</span><span class="n">pre_dispatch_mode</span><span class="si">}</span><span class="s2">, mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">pre_dispatch_mode</span> <span class="ow">or</span> <span class="n">mode</span></div>


<div class="viewcode-block" id="handle_sym_dispatch"><a class="viewcode-back" href="../../../../generated/torch.fx.experimental.proxy_tensor.handle_sym_dispatch.html#torch.fx.experimental.proxy_tensor.handle_sym_dispatch">[docs]</a><span class="k">def</span> <span class="nf">handle_sym_dispatch</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Call into the currently active proxy tracing mode to do a</span>
<span class="sd">    SymInt/SymFloat/SymBool dispatch trace on a function that operates on</span>
<span class="sd">    these arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">get_proxy_mode</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">mode</span>
    <span class="c1"># Have to do it manually, because we&#39;re not doing the normal torch</span>
    <span class="c1"># dispatch machinery which disables it for us</span>
    <span class="k">with</span> <span class="n">disable_proxy_modes_tracing</span><span class="p">():</span>
        <span class="c1"># TODO: properly compute types</span>
        <span class="n">types</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Type</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">mode</span><span class="o">.</span><span class="n">__sym_dispatch__</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type, return-value]</span></div>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_proxy_modes_tracing</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">ProxyTorchDispatchMode</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">_disable_infra_mode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TorchDispatchModeKey</span><span class="o">.</span><span class="n">PROXY</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">maybe_handle_decomp</span><span class="p">(</span>
    <span class="n">proxy_mode</span><span class="p">:</span> <span class="n">ProxyTorchDispatchMode</span><span class="p">,</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">OpOverload</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">CURRENT_DECOMPOSITION_TABLE</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">proxy_mode</span><span class="p">:</span>
            <span class="n">proxy_mode</span><span class="o">.</span><span class="n">decomp_layers</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">CURRENT_DECOMPOSITION_TABLE</span><span class="p">[</span><span class="n">op</span><span class="p">](</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">proxy_mode</span><span class="o">.</span><span class="n">decomp_layers</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="n">out</span>

    <span class="k">return</span> <span class="bp">NotImplemented</span>


<span class="k">def</span> <span class="nf">get_isolated_graphmodule</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
    <span class="n">tracing_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;real&quot;</span><span class="p">,</span>
    <span class="n">decomposition_table</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A helper function used to get the GraphModule for the given func.</span>

<span class="sd">    It&#39;s expected to be used in the ProxyTensor tracing context.</span>
<span class="sd">    It detaches the args and kwargs from the current tracer so that the trace of</span>
<span class="sd">    the current graph module can be created without any side-effects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">wrapped</span><span class="p">,</span> <span class="n">all_args</span> <span class="o">=</span> <span class="n">wrapper_and_args_for_make_fx</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">disable_proxy_modes_tracing</span><span class="p">():</span>
        <span class="n">gm</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span>
            <span class="n">wrapped</span><span class="p">,</span> <span class="n">decomposition_table</span><span class="o">=</span><span class="n">decomposition_table</span><span class="p">,</span> <span class="n">tracing_mode</span><span class="o">=</span><span class="n">tracing_mode</span>
        <span class="p">)(</span><span class="n">all_args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gm</span>


<span class="k">def</span> <span class="nf">_set_unbacked_bindings</span><span class="p">(</span><span class="n">out</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">out_proxy</span><span class="p">:</span> <span class="n">_NestedProxys</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A helper function for setting up unbacked_bindings on the destination FX graph.&quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.symbolic_shapes</span> <span class="kn">import</span> <span class="n">compute_unbacked_bindings</span>

    <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;_set_unbacked_bindings </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">out_proxy</span><span class="p">)</span>

    <span class="c1"># Can&#39;t use detect_fake_mode here,</span>
    <span class="c1">#</span>
    <span class="c1"># python test/distributed/_tensor/test_dtensor_compile.py -k</span>
    <span class="c1"># test_tp_compile_fullgraph_is_seq_parallel_False</span>
    <span class="c1">#</span>
    <span class="c1"># will fail.  Very strange, it probably isn&#39;t right for them to be using</span>
    <span class="c1"># two fake modes there...</span>
    <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_dispatch_mode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TorchDispatchModeKey</span><span class="o">.</span><span class="n">FAKE</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">and</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">symbol_to_path</span> <span class="o">:=</span> <span class="n">compute_unbacked_bindings</span><span class="p">(</span><span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_proxy</span><span class="p">,</span> <span class="n">Proxy</span><span class="p">),</span> <span class="n">out_proxy</span>
            <span class="n">out_proxy</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;unbacked_bindings&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">symbol_to_path</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>