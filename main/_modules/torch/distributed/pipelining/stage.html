


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.pipelining.stage &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/pipelining/stage.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.5.0a0+gite72ed47 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/distributed/pipelining/stage.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/get_start_xpu.html">Pytorch 2.4: Getting Started on Intel GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_environment_variables.html">Torch Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.pipelining.stage</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.pipelining.stage</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.fx</span> <span class="k">as</span> <span class="nn">fx</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch._subclasses.fake_tensor</span> <span class="kn">import</span> <span class="n">FakeTensor</span>
<span class="kn">from</span> <span class="nn">torch.distributed._composable.fsdp.fully_shard</span> <span class="kn">import</span> <span class="n">FSDPModule</span><span class="p">,</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="nn">torch.fx.node</span> <span class="kn">import</span> <span class="n">map_aggregate</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span>

<span class="kn">from</span> <span class="nn">._backward</span> <span class="kn">import</span> <span class="n">stage_backward</span><span class="p">,</span> <span class="n">stage_backward_input</span><span class="p">,</span> <span class="n">stage_backward_weight</span>
<span class="kn">from</span> <span class="nn">._debug</span> <span class="kn">import</span> <span class="n">map_debug_info</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">flatten_args</span><span class="p">,</span> <span class="n">PipeInfo</span><span class="p">,</span> <span class="n">validate_tensors_metadata</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;PipelineStage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;build_stage&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_RootArgPlaceholder</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Placeholder for model-level inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_RecvInfo</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents a stage input.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">source</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Name of this input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_name</span> <span class="o">=</span> <span class="n">input_name</span>
        <span class="c1"># Stage index of the source of this input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">source</span>
        <span class="c1"># Buffer to receive the input into.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">buffer</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;_RecvInfo(input=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_name</span><span class="si">}</span><span class="s2">, source=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="si">}</span><span class="s2">, shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">)&quot;</span>


<span class="c1"># An input can be either a received activation or a model input</span>
<span class="n">InputInfo</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">_RecvInfo</span><span class="p">,</span> <span class="n">_RootArgPlaceholder</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_make_tensor_from_meta</span><span class="p">(</span>
    <span class="n">example</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">],</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a real tensor from a tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
        <span class="n">example</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">example</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">layout</span><span class="o">=</span><span class="n">example</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">class</span> <span class="nc">_PipelineStageBase</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for pipeline stages.</span>
<span class="sd">    Defines or implements common methods used by the `_PipelineStage` used by</span>
<span class="sd">    the tracing frontend and `PipelineStage` used by manual frontend.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">submodule</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">stage_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_stages</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dw_builder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            submodule (torch.nn.Module): The module to be executed in this stage.</span>
<span class="sd">            stage_index (int): The index of this stage.</span>
<span class="sd">            num_stages (int): The total number of stages in this pipeline.</span>
<span class="sd">            device (torch.device): The device to run this stage on.</span>
<span class="sd">            group (Optional[dist.ProcessGroup]): The process group to use for communication.</span>
<span class="sd">                If `None`, the default process group will be used.</span>
<span class="sd">                Default: `None`.</span>
<span class="sd">            dw_builder (Optional[Callable[[], Callable[..., None]]): If provided, dw_runner is a builder function</span>
<span class="sd">                that will build a new dw_runner function that will run parts of module backward that were intentionally</span>
<span class="sd">                skipped during the module&#39;s actual backward pass. The builder must be invoked by stage after stage runs</span>
<span class="sd">                model backwards, and stage should save the latest dw_runner to run during weight pass.</span>
<span class="sd">                If not provided, a dw_runner will be generated automatically by traversing the autograd graph.</span>
<span class="sd">                When used with schedules that only have F and B steps, the fresh dw_runner function will be called as</span>
<span class="sd">                part of B.</span>
<span class="sd">                When used with F,B,W schedules, the dw_runner function implements &#39;W&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">stage_index</span> <span class="o">&gt;=</span> <span class="n">num_stages</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Stage index </span><span class="si">{</span><span class="n">stage_index</span><span class="si">}</span><span class="s2"> is out of range of </span><span class="si">{</span><span class="n">num_stages</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">submod</span> <span class="o">=</span> <span class="n">submodule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span> <span class="o">=</span> <span class="n">stage_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span> <span class="o">=</span> <span class="n">num_stages</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dw_builder</span> <span class="o">=</span> <span class="n">dw_builder</span>

        <span class="c1"># backward state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># store dw_runner per microbatch_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dw_runner</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># `group_rank` is rank in process group `group`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Pipeline group size </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2"> cannot be larger than number of stages </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Run time states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_outputs_meta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># map microbatch ID to list of forward tensor args</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fwd_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Caching chunk outputs for final output merge or reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_chunks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Initialize has_backward to false; this will be set to true if loss</span>
        <span class="c1"># function is passed to pipeline schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_backward</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Log prefix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[Stage </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="si">}</span><span class="s2">]&quot;</span>

        <span class="c1"># Forward infra</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">InputInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_requires_grad</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_send_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Backward infra will created lazily</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_recv_info</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_send_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Number of backward chunks seen. This is used to determine when to do</span>
        <span class="c1"># grad reduction in DDP or FSDP.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_seen_bwd_chunks</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># To be populated later by the Schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stage_index_to_group_rank</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">has_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns true if this stage has a backward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_backward</span>

    <span class="nd">@has_backward</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">has_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">has_backward</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_has_backward</span> <span class="o">=</span> <span class="n">has_backward</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_first</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns true if this stage is the first stage in the pipeline.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_last</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns true if this stage is the last stage in the pipeline.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_check_chunk_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Attempted to access chunk_id before chunks have been configured.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">chunk_id</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunks</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Chunk id </span><span class="si">{</span><span class="n">chunk_id</span><span class="si">}</span><span class="s2"> is out of range [0, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">chunks</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_configure_outputs_meta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs_meta</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Track the output shapes/dtype of this stage since they determine the send operation(s) which must match</span>
<span class="sd">        recv operations of the next stage.  The next stage _will_ be freezing its recv buffers based on its initial</span>
<span class="sd">        configuration, so it&#39;s important to also freeze/validate the output side to avoid any send/recv mismatches</span>
<span class="sd">        which could show up as hangs, silent corruption, or other errors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_outputs_meta</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Attempting to reconfigure output_meta, which is not supported&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_outputs_meta</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">outputs_meta</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="nf">get_outputs_meta</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the output metadata (meta tensors) reprensenting the outputs of this stage&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_outputs_meta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Attempted to get_outputs_meta() without configuring output meta&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outputs_meta</span>

    <span class="k">def</span> <span class="nf">_create_grad_send_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">args_recv_info</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a list of stage indices to send gradients to.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">grad_send_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span> <span class="nf">map_recv_to_send</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
            <span class="c1"># Note: we send gradients back to previous stage as long as in</span>
            <span class="c1"># forward it is a received input, regardless of whether it requires</span>
            <span class="c1"># grad. It is up to the previous stage to disgard this gradient.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">_RecvInfo</span><span class="p">):</span>
                <span class="n">grad_send_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">source</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">source</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad_send_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="k">return</span> <span class="kc">None</span>

        <span class="n">map_aggregate</span><span class="p">(</span><span class="n">args_recv_info</span><span class="p">,</span> <span class="n">map_recv_to_send</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Grad send info: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span> <span class="n">grad_send_info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_send_info</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_prepare_forward_infra</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_microbatches</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_prepare_backward_infra</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_microbatches</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># TODO: this is needed for backward_maybe_with_nosync</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunks</span> <span class="o">=</span> <span class="n">num_microbatches</span>

        <span class="k">for</span> <span class="n">mb_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_microbatches</span><span class="p">):</span>
            <span class="c1"># `grad_recv_info` is a mirror of `act_send_info`</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_recv_info</span><span class="p">[</span><span class="n">mb_index</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_grad_recv_info</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">act_send_info</span>
            <span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_create_grad_recv_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">act_send_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_RecvInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_get_recv_ops</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">recv_infos</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">InputInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function shared by `get_fwd_recv_ops` and `get_bwd_recv_ops`.</span>
<span class="sd">        Returns a list of ops that correspond to the recv infos.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ops</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">recv_infos</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">_RecvInfo</span><span class="p">):</span>
                <span class="k">continue</span>

            <span class="n">peer_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_index_to_group_rank</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">source</span><span class="p">]</span>
            <span class="n">peer_global_rank</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">peer_rank</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="n">peer_rank</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># TODO</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">,</span> <span class="n">info</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="n">peer_global_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">ops</span>

    <span class="k">def</span> <span class="nf">get_fwd_recv_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of ops that are needed to receive the input arguments</span>
<span class="sd">        for this stage.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">recv_infos</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">InputInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">[</span><span class="n">fwd_chunk_id</span><span class="p">]</span>

        <span class="c1"># In case there is backward pass, set requires_grad for receive buffers</span>
        <span class="c1"># before first forward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_backward</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_requires_grad</span><span class="p">[</span><span class="n">fwd_chunk_id</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">recv_infos</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">_RecvInfo</span><span class="p">):</span>
                    <span class="n">a</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_recv_ops</span><span class="p">(</span><span class="n">recv_infos</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_bwd_recv_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of ops that are needed to receive the gradients</span>
<span class="sd">        for this stage.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_backward</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="n">recv_infos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_recv_info</span><span class="p">[</span><span class="n">bwd_chunk_id</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_recv_ops</span><span class="p">(</span><span class="n">recv_infos</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_fwd_send_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the activation send ops for current stage&#39;s forward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_chunks</span><span class="p">[</span><span class="n">fwd_chunk_id</span><span class="p">]</span>
        <span class="c1"># Unify output form to tuple for easy correspondance with</span>
        <span class="c1"># `act_send_info`</span>
        <span class="n">output_tuple</span> <span class="o">=</span> <span class="n">output</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">tuple</span> <span class="k">else</span> <span class="p">(</span><span class="n">output</span><span class="p">,)</span>

        <span class="n">ops</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tuple</span><span class="p">):</span>
            <span class="n">dst_stages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_send_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">dst</span> <span class="ow">in</span> <span class="n">dst_stages</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">dst</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Sending tensor to Stage </span><span class="si">%s</span><span class="s2">: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span>
                    <span class="n">dst</span><span class="p">,</span>
                    <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                <span class="p">)</span>
                <span class="n">peer_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_index_to_group_rank</span><span class="p">[</span><span class="n">dst</span><span class="p">]</span>
                <span class="n">peer_global_rank</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">peer_rank</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="n">peer_rank</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># TODO</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">peer_global_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">ops</span>

    <span class="k">def</span> <span class="nf">get_bwd_send_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the gradient send ops for current stage&#39;s backward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_chunk_id</span><span class="p">(</span><span class="n">bwd_chunk_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_backward</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Create bwd send infra lazily</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_send_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Send info for input grads during backward:</span>
            <span class="c1"># List of destinations corresponding to input grads</span>
            <span class="c1"># Can be None if an input has no grad</span>
            <span class="c1"># `grad_send_info` is a mirror of `args_recv_info`</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_send_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_grad_send_info</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">ops</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">grad_recv_stage</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grads_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_send_info</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">grad_recv_stage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Sending gradient to Stage </span><span class="si">%s</span><span class="s2">: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span>
                    <span class="n">grad_recv_stage</span><span class="p">,</span>
                    <span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                <span class="p">)</span>
                <span class="n">peer_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_index_to_group_rank</span><span class="p">[</span><span class="n">grad_recv_stage</span><span class="p">]</span>
                <span class="n">peer_global_rank</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">peer_rank</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="n">peer_rank</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># TODO</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">peer_global_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">grad_recv_stage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="si">}</span><span class="s2">] for chunk </span><span class="si">{</span><span class="n">bwd_chunk_id</span><span class="si">}</span><span class="s2"> has gradients </span><span class="si">{</span><span class="n">grad</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;and is expecting to send gradients to stage </span><span class="si">{</span><span class="n">grad_recv_stage</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span>

    <span class="k">def</span> <span class="nf">clear_runtime_states</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clear runtime states of the stage.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># map microbatch ID to list of forward tensor args</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fwd_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="c1"># Caching chunk outputs for final output merge or reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_chunks</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="c1"># Reset bwd chunk counter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_seen_bwd_chunks</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Clear grad of input buffers in between schedule steps. This is because</span>
        <span class="c1"># `torch.autograd.backward()` will accumulate gradients into leaf</span>
        <span class="c1"># tensors by default. For gradients to pass back to previous stages, we</span>
        <span class="c1"># don&#39;t want such accumulation.</span>
        <span class="k">for</span> <span class="n">recv_tuple</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>  <span class="c1"># iterate over all chunks</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">recv_tuple</span><span class="p">:</span>  <span class="c1"># iterate over all input args</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">_RecvInfo</span><span class="p">):</span>
                    <span class="c1"># Set to None is the newer and recommended way to clear grads, compared to `zero_()`.</span>
                    <span class="c1"># See https://github.com/pytorch/pytorch/pull/92731</span>
                    <span class="n">a</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_map_tensor_from_recv_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">recv_infos</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">InputInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Map tensors from recv infos to a list.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">get_recv_tensor</span><span class="p">(</span><span class="n">info</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">_RecvInfo</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">info</span><span class="o">.</span><span class="n">buffer</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected _RecvInfo but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">info</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">tensors</span> <span class="o">=</span> <span class="n">map_aggregate</span><span class="p">(</span>
            <span class="n">recv_infos</span><span class="p">,</span>
            <span class="n">get_recv_tensor</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">tensors</span>

    <span class="k">def</span> <span class="nf">_retrieve_recv_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve the activations received for the current stage during forward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">recv_infos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">[</span><span class="n">fwd_chunk_id</span><span class="p">]</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_tensor_from_recv_info</span><span class="p">(</span><span class="n">recv_infos</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">activations</span>

    <span class="k">def</span> <span class="nf">_retrieve_recv_grads</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve the gradients received for the current stage during backward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">recv_infos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_recv_info</span><span class="p">[</span><span class="n">bwd_chunk_id</span><span class="p">]</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_tensor_from_recv_info</span><span class="p">(</span><span class="n">recv_infos</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grads</span>

    <span class="k">def</span> <span class="nf">forward_maybe_with_nosync</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># If submod is wrapped with DDP, we use the `no_sync` context manager to</span>
        <span class="c1"># avoid gradient all-reduce per microbatch</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="p">,</span> <span class="n">DistributedDataParallel</span><span class="p">):</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">no_sync</span><span class="p">():</span>  <span class="c1"># type: ignore[operator]</span>
                <span class="n">out_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_val</span>

    <span class="k">def</span> <span class="nf">backward_maybe_with_nosync</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backward_type</span><span class="p">,</span> <span class="n">bwd_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether using PP with FSDP or DDP, there are some runtime differences between the last backward step and the</span>
<span class="sd">        other steps.  Namely, we need to accumulate gradients on previous steps and reduce them on the last step, but</span>
<span class="sd">        there are additional state-variables and performance considerations depending on the data parallelism used.</span>
<span class="sd">        This helper should adapt any pipeline parallel schedule to work with common/supported data parallel libraries.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">full_backward</span> <span class="o">=</span> <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;full_backward&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">full_backward</span><span class="p">:</span>
            <span class="n">last_backward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seen_bwd_chunks</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunks</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># type: ignore[operator]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For backwards are split into weight and input, we will see twice as many bwd_chunks</span>
            <span class="n">last_backward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seen_bwd_chunks</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunks</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># type: ignore[operator]</span>

        <span class="k">def</span> <span class="nf">perform_backward</span><span class="p">(</span><span class="n">backward_type</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">backward_type</span> <span class="o">==</span> <span class="s2">&quot;full&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">stage_backward</span><span class="p">(</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;stage_output&quot;</span><span class="p">],</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;output_grads&quot;</span><span class="p">],</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;input_values&quot;</span><span class="p">],</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">backward_type</span> <span class="o">==</span> <span class="s2">&quot;input&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">stage_backward_input</span><span class="p">(</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;stage_output&quot;</span><span class="p">],</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;output_grads&quot;</span><span class="p">],</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;input_values&quot;</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">backward_type</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">stage_backward_weight</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown backward type: </span><span class="si">{</span><span class="n">backward_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># If submod is wrapped by DDP</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="p">,</span> <span class="n">DistributedDataParallel</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">last_backward</span><span class="p">:</span>
                <span class="c1"># Last chunk, prepare for gradient reduction</span>
                <span class="c1"># HACK: reaching into DDP implementation details here. Is there a better way?</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">prepare_for_backward</span><span class="p">(</span>  <span class="c1"># type: ignore[union-attr, operator]</span>
                    <span class="nb">list</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">_find_tensors</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;stage_output&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">perform_backward</span><span class="p">(</span><span class="n">backward_type</span><span class="p">)()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">no_sync</span><span class="p">():</span>  <span class="c1"># type: ignore[operator]</span>
                    <span class="n">result</span> <span class="o">=</span> <span class="n">perform_backward</span><span class="p">(</span><span class="n">backward_type</span><span class="p">)()</span>
        <span class="c1"># If submod is a FSDP module</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="p">,</span> <span class="n">FSDPModule</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">set_is_last_backward</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">set_reshard_after_backward</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">set_requires_gradient_sync</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">perform_backward</span><span class="p">(</span><span class="n">backward_type</span><span class="p">)()</span>
            <span class="k">if</span> <span class="n">last_backward</span><span class="p">:</span>
                <span class="c1"># Manually call post backward for FSDP</span>
                <span class="k">def</span> <span class="nf">run_post_backward</span><span class="p">(</span><span class="n">fsdp_module</span><span class="p">:</span> <span class="n">FSDPModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">fsdp_module</span><span class="o">.</span><span class="n">set_is_last_backward</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                    <span class="n">fsdp_module</span><span class="o">.</span><span class="n">set_reshard_after_backward</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                    <span class="n">fsdp_module</span><span class="o">.</span><span class="n">set_requires_gradient_sync</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                    <span class="n">fsdp_state</span> <span class="o">=</span> <span class="n">fully_shard</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="n">fsdp_module</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">fsdp_state</span><span class="o">.</span><span class="n">_state_ctx</span><span class="o">.</span><span class="n">all_states</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">_fsdp_param_group</span><span class="p">:</span>
                            <span class="n">state</span><span class="o">.</span><span class="n">_fsdp_param_group</span><span class="o">.</span><span class="n">post_backward</span><span class="p">()</span>

                <span class="n">run_post_backward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Non-DP submodule, regular backward</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">perform_backward</span><span class="p">(</span><span class="n">backward_type</span><span class="p">)()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_seen_bwd_chunks</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># for stage_backward_input()</span>
            <span class="n">grads</span><span class="p">,</span> <span class="n">param_groups</span> <span class="o">=</span> <span class="n">result</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">grads</span><span class="p">,</span> <span class="n">param_groups</span> <span class="o">=</span> <span class="n">result</span><span class="p">,</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">param_groups</span>

    <span class="k">def</span> <span class="nf">forward_one_chunk</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">fwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform forward pass on the stage with one microbatch.</span>
<span class="sd">        `args` and `kwargs` are the inputs from *external* to this stage. They</span>
<span class="sd">        applies only to the first stage in most cases.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first</span><span class="p">:</span>
            <span class="c1"># First stage doesn&#39;t need to receive anything</span>
            <span class="n">composite_args</span> <span class="o">=</span> <span class="n">args</span>
            <span class="n">composite_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Receive activations for this chunk</span>
            <span class="c1"># Activations only come in args form</span>
            <span class="n">composite_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_recv_activations</span><span class="p">(</span><span class="n">fwd_chunk_id</span><span class="p">)</span>
            <span class="n">composite_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_fwd_input</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Compute forward</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_maybe_with_nosync</span><span class="p">(</span><span class="o">*</span><span class="n">composite_args</span><span class="p">,</span> <span class="o">**</span><span class="n">composite_kwargs</span><span class="p">)</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">exc_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="si">}</span><span class="s2"> failed to run forward:</span>
<span class="s2">            args: </span><span class="si">{</span><span class="n">map_debug_info</span><span class="p">(</span><span class="n">composite_args</span><span class="p">)</span><span class="si">}</span>
<span class="s2">            kwargs: </span><span class="si">{</span><span class="n">map_debug_info</span><span class="p">(</span><span class="n">composite_kwargs</span><span class="p">)</span><span class="si">}</span>
<span class="s2">            &quot;&quot;&quot;</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">exc_msg</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>
            <span class="c1"># HACK: this is a hacky workaround for the fact that export creates</span>
            <span class="c1"># output in list format</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># Unify output form to tuple for easy correspondance with</span>
        <span class="c1"># `act_send_info`</span>
        <span class="n">output_tuple</span> <span class="o">=</span> <span class="n">output</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">tuple</span> <span class="k">else</span> <span class="p">(</span><span class="n">output</span><span class="p">,)</span>
        <span class="c1"># Prepare for final output merge or reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># Save activations and inputs for backward</span>
        <span class="n">flat_args</span> <span class="o">=</span> <span class="n">flatten_args</span><span class="p">(</span><span class="n">composite_args</span><span class="p">)</span>
        <span class="n">flat_kwargs</span> <span class="o">=</span> <span class="n">flatten_args</span><span class="p">(</span><span class="n">composite_kwargs</span><span class="p">)</span>
        <span class="n">flatten_input_tensors</span> <span class="o">=</span> <span class="n">flat_args</span> <span class="o">+</span> <span class="n">flat_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fwd_cache</span><span class="p">[</span><span class="n">fwd_chunk_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_tuple</span><span class="p">,</span>  <span class="c1"># stage_output</span>
            <span class="n">flatten_input_tensors</span><span class="p">,</span>  <span class="c1"># input_values</span>
        <span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Forwarded chunk </span><span class="si">%s</span><span class="s2">, outputs: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span>
            <span class="n">fwd_chunk_id</span><span class="p">,</span>
            <span class="n">map_debug_info</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_fwd_outputs</span><span class="p">(</span><span class="n">output_tuple</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">backward_one_chunk</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">bwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">full_backward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform backward pass on the module.</span>
<span class="sd">        This should only be called once per microbatch.</span>

<span class="sd">        If full_backward is True (the default), the full backward pass including weight and input gradients will be run,</span>
<span class="sd">        and it is an error to call `backward_weight_one_chunk` for this bwd_chunk_id.</span>

<span class="sd">        If full_backward is False, it is optional that `dw_runner` was provided to the PipelineStage at __init__ time,</span>
<span class="sd">        and a subsequent call to `backward_weight_one_chunk` is required to invoke dw_runner and complete the backward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_chunk_id</span><span class="p">(</span><span class="n">bwd_chunk_id</span><span class="p">)</span>

        <span class="p">(</span>
            <span class="n">stage_output</span><span class="p">,</span>
            <span class="n">input_values</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fwd_cache</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">bwd_chunk_id</span><span class="p">)</span>

        <span class="c1"># Compute backward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last</span><span class="p">:</span>
            <span class="c1"># Last stage computes gradients from loss and has no gradients from</span>
            <span class="c1"># next stage</span>
            <span class="n">bwd_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;stage_output&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
                <span class="s2">&quot;output_grads&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s2">&quot;input_values&quot;</span><span class="p">:</span> <span class="n">input_values</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Otherwise, receive gradients from next stage</span>
            <span class="n">grads_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_recv_grads</span><span class="p">(</span><span class="n">bwd_chunk_id</span><span class="p">)</span>
            <span class="c1"># If an input to the pipeline requires gradient,</span>
            <span class="c1"># `torch.autograd.backward` will accumulate the gradient into the</span>
            <span class="c1"># `.grad` field of such input</span>
            <span class="n">bwd_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;stage_output&quot;</span><span class="p">:</span> <span class="n">stage_output</span><span class="p">,</span>
                <span class="s2">&quot;output_grads&quot;</span><span class="p">:</span> <span class="n">grads_output</span><span class="p">,</span>
                <span class="s2">&quot;input_values&quot;</span><span class="p">:</span> <span class="n">input_values</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="c1"># Save full_backward</span>
        <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;full_backward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">full_backward</span>

        <span class="c1"># Custom backward function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dw_builder</span><span class="p">:</span>
            <span class="c1"># TODO: We may want to change our semantics so we are allowed to ignore</span>
            <span class="c1"># the &#39;dw_builder&#39; and call full_backward directly when it is a full_backward op.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grads_input</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_maybe_with_nosync</span><span class="p">(</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">bwd_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">full_backward</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dw_builder</span><span class="p">()()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dw_runner</span><span class="p">[</span><span class="n">bwd_chunk_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dw_builder</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">full_backward</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grads_input</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_maybe_with_nosync</span><span class="p">(</span>
                    <span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">bwd_kwargs</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># perform the partial backwards for the inputs with a custom backward function</span>
                <span class="c1"># when the &quot;stage_ouput&quot; is a loss, then it is a tensor, otherwise it is a tuple of tensors</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;stage_output&quot;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;stage_output&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;stage_output&quot;</span><span class="p">],)</span>

                <span class="n">grads_input</span><span class="p">,</span> <span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_maybe_with_nosync</span><span class="p">(</span>
                    <span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">bwd_kwargs</span>
                <span class="p">)</span>

                <span class="c1"># TODO: we dont need to save this, add to dw_runner?</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backward_state</span><span class="p">[</span><span class="n">bwd_chunk_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">input_values</span><span class="p">,</span>
                    <span class="n">param_groups</span><span class="p">,</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;stage_output&quot;</span><span class="p">],</span>
                    <span class="n">bwd_kwargs</span><span class="p">[</span><span class="s2">&quot;output_grads&quot;</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grads_input</span> <span class="o">=</span> <span class="n">grads_input</span>
                <span class="c1"># Save a placeholder for the dw_runner</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dw_runner</span><span class="p">[</span><span class="n">bwd_chunk_id</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="kc">None</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Backwarded chunk </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span> <span class="n">bwd_chunk_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">backward_weight_one_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bwd_chunk_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">bwd_chunk_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dw_runner</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="si">}</span><span class="s2"> Attempted to run backward_weight_one_chunk for chunk </span><span class="si">{</span><span class="n">bwd_chunk_id</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="s2">&quot; without first calling `backward_one_chunk(full_backward=False)`&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dw_builder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dw_runner</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">bwd_chunk_id</span><span class="p">)()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">input_values</span><span class="p">,</span>
                <span class="n">param_groups</span><span class="p">,</span>
                <span class="n">stage_output</span><span class="p">,</span>
                <span class="n">output_grads</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">bwd_chunk_id</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">bwd_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;stage_output&quot;</span><span class="p">:</span> <span class="n">stage_output</span><span class="p">,</span>
                    <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">,</span>
                    <span class="s2">&quot;full_backward&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="n">weight_grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_maybe_with_nosync</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">bwd_kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># TODO: figure out a better way to do this:</span>
                <span class="c1"># if inputs does not require gradient,</span>
                <span class="c1"># then the parameter group will not be fully captured during stage_backward_input</span>
                <span class="c1"># in this case, we need call grad directly on the parameters</span>
                <span class="c1"># To solve: make input fn do the intersect compute and then finish it off during W</span>
                <span class="n">bwd_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;stage_output&quot;</span><span class="p">:</span> <span class="n">stage_output</span><span class="p">,</span>
                    <span class="s2">&quot;output_grads&quot;</span><span class="p">:</span> <span class="n">output_grads</span><span class="p">,</span>
                    <span class="s2">&quot;input_values&quot;</span><span class="p">:</span> <span class="n">input_values</span><span class="p">,</span>
                    <span class="s2">&quot;full_backward&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backward_maybe_with_nosync</span><span class="p">(</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">bwd_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_fwd_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Raises a RuntimeError if shapes of input args/kwargs do not match the shapes configured for this stage.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first</span><span class="p">:</span>
            <span class="c1"># TODO why is there a separate recv_info for each pipeline chunk?</span>
            <span class="c1"># kwen2501: to avoid passing a `fwd_chunk_id` to this function, we</span>
            <span class="c1"># check all chunks against args_recv_info[0]</span>
            <span class="n">expected_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We don&#39;t check inputs for non-0 stages assuming they don&#39;t accept</span>
            <span class="c1"># user inputs in canonical pipeline scenarios</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="c1"># TODO- need a mapping of kwarg to position in self.args_recv_info</span>
            <span class="c1"># without it, we just validate shapes for args and ignore kwargs</span>
            <span class="n">expected_args</span> <span class="o">=</span> <span class="n">expected_args</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected_args</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)]</span>

        <span class="c1"># TODO- need a mapping of kwarg to position in self.args_recv_info</span>
        <span class="c1"># maybe it&#39;s impossible to tell whether the len mismatches because</span>
        <span class="c1"># (a) the user passed an extra arg or missed an arg</span>
        <span class="c1"># (b) the user did not pass a kwarg, which has a default value baked into expected_args</span>
        <span class="n">expected_tensors_meta</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">e</span><span class="o">.</span><span class="n">meta</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">_RootArgPlaceholder</span><span class="p">)</span> <span class="k">else</span> <span class="n">e</span><span class="o">.</span><span class="n">buffer</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">expected_args</span>
        <span class="p">]</span>
        <span class="n">validate_tensors_metadata</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Stage </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="si">}</span><span class="s2"> forward inputs&quot;</span><span class="p">,</span> <span class="n">expected_tensors_meta</span><span class="p">,</span> <span class="n">args</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_fwd_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Raises a RuntimeError if this stage produces an output of unexpected shape/dtype.</span>
<span class="sd">        Most likely, this could be cause either by incorrect user specification of output shapes, or becuase</span>
<span class="sd">        shape inference was done on the original model but then at runtime the model is wrapped with something like</span>
<span class="sd">        mixed precision which changes output dtype.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">expected_tensors_meta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_outputs_meta</span><span class="p">()</span>
        <span class="n">validate_tensors_metadata</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Stage </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="si">}</span><span class="s2"> forward outputs&quot;</span><span class="p">,</span> <span class="n">expected_tensors_meta</span><span class="p">,</span> <span class="n">outputs</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_PipelineStage</span><span class="p">(</span><span class="n">_PipelineStageBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">stage_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">stage_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">pipe_info</span><span class="p">:</span> <span class="n">PipeInfo</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a pipeline stage given a stage_module to be wrapped by this stage</span>
<span class="sd">        and a `pipe_info` describing the stage relationship of the pipeline.</span>

<span class="sd">        Args:</span>
<span class="sd">            stage_module (torch.nn.Module): the module to be wrapped by this stage</span>
<span class="sd">            stage_index (int): the index of this stage in the pipeline</span>
<span class="sd">            pipe_info (PipeInfo): information about the pipeline, can be retrieved by `pipe.info()`</span>
<span class="sd">            device (torch.device): the device to be used by this stage</span>
<span class="sd">            group (Optional[dist.ProcessGroup]): the process group to be used by this stage</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_PipelineStageBase</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">stage_module</span><span class="p">,</span>
            <span class="n">stage_index</span><span class="p">,</span>
            <span class="n">pipe_info</span><span class="o">.</span><span class="n">num_stages</span><span class="p">,</span>
            <span class="n">device</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pipe_info</span> <span class="o">=</span> <span class="n">pipe_info</span>

        <span class="c1"># Find stage nodes in graph</span>
        <span class="n">submod_nodes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">node</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">pipe_info</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">submod_nodes</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of submodules in pipe graph </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">submod_nodes</span><span class="p">)</span><span class="si">}</span><span class="s2"> does not match number of stages </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Find my stage node in graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">submod_nodes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">name</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;[</span><span class="si">%s</span><span class="s2">] Creating PipelineStage </span><span class="si">%s</span><span class="s2"> for </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">group_rank</span><span class="p">,</span>
            <span class="n">stage_index</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Create mapping from stage name to stage index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">submod_to_stage_index</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">submod_nodes</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">submod_to_stage_index</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

        <span class="c1"># Cast submodule to device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_move_submod_to_device</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_move_submod_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Move submodule to indicated device if possible</span>
        <span class="c1"># Note: we cannot move meta module to real devices because meta tensors</span>
        <span class="c1"># do not support to() method. One needs to do an in-place tensor swap in</span>
        <span class="c1"># that case.</span>
        <span class="n">has_meta_param</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_meta</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">has_meta_param</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Found meta parameters!&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_forward_infra</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_microbatches</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create send/recv infrastructures for activations (during forward)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Flag per chunk to keep track of whether we have set `requires_grad`</span>
        <span class="c1"># for receive buffers. Format: {chunk : Boolean}</span>
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_microbatches</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">[</span><span class="n">chunk</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_act_recv_info</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_requires_grad</span><span class="p">[</span><span class="n">chunk</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Send info during forward for each activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_send_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_act_send_info</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_stage_index_of_submod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">submod_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a submodule name, return the stage index of the submodule.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">submod_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod_to_stage_index</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stage id of </span><span class="si">{</span><span class="n">submod_name</span><span class="si">}</span><span class="s2"> not found&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod_to_stage_index</span><span class="p">[</span><span class="n">submod_name</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_create_act_recv_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a tuple of `_RecvInfo` for inputs to the stage.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">create_recv_tensor</span><span class="p">(</span><span class="n">placeholder</span><span class="p">,</span> <span class="n">arg_node</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Create a receive buffer for a placeholder.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">example_value</span> <span class="o">=</span> <span class="n">placeholder</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">arg_node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
                <span class="c1"># This is a root level placeholder, thus an input argument to the entire model.</span>
                <span class="c1"># We are likely at stage 0, hence no need to create a receive buffer.</span>
                <span class="k">return</span> <span class="n">_RootArgPlaceholder</span><span class="p">(</span><span class="n">example_value</span><span class="p">)</span>

            <span class="c1"># Figure out the source stage of this input</span>
            <span class="k">while</span> <span class="n">arg_node</span><span class="o">.</span><span class="n">target</span> <span class="ow">is</span> <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span><span class="p">:</span>
                <span class="c1"># If the input is a getitem, we need to go deeper</span>
                <span class="n">arg_node</span> <span class="o">=</span> <span class="n">arg_node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">arg_node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Expecting call_module, got </span><span class="si">{</span><span class="n">arg_node</span><span class="o">.</span><span class="n">op</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">src_stage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_stage_index_of_submod</span><span class="p">(</span><span class="n">arg_node</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

            <span class="c1"># Create a receive buffer for this placeholder</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Creating recv buffer for input &#39;</span><span class="si">%s</span><span class="s2">&#39; : </span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span>
                <span class="n">placeholder</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">example_value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">example_value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">buffer</span> <span class="o">=</span> <span class="n">_make_tensor_from_meta</span><span class="p">(</span><span class="n">example_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">_RecvInfo</span><span class="p">(</span>
                <span class="n">arg_node</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">src_stage</span><span class="p">,</span>
                <span class="n">buffer</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">args_recv_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Filter out placeholder nodes from `self.submod` (a GraphModule)</span>
        <span class="n">placeholders</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">node</span><span class="p">:</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span>
        <span class="p">)</span>
        <span class="c1"># `placeholders` are nodes internal to submod.</span>
        <span class="c1"># `self.node.args` are dependency nodes in the outer graph.</span>
        <span class="c1"># The two are 1:1.</span>
        <span class="k">for</span> <span class="n">placeholder</span><span class="p">,</span> <span class="n">arg_node</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">placeholders</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">):</span>
            <span class="c1"># Create a receive buffer for this placeholder</span>
            <span class="n">recv_info</span> <span class="o">=</span> <span class="n">create_recv_tensor</span><span class="p">(</span><span class="n">placeholder</span><span class="p">,</span> <span class="n">arg_node</span><span class="p">)</span>
            <span class="n">args_recv_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">recv_info</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Activation recv / args info: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span> <span class="n">args_recv_info</span>
        <span class="p">)</span>
        <span class="c1"># `args` is a Tuple, hence we will return a Tuple[InputInfo]</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args_recv_info</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_dst_rank</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">user</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the destination rank of a `user` node.</span>
<span class="sd">        If the `user` is not a submod, `None` may be returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">user</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
            <span class="c1"># User is a stage (`call_module`)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_stage_index_of_submod</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># - If user.op == &quot;output&quot;:</span>
            <span class="c1">#   No need to send back to rank 0</span>
            <span class="c1"># - If user.target is stage_backward:</span>
            <span class="c1">#   No need to send assuming submod output is stored locally or</span>
            <span class="c1">#   should be re-calucated in case of activation checkpointing</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_create_act_send_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a dict of send info for activations.</span>
<span class="sd">        The dict is of the form:</span>
<span class="sd">        {</span>
<span class="sd">            output_index: [dst_rank_0, dst_rank_1, ...],</span>
<span class="sd">            ...</span>
<span class="sd">        }</span>
<span class="sd">        where the list of `dst_rank`s covers the case where an output value may</span>
<span class="sd">        be consumed by multiple stages.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Output index: List of receiver ranks</span>
        <span class="n">act_send_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">out_idx</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">users</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">user</span><span class="o">.</span><span class="n">target</span> <span class="ow">is</span> <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span><span class="p">:</span>
                <span class="c1"># Recursively find the real destination</span>
                <span class="n">gi_dsts</span> <span class="o">=</span> <span class="n">act_send_info</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">out_idx</span><span class="p">,</span> <span class="p">[])</span>
                <span class="k">for</span> <span class="n">gi_user</span> <span class="ow">in</span> <span class="n">user</span><span class="o">.</span><span class="n">users</span><span class="p">:</span>
                    <span class="n">dst_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_dst_rank</span><span class="p">(</span><span class="n">gi_user</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">dst_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">gi_dsts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dst_rank</span><span class="p">)</span>
                <span class="c1"># Next `getitem` will point to the next output index</span>
                <span class="n">out_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># In case of single output value, `out_idx` will not increase</span>
                <span class="n">dsts</span> <span class="o">=</span> <span class="n">act_send_info</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">out_idx</span><span class="p">,</span> <span class="p">[])</span>
                <span class="n">dst_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_dst_rank</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">dst_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">dsts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dst_rank</span><span class="p">)</span>

        <span class="n">output_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_node</span><span class="p">()</span>
        <span class="n">output_vals</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">v</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">flatten_args</span><span class="p">(</span><span class="n">output_node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_configure_outputs_meta</span><span class="p">(</span><span class="n">output_vals</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Send info: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span> <span class="n">act_send_info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">act_send_info</span>

    <span class="k">def</span> <span class="nf">_get_output_node</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">output_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">node</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_nodes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">output_node</span> <span class="o">=</span> <span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">output_node</span>

    <span class="k">def</span> <span class="nf">_create_grad_recv_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">act_send_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_RecvInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a tuple of `_RecvInfo` for gradients.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Dict[output_index, _RecvInfo]</span>
        <span class="n">grad_recv_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">_RecvInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">output_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_node</span><span class="p">()</span>

        <span class="c1"># The output node may take multiple args, meaning the submod having multiple output values.</span>
        <span class="n">output_vals</span> <span class="o">=</span> <span class="n">flatten_args</span><span class="p">(</span><span class="n">output_node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">out_idx</span><span class="p">,</span> <span class="n">dst_list</span> <span class="ow">in</span> <span class="n">act_send_info</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">dst_list</span><span class="p">:</span>
                <span class="c1"># No actual receiver for activation so no grad coming back</span>
                <span class="k">continue</span>

            <span class="n">output</span> <span class="o">=</span> <span class="n">output_vals</span><span class="p">[</span><span class="n">out_idx</span><span class="p">]</span>
            <span class="n">example_value</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="si">}</span><span class="s2"> Creating grad recv buffer for output </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> &quot;</span>  <span class="c1"># noqa: G004</span>
                <span class="sa">f</span><span class="s2">&quot;: </span><span class="si">{</span><span class="n">example_value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">example_value</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

            <span class="c1"># TODO: otherwise needs grad accumulation</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">dst_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Backward of skip connections not supported yet&quot;</span>
            <span class="n">grad_src</span> <span class="o">=</span> <span class="n">dst_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">grad_recv_info</span><span class="p">[</span><span class="n">out_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">_RecvInfo</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">grad_src</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>  <span class="c1"># noqa: G004</span>
                <span class="n">grad_src</span><span class="p">,</span>
                <span class="n">_make_tensor_from_meta</span><span class="p">(</span><span class="n">example_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="c1"># Convert to tuple for convenience in get_ops and retrieve tensor</span>
        <span class="n">grad_recv_info_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">grad_recv_info</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> Grad recv info: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_prefix</span><span class="p">,</span> <span class="n">grad_recv_info_tuple</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_recv_info_tuple</span>


<span class="c1"># A helper function to create a pipeline stage based on traced pipeline information</span>
<div class="viewcode-block" id="build_stage"><a class="viewcode-back" href="../../../../distributed.pipelining.html#torch.distributed.pipelining.stage.build_stage">[docs]</a><span class="k">def</span> <span class="nf">build_stage</span><span class="p">(</span>
    <span class="n">stage_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">stage_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">pipe_info</span><span class="p">:</span> <span class="n">PipeInfo</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_PipelineStage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a pipeline stage given a stage_module to be wrapped by this stage</span>
<span class="sd">    and pipeline information.</span>

<span class="sd">    Args:</span>
<span class="sd">        stage_module (torch.nn.Module): the module to be wrapped by this stage</span>
<span class="sd">        stage_index (int): the index of this stage in the pipeline</span>
<span class="sd">        pipe_info (PipeInfo): information about the pipeline, can be retrieved by `pipe.info()`</span>
<span class="sd">        device (torch.device): the device to be used by this stage</span>
<span class="sd">        group (Optional[dist.ProcessGroup]): the process group to be used by this stage</span>

<span class="sd">    Returns:</span>
<span class="sd">        _PipelineStage: a pipeline stage that can run with `PipelineSchedules`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_PipelineStage</span><span class="p">(</span>
        <span class="n">stage_module</span><span class="p">,</span>
        <span class="n">stage_index</span><span class="p">,</span>
        <span class="n">pipe_info</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
        <span class="n">group</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="c1"># Manual PipelineStage functions and definition</span>

<span class="n">METADATA_TENSOR_LEN</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">PLACEHOLDER_VAL</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>


<span class="k">def</span> <span class="nf">_create_empty_tensors</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a list of empty tensors with the same properties (like shape and dtype) as the input tensor(s),</span>
<span class="sd">    and places them on the specified device.</span>
<span class="sd">    Args:</span>
<span class="sd">        tensor (Union[torch.Tensor, List[torch.tensor]]): The input tensor(s).</span>
<span class="sd">        device (torch.device): The device where the new tensors will be placed.</span>
<span class="sd">    Returns:</span>
<span class="sd">        List[torch.Tensor]: A list of empty tensors with the same properties as the input tensor(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)]</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor</span><span class="p">]</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2"> cannot create empty tensors&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_metadata_tensor</span><span class="p">(</span>
    <span class="n">tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a metadata tensor that can be sent over the wire.</span>
<span class="sd">    This tensor contains the number of dimensions and the shape of each tensor being sent.</span>

<span class="sd">    The data is of format [num_dims, dim1, dim2, ...].</span>
<span class="sd">    If the tensor is None, a tensor of only placeholder values will be returned.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        tensors: A list of tensors, the tensors will converted into its shape dimensions and</span>
<span class="sd">                 these dimensions will be concatenated.</span>
<span class="sd">        device: The device where the metadata tensor will be created.</span>
<span class="sd">    If the tensor is None, then this tensor will contain PLACEHOLDER_VALs.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">metadata_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
        <span class="p">(</span><span class="n">METADATA_TENSOR_LEN</span><span class="p">,),</span>
        <span class="n">PLACEHOLDER_VAL</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="c1"># Create a list of tensors containing the number of dimensions and the shape of each tensor</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># data is of format [num_dims, dim1, dim2, ...]</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span>
        <span class="p">]</span>
        <span class="c1"># Concatenate the data into a single tensor</span>
        <span class="n">data_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">dt_shape</span> <span class="o">=</span> <span class="n">data_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">dt_shape</span> <span class="o">&gt;</span> <span class="n">METADATA_TENSOR_LEN</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Metadata tensor size (</span><span class="si">{</span><span class="n">dt_shape</span><span class="si">}</span><span class="s2">) exceeds maximum allowed length (</span><span class="si">{</span><span class="n">METADATA_TENSOR_LEN</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
        <span class="n">metadata_tensor</span><span class="p">[:</span><span class="n">dt_shape</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_tensor</span>
    <span class="k">return</span> <span class="n">metadata_tensor</span>


<span class="k">def</span> <span class="nf">_extract_metadata_from_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the number of dimensions and the shape of each tensor from a metadata tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">PLACEHOLDER_VAL</span><span class="p">:</span>
        <span class="n">num_dims</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">tensor</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">num_dims</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">metadata</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="n">num_dims</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">metadata</span>


<span class="k">def</span> <span class="nf">_get_stage_shapes</span><span class="p">(</span>
    <span class="n">stage_modules</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
    <span class="n">stage_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">num_stages</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">microbatch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a dry run through all the pipeline stages (a rank can have multiple pipeline stages in the case of</span>
<span class="sd">    virtual pipelining) and returns the shape of the inputs and outputs of the module.</span>
<span class="sd">    Only the first stage must pass in a microbatch.</span>

<span class="sd">    Each rank must call _get_stage_shapes or the program will hang.</span>

<span class="sd">    Args:</span>
<span class="sd">        stage_modules: The chunks assigned to this rank. Rhe length should be 1 for any</span>
<span class="sd">                non-interleaved schedules and &gt;1 for any interleaved schedules.</span>
<span class="sd">        stage_ids: The id of the stages assigned to this rank.</span>
<span class="sd">        num_stages: Total number of stages.</span>
<span class="sd">        rank: Rank of the current process.</span>
<span class="sd">        world_size: Number of processes participating in the pipeline.</span>
<span class="sd">        device: Device where the tensors are allocated.</span>

<span class="sd">    Returns a dictionary containing the following keys:</span>
<span class="sd">        &quot;inputs&quot;: Shape of the inputs to the module</span>
<span class="sd">        &quot;outputs&quot;: Shape of the outputs of the module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">stage_id_to_shapes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">stage_id</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">stage_ids</span><span class="p">,</span> <span class="n">stage_modules</span><span class="p">):</span>
        <span class="n">input_shape_metadata_tensor</span> <span class="o">=</span> <span class="n">_create_metadata_tensor</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># TODO: Assumes prev_stage == rank - 1 and next_stage == rank + 1</span>
        <span class="n">prev_rank</span> <span class="o">=</span> <span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">world_size</span>
        <span class="n">next_rank</span> <span class="o">=</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">world_size</span>
        <span class="n">shapes</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># first stage doesn&#39;t receive anything and uses a microbatch</span>
        <span class="k">if</span> <span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">microbatch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Microbatch is required for first stage&quot;</span><span class="p">)</span>
            <span class="n">example_fwd_inputs</span> <span class="o">=</span> <span class="n">microbatch</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_fwd_inputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">example_fwd_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">example_fwd_inputs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># other stages must receive shape information</span>
            <span class="c1"># TODO: send/recv should take a group, rather than use the default group</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">input_shape_metadata_tensor</span><span class="p">,</span> <span class="n">prev_rank</span><span class="p">)</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">_extract_metadata_from_tensor</span><span class="p">(</span><span class="n">input_shape_metadata_tensor</span><span class="p">)</span>
            <span class="n">example_fwd_inputs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape_list</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">shape_list</span> <span class="ow">in</span> <span class="n">metadata</span>
            <span class="p">]</span>
        <span class="n">shapes</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">fwd_input</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">fwd_input</span> <span class="ow">in</span> <span class="n">example_fwd_inputs</span><span class="p">]</span>

        <span class="c1"># perform forward</span>
        <span class="c1"># TODO: if forward fails raise a more descriptive error explaining which stage failed</span>
        <span class="n">fwd_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">example_fwd_inputs</span><span class="p">)</span>
        <span class="n">fwd_outputs</span> <span class="o">=</span> <span class="n">_create_empty_tensors</span><span class="p">(</span><span class="n">fwd_outputs</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">shapes</span><span class="p">[</span><span class="s2">&quot;outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">fwd_output</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">fwd_output</span> <span class="ow">in</span> <span class="n">fwd_outputs</span><span class="p">]</span>

        <span class="c1"># send shape dims</span>
        <span class="k">if</span> <span class="n">stage_id</span> <span class="o">!=</span> <span class="n">num_stages</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">output_shape_metadata_tensor</span> <span class="o">=</span> <span class="n">_create_metadata_tensor</span><span class="p">(</span>
                <span class="n">fwd_outputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">output_shape_metadata_tensor</span><span class="p">,</span> <span class="n">next_rank</span><span class="p">)</span>
        <span class="n">stage_id_to_shapes</span><span class="p">[</span><span class="n">stage_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">shapes</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">stage_id_to_shapes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stage_id_to_shapes</span>


<div class="viewcode-block" id="PipelineStage"><a class="viewcode-back" href="../../../../distributed.pipelining.html#torch.distributed.pipelining.stage.PipelineStage">[docs]</a><span class="k">class</span> <span class="nc">PipelineStage</span><span class="p">(</span><span class="n">_PipelineStageBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class representing a pipeline stage in a pipeline parallelism setup.</span>
<span class="sd">    This class is created manually by providing a example input (and optionally output)</span>
<span class="sd">    as opposed to the PipelineStage class that is outputed from pipeline().</span>
<span class="sd">    This class extends the `_PipelineStageBase` class and can similarly be used</span>
<span class="sd">    in `PipelineScheule`.</span>

<span class="sd">    Args:</span>
<span class="sd">        submodule (nn.Module): The PyTorch module wrapped by this stage.</span>
<span class="sd">        stage_index (int): The ID of this stage.</span>
<span class="sd">        num_stages (int): The total number of stages.</span>
<span class="sd">        device (torch.device): The device where this stage is located.</span>
<span class="sd">        input_args (Union[torch.Tensor, Tuple[torch.tensor]], optional): The input arguments for the submodule.</span>
<span class="sd">        output_args (Union[torch.Tensor, Tuple[torch.tensor]], optional): The output arguments for the submodule.</span>
<span class="sd">        group (dist.ProcessGroup, optional): The process group for distributed training. If None, default group.</span>
<span class="sd">        dw_builder: TODO clean up comments</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">submodule</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">stage_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_stages</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="n">input_args</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
        <span class="n">output_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dw_builder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">stage_index</span><span class="p">,</span> <span class="n">num_stages</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">dw_builder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># When we materialize the model partition on cuda, we call reset_parameters() if it is available</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">_create_empty_tensors</span><span class="p">(</span><span class="n">input_args</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;output_args not provided, performing forward using input_args&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
            <span class="c1"># create buffers for the output so that the data is in the correct</span>
            <span class="c1"># shape in order to use in p2p op (send)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">_create_empty_tensors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">_create_empty_tensors</span><span class="p">(</span><span class="n">output_args</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_configure_outputs_meta</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">))</span>

        <span class="c1"># these are the buffers used in backwards send/recv, they are allocated later</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs_grad</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span> <span class="nf">stage_global_rank</span><span class="p">(</span><span class="n">peer_rank</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">peer_rank</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="n">peer_rank</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prev_stage</span> <span class="o">=</span> <span class="n">stage_global_rank</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">group_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_stage</span> <span class="o">=</span> <span class="n">stage_global_rank</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">group_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;finished pipeline stage init, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_first</span><span class="si">=}</span><span class="s2">, &quot;</span>  <span class="c1"># noqa: G004</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_last</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="si">=}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;inputs: </span><span class="si">{</span><span class="p">[</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">inp</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;output: </span><span class="si">{</span><span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_forward_infra</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_microbatches</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Receive info during forward</span>
        <span class="c1"># TODO: create args_recv_info lazily? (same needed for PipelineStage)</span>
        <span class="k">for</span> <span class="n">chunk_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_microbatches</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_requires_grad</span><span class="p">[</span><span class="n">chunk_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first</span><span class="p">:</span>
                <span class="c1"># We assume that we always receive from stage - 1</span>
                <span class="n">recv_infos</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">_RecvInfo</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;recv_for_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="si">}</span><span class="s2">_from_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="n">_make_tensor_from_meta</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span>
                    <span class="p">]</span>
                <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">[</span><span class="n">chunk_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">recv_infos</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">args_recv_info</span><span class="p">[</span><span class="n">chunk_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">_RootArgPlaceholder</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">]</span>
                <span class="p">)</span>

        <span class="c1"># Send info during forward for each activation</span>
        <span class="c1"># only need the rank that is being sent to</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_send_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)):</span>
            <span class="c1"># We assume we always send to stage + 1</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">act_send_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">act_send_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_create_grad_recv_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">act_send_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_RecvInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
        <span class="n">grad_recv_info</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_RecvInfo</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last</span><span class="p">:</span>
            <span class="c1"># Receiving gradients from multiple sources is not supported</span>
            <span class="c1"># hence we only take the first destination</span>
            <span class="n">grad_recv_info</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">_RecvInfo</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;recv_grad_for_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_index</span><span class="si">}</span><span class="s2">_from_</span><span class="si">{</span><span class="n">dst_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">dst_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="n">_make_tensor_from_meta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">dst_list</span> <span class="ow">in</span> <span class="n">act_send_info</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_recv_info</span>

    <span class="k">def</span> <span class="nf">_init_p2p_neighbors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set up p2p communitors between previous and next stages</span>
<span class="sd">        by sending a dummy tensor.</span>

<span class="sd">        If this is used, must be called for all pipeline stages.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">recv_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="n">send_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="c1"># forward</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first</span><span class="p">:</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">,</span> <span class="n">recv_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last</span><span class="p">:</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">,</span> <span class="n">send_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">))</span>

        <span class="c1"># backward</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first</span><span class="p">:</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">,</span> <span class="n">send_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last</span><span class="p">:</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">,</span> <span class="n">recv_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">))</span>

        <span class="k">return</span> <span class="kc">True</span></div>


<span class="k">def</span> <span class="nf">_validate_stage_shapes</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">PipelineStage</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check that the buffer shapes match between stages was expected by performing an all_gather between</span>
<span class="sd">    all stages.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No pipeline stages provided.&quot;</span><span class="p">)</span>

    <span class="n">virtual_pipeline_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="p">)</span>
    <span class="n">all_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">pipeline_stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">num_stages</span> <span class="o">=</span> <span class="n">pipeline_stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_stages</span>

    <span class="c1"># perform all gathers between all stages</span>
    <span class="k">for</span> <span class="n">virtual_id</span><span class="p">,</span> <span class="n">stage</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="p">):</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="n">stage</span><span class="o">.</span><span class="n">group_size</span>
        <span class="n">stage_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">stage</span><span class="o">.</span><span class="n">stage_index</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">stage</span><span class="o">.</span><span class="n">group_rank</span>
        <span class="c1"># check that world_size and num_stages are consistent across all stages</span>
        <span class="k">if</span> <span class="n">stage</span><span class="o">.</span><span class="n">group_size</span> <span class="o">!=</span> <span class="n">world_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Stage id </span><span class="si">{</span><span class="n">stage_id</span><span class="si">}</span><span class="s2"> has world size (</span><span class="si">{</span><span class="n">stage</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">) </span><span class="se">\</span>
<span class="s2">                which does not match world size (</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">) of other stages.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">stage</span><span class="o">.</span><span class="n">num_stages</span> <span class="o">!=</span> <span class="n">num_stages</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Stage id </span><span class="si">{</span><span class="n">stage_id</span><span class="si">}</span><span class="s2"> has num stages (</span><span class="si">{</span><span class="n">stage</span><span class="o">.</span><span class="n">num_stages</span><span class="si">}</span><span class="s2">) </span><span class="se">\</span>
<span class="s2">                which does not match num stages (</span><span class="si">{</span><span class="n">num_stages</span><span class="si">}</span><span class="s2">) of other stages.&quot;</span>
            <span class="p">)</span>

        <span class="n">pg_rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="n">stage</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="n">pg_rank</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> is not equal to process group rank </span><span class="si">{</span><span class="n">pg_rank</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">num_stages</span> <span class="o">:=</span> <span class="n">stage</span><span class="o">.</span><span class="n">num_stages</span><span class="p">)</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of stages (</span><span class="si">{</span><span class="n">num_stages</span><span class="si">}</span><span class="s2">) must be a multiple of the world_size (</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>

        <span class="c1"># all gather each ranks inputs</span>
        <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">_create_metadata_tensor</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">stage</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">stage</span><span class="o">.</span><span class="n">group_size</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">expected_inputs</span> <span class="o">=</span> <span class="n">stage</span><span class="o">.</span><span class="n">inputs</span>
        <span class="n">stage_input</span> <span class="o">=</span> <span class="n">_create_metadata_tensor</span><span class="p">(</span><span class="n">expected_inputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">stage</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">stage_input</span><span class="p">)</span>
        <span class="n">stage_input_shapes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">_extract_metadata_from_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensor_list</span>
        <span class="p">]</span>

        <span class="c1"># all gather each ranks outputs</span>
        <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">_create_metadata_tensor</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">stage</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">stage</span><span class="o">.</span><span class="n">group_size</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">expected_outputs</span> <span class="o">=</span> <span class="n">stage</span><span class="o">.</span><span class="n">outputs</span>
        <span class="n">stage_output</span> <span class="o">=</span> <span class="n">_create_metadata_tensor</span><span class="p">(</span><span class="n">expected_outputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">stage</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">stage_output</span><span class="p">)</span>
        <span class="n">stage_output_shapes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">_extract_metadata_from_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensor_list</span>
        <span class="p">]</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Rank: </span><span class="si">{</span><span class="n">pg_rank</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G004</span>
            <span class="sa">f</span><span class="s2">&quot;Stage id: </span><span class="si">{</span><span class="n">stage_id</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Stage num stages: </span><span class="si">{</span><span class="n">stage</span><span class="o">.</span><span class="n">num_stages</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Stage rank: </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Stage world size: </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Stage </span><span class="si">{</span><span class="n">virtual_id</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">world_size</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="p">(</span><span class="n">virtual_id</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">world_size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> input shapes: </span><span class="si">{</span><span class="n">stage_input_shapes</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G003</span>
            <span class="sa">f</span><span class="s2">&quot;Stage </span><span class="si">{</span><span class="n">virtual_id</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">world_size</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="p">(</span><span class="n">virtual_id</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">world_size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> output shapes: </span><span class="si">{</span><span class="n">stage_output_shapes</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G003</span>
        <span class="p">)</span>

        <span class="n">all_inputs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">stage_input_shapes</span><span class="p">)</span>
        <span class="n">all_outputs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">stage_output_shapes</span><span class="p">)</span>

        <span class="c1"># log only rank 0&#39;s view, they will all be equivalent</span>
        <span class="k">if</span> <span class="n">pg_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;all stage inputs: </span><span class="si">%s</span><span class="s2"> </span><span class="se">\n</span><span class="s2"> all stage outputs: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">all_inputs</span><span class="p">,</span> <span class="n">all_outputs</span>
            <span class="p">)</span>

    <span class="c1"># Check if the output for stage 0 matches the input at stage 1, and so forth</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">virtual_pipeline_size</span> <span class="o">*</span> <span class="n">world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">out</span> <span class="o">:=</span> <span class="n">all_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">!=</span> <span class="p">(</span><span class="n">inp</span> <span class="o">:=</span> <span class="n">all_inputs</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Stage_id </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> output shape </span><span class="si">{</span><span class="n">out</span><span class="si">}</span><span class="s2"> at does not match stage_id </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> input shape </span><span class="si">{</span><span class="n">inp</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>