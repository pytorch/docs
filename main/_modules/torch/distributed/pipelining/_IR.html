


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.pipelining._IR &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/pipelining/_IR.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.6.0a0+gite8b1409 ) &#x25BC</a>
    </div>
     <div class="searchbox">
        <script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
        <div class="gcse-search"></div>
     </div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/distributed/pipelining/_IR.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/get_start_xpu.html">Getting Started on Intel GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_environment_variables.html">Torch Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.pipelining._IR</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.pipelining._IR</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">inspect</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Signature</span><span class="p">,</span> <span class="n">signature</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">MethodType</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx</span> <span class="k">as</span> <span class="nn">fx</span>
<span class="kn">from</span> <span class="nn">torch.distributed</span> <span class="kn">import</span> <span class="n">ProcessGroup</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">ExportedProgram</span>
<span class="kn">from</span> <span class="nn">torch.export.unflatten</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_assign_attr</span><span class="p">,</span>
    <span class="n">_AttrKind</span><span class="p">,</span>
    <span class="n">_sink_params</span><span class="p">,</span>
    <span class="n">InterpreterModule</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.fx.node</span> <span class="kn">import</span> <span class="n">map_aggregate</span>
<span class="kn">from</span> <span class="nn">torch.fx.passes.split_module</span> <span class="kn">import</span> <span class="n">split_module</span>

<span class="kn">from</span> <span class="nn">._backward</span> <span class="kn">import</span> <span class="n">_null_coalesce_accumulate</span><span class="p">,</span> <span class="n">stage_backward</span>
<span class="kn">from</span> <span class="nn">._unflatten</span> <span class="kn">import</span> <span class="n">_outline_submodules</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">PipeInfo</span>
<span class="kn">from</span> <span class="nn">.stage</span> <span class="kn">import</span> <span class="n">_PipelineStage</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># TODO:</span>
<span class="c1"># 1. investigate gradient sync for shared parameters. how does DDP do it?</span>
<span class="c1"># 2. Add parameter movement to split_module</span>


<span class="k">def</span> <span class="nf">_find_loss_from_output_and_spec</span><span class="p">(</span><span class="n">output_val</span><span class="p">,</span> <span class="n">spec_val</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">spec_val</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">spec_val</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_val</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Loss spec must specify a dynamic value but got </span><span class="si">{</span><span class="n">output_val</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">output_val</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">spec_val</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_val</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Output value </span><span class="si">{</span><span class="n">output_val</span><span class="si">}</span><span class="s2"> must match type of loss specification &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">spec_val</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_val</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">spec_val</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Output value </span><span class="si">{</span><span class="n">output_val</span><span class="si">}</span><span class="s2"> must match length of loss specification &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">spec_val</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">spec</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_val</span><span class="p">,</span> <span class="n">spec_val</span><span class="p">):</span>
            <span class="n">loss_val</span> <span class="o">=</span> <span class="n">_find_loss_from_output_and_spec</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">spec</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">loss_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">loss_val</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Did not find loss value in specification </span><span class="si">{</span><span class="n">spec_val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">spec_val</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_val</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Output value </span><span class="si">{</span><span class="n">output_val</span><span class="si">}</span><span class="s2"> must match type of loss specification &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">spec_val</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">set</span><span class="p">(</span><span class="n">output_val</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">!=</span> <span class="nb">set</span><span class="p">(</span><span class="n">spec_val</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Output value </span><span class="si">{</span><span class="n">output_val</span><span class="si">}</span><span class="s2"> must match keys of loss specification &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">spec_val</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">spec_val</span><span class="p">:</span>
            <span class="n">loss_val</span> <span class="o">=</span> <span class="n">_find_loss_from_output_and_spec</span><span class="p">(</span><span class="n">output_val</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">spec_val</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">loss_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">loss_val</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Did not find loss value in specification </span><span class="si">{</span><span class="n">spec_val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">spec_val</span><span class="p">)</span><span class="si">}</span><span class="s2"> in loss specification&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_find_loss_output</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">,</span> <span class="n">output_loss_value_spec</span><span class="p">):</span>
    <span class="n">output_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">g</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_nodes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">output_node</span> <span class="o">=</span> <span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">output_val</span> <span class="o">=</span> <span class="n">output_node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">generated_spec</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">TrivialLossWrapper</span><span class="p">):</span>
        <span class="c1"># TrivialLossWrapper is pre-defined by PiPPy.</span>
        <span class="c1"># It has loss as the only output so we can safely assume the first output arg is the loss.</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">loss_node</span> <span class="o">=</span> <span class="n">output_val</span>
        <span class="n">generated_spec</span> <span class="o">=</span> <span class="n">TrivialLossWrapper</span><span class="o">.</span><span class="n">loss_spec</span>
    <span class="k">elif</span> <span class="n">output_loss_value_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Use default spec, i.e. search for &quot;loss&quot; in output values</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_val</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;loss&quot;</span> <span class="ow">in</span> <span class="n">output_val</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">loss_node</span> <span class="o">=</span> <span class="n">output_val</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
            <span class="n">generated_spec</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;loss&quot;</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">output_val</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss_node</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">generated_spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_node</span> <span class="o">=</span> <span class="n">_find_loss_from_output_and_spec</span><span class="p">(</span><span class="n">output_val</span><span class="p">,</span> <span class="n">output_loss_value_spec</span><span class="p">)</span>
        <span class="n">generated_spec</span> <span class="o">=</span> <span class="n">output_loss_value_spec</span>

    <span class="k">return</span> <span class="n">loss_node</span><span class="p">,</span> <span class="n">output_node</span><span class="p">,</span> <span class="n">generated_spec</span>


<span class="k">def</span> <span class="nf">_insert_stage_symbolic_backward</span><span class="p">(</span>
    <span class="n">g</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">,</span>
    <span class="n">loss_node</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span>
    <span class="n">output_node</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Collect metadata about tuple output values. TODO: move this to split_module or FX IR</span>
    <span class="n">tuples</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span><span class="p">:</span>
            <span class="c1"># In the forward pass, only emit placeholder, module calls, and</span>
            <span class="c1"># getitem calls. If we have a target other than getitem in this</span>
            <span class="c1"># (forward-only) code, there is a bug.</span>
            <span class="k">assert</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;Found non-getitem call in forward pass. &quot;</span>
                <span class="s2">&quot;Please report a bug to PiPPy&quot;</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="p">),</span> <span class="s2">&quot;Found malformed getitem call. Please report a bug to PiPPy&quot;</span>
            <span class="n">indexed_value</span><span class="p">,</span> <span class="n">node_idx</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>

            <span class="c1"># indexed_value is a collection that we are indexing into. It could</span>
            <span class="c1"># exist in the tuples map if we&#39;ve processed another `getitem`</span>
            <span class="c1"># already.</span>
            <span class="n">existing_list_size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">tuples</span><span class="p">[</span><span class="n">indexed_value</span><span class="p">])</span> <span class="k">if</span> <span class="n">indexed_value</span> <span class="ow">in</span> <span class="n">tuples</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">new_list_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">node_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">existing_list_size</span><span class="p">)</span>

            <span class="n">reconstructed_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">new_list_size</span><span class="p">)]</span>

            <span class="c1"># Copy over existing elements if present</span>
            <span class="k">if</span> <span class="n">indexed_value</span> <span class="ow">in</span> <span class="n">tuples</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tuples</span><span class="p">[</span><span class="n">indexed_value</span><span class="p">]):</span>
                    <span class="n">reconstructed_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

            <span class="c1"># Populate value represented by this node</span>
            <span class="n">reconstructed_list</span><span class="p">[</span><span class="n">node_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span>

            <span class="n">tuples</span><span class="p">[</span><span class="n">indexed_value</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">reconstructed_list</span><span class="p">)</span>

    <span class="c1"># Keep track of nodes that dominate the loss node.</span>
    <span class="c1"># We will only emit backward operations for nodes that can contribute</span>
    <span class="c1"># to the specified loss value.</span>
    <span class="n">live_nodes</span> <span class="o">=</span> <span class="p">{</span><span class="n">loss_node</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
    <span class="n">val_to_grad</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span><span class="n">loss_node</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">assign_or_accumulate_grad</span><span class="p">(</span><span class="n">forward_node</span><span class="p">,</span> <span class="n">grad_value</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">forward_node</span> <span class="ow">in</span> <span class="n">val_to_grad</span> <span class="ow">and</span> <span class="n">forward_node</span><span class="o">.</span><span class="n">op</span> <span class="o">!=</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
            <span class="n">grad_value</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
                <span class="n">_null_coalesce_accumulate</span><span class="p">,</span>
                <span class="p">(</span><span class="n">val_to_grad</span><span class="p">[</span><span class="n">forward_node</span><span class="p">],</span> <span class="n">grad_value</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="n">val_to_grad</span><span class="p">[</span><span class="n">forward_node</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_value</span>

    <span class="k">with</span> <span class="n">g</span><span class="o">.</span><span class="n">inserting_before</span><span class="p">(</span><span class="n">output_node</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">live_nodes</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">def</span> <span class="nf">add_to_live_nodes</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
                <span class="n">live_nodes</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">map_arg</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">add_to_live_nodes</span><span class="p">)</span>
            <span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">map_arg</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">add_to_live_nodes</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
                <span class="n">output_grads</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">],</span> <span class="o">...</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">]]</span>
                <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">tuples</span><span class="p">:</span>
                    <span class="n">stage_output</span> <span class="o">=</span> <span class="n">tuples</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
                    <span class="n">output_grads</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">val_to_grad</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">tuples</span><span class="p">[</span><span class="n">node</span><span class="p">])</span>
                    <span class="n">outputs_with_grads_idxs</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tuples</span><span class="p">[</span><span class="n">node</span><span class="p">])</span> <span class="k">if</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">live_nodes</span>
                    <span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">stage_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">node</span><span class="p">,)</span>
                    <span class="n">output_grads</span> <span class="o">=</span> <span class="n">val_to_grad</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
                    <span class="n">outputs_with_grads_idxs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">output_grads</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">output_grads</span><span class="p">,)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_grads</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                    <span class="k">else</span> <span class="n">output_grads</span>
                <span class="p">)</span>

                <span class="n">grad_call</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
                    <span class="n">stage_backward</span><span class="p">,</span>
                    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span>
                        <span class="s2">&quot;stage_output&quot;</span><span class="p">:</span> <span class="n">stage_output</span><span class="p">,</span>
                        <span class="s2">&quot;output_grads&quot;</span><span class="p">:</span> <span class="n">output_grads</span><span class="p">,</span>
                        <span class="s2">&quot;input_values&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">all_input_nodes</span><span class="p">),</span>
                        <span class="s2">&quot;outputs_with_grads_idxs&quot;</span><span class="p">:</span> <span class="n">outputs_with_grads_idxs</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">)</span>
                <span class="c1"># Insert backward stage debug info</span>
                <span class="n">kwargs_copy</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">grad_call</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="n">grad_call</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs_copy</span>

                <span class="n">grad_call_proxy</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">(</span><span class="n">grad_call</span><span class="p">)</span>
                <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_call_proxy</span><span class="o">.</span><span class="n">node</span>

                <span class="n">input_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">all_input_nodes</span><span class="p">)</span>
                <span class="n">grads_proxy</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">input_node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_nodes</span><span class="p">):</span>
                    <span class="n">assign_or_accumulate_grad</span><span class="p">(</span><span class="n">input_node</span><span class="p">,</span> <span class="n">grads_proxy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">node</span><span class="p">)</span>  <span class="c1"># type: ignore[index]</span>

    <span class="k">return</span> <span class="n">g</span>


<span class="k">class</span> <span class="nc">PipeSequential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_sequential</span><span class="p">(</span><span class="n">sequential_instance</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">PipeSequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">sequential_instance</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pipe_split</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">input</span>


<span class="k">class</span> <span class="nc">LossWrapper</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LossWrapper is a convenient abstract class that allows you to wrap up both</span>
<span class="sd">    your model as well as its loss function and specify the connectivity between</span>
<span class="sd">    the inputs, model, loss function, and output value. Example::</span>

<span class="sd">        class MyModelWrapper(LossWrapper):</span>
<span class="sd">            def forward(self, x, targets):</span>
<span class="sd">                model_out = self.module(x)</span>
<span class="sd">                loss_value = self.loss_fn(model_out, targets)</span>
<span class="sd">                return loss_value</span>

<span class="sd">    The above example defines a connectivity where we expect the forward/loss/backward</span>
<span class="sd">    training procedure to take two arguments (x and targets), pass x into the module</span>
<span class="sd">    to get the output of the feedforward computation, pass the model output and the</span>
<span class="sd">    targets value into the loss function, and get and return the loss value, which will</span>
<span class="sd">    be backpropagated by PiPPy. The above class would then be instantiated like::</span>

<span class="sd">        model = ... # instantiate the model</span>
<span class="sd">        loss_fn = torch.nn.MSELoss() # for the sake of demonstration</span>

<span class="sd">        wrapper = MyModelWrapper(model, loss_fn)</span>
<span class="sd">        pipe = Pipe.from_tracing(wrapper, ...)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;This instance of LossWrapper does not have an overridden&quot;</span>
            <span class="s2">&quot;forward(). Please implement forward() to specify the arguments, &quot;</span>
            <span class="s2">&quot;connection between the module and loss, and loss output &quot;</span>
            <span class="s2">&quot;value.&quot;</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">TrivialLossWrapper</span><span class="p">(</span><span class="n">LossWrapper</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">model_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">model_out</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="n">loss_spec</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># Pipe model representation</span>
<span class="c1">#</span>
<span class="c1"># Pipe can be thought of as an `nn.Sequential++`. That is to say: it specifies</span>
<span class="c1"># a single topological ordering of pipeline &quot;stages&quot; that, when run in series,</span>
<span class="c1"># constitutes all of the operations of the program. However, unlike `nn.Sequential`,</span>
<span class="c1"># Pipe allows non-local usages of values, so long as those uses still respect</span>
<span class="c1"># topological ordering. In particular:</span>
<span class="c1">#</span>
<span class="c1"># 1. Non-local activations. This type of usage can appear in, for example, skip</span>
<span class="c1">#    connections. These values will be directly transmitted from the &quot;def&quot; stage</span>
<span class="c1">#    to all stages that use them skipping intermediate stages. During autograd,</span>
<span class="c1">#    gradients will be propagated back through this skip connection reverse</span>
<span class="c1">#    to how activations propagated in the forward pass.</span>
<span class="c1"># 2. Non-local parameter/module invocations. This occurs when a parameter is used</span>
<span class="c1">#    in a stage downstream of where it is resident. These values can be carried</span>
<span class="c1">#    forward similarly to (1), but in addition one might want to replicate the</span>
<span class="c1">#    value on multiple stages. Gradients for these shared parameters will be</span>
<span class="c1">#    accumulated separately on each stage, but there will be an additional</span>
<span class="c1">#    gradient accumulation before the optimizer step.</span>


<span class="c1"># Register `_pipe_split()` as an ATen operator. This is required for Export to</span>
<span class="c1"># preserve this marker in the graph.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="s2">&quot;pippy::_pipe_split&quot;</span><span class="p">,</span> <span class="s2">&quot;() -&gt; ()&quot;</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="s2">&quot;pippy::_pipe_split&quot;</span><span class="p">,</span> <span class="s2">&quot;BackendSelect&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_pipe_split</span><span class="p">():</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">register_fake</span><span class="p">(</span><span class="s2">&quot;pippy::_pipe_split&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore[no-redef]</span>
<span class="k">def</span> <span class="nf">_pipe_split</span><span class="p">():</span>  <span class="c1"># noqa: F811</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="c1"># Add an alias for convenience</span>
<span class="n">aten_pipe_split_alias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">pippy</span><span class="o">.</span><span class="n">_pipe_split</span><span class="o">.</span><span class="n">default</span>

<span class="c1"># Ask Export to preserve the `_pipe_split` op.</span>
<span class="c1"># See examples in pytorch/torch/fx/node.py</span>
<span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">_side_effectful_functions</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">aten_pipe_split_alias</span><span class="p">)</span>


<span class="c1"># User facing API</span>
<div class="viewcode-block" id="pipe_split"><a class="viewcode-back" href="../../../../distributed.pipelining.html#torch.distributed.pipelining.pipe_split">[docs]</a><span class="k">def</span> <span class="nf">pipe_split</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    pipe_split is a special operator that is used to mark the boundary between</span>
<span class="sd">    stages in a module. It is used to split the module into stages. It is a</span>
<span class="sd">    no-op if your annotated module is run eagerly.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; def forward(self, x):</span>
<span class="sd">        &gt;&gt;&gt;     x = torch.mm(x, self.mm_param)</span>
<span class="sd">        &gt;&gt;&gt;     x = torch.relu(x)</span>
<span class="sd">        &gt;&gt;&gt;     pipe_split()</span>
<span class="sd">        &gt;&gt;&gt;     x = self.lin(x)</span>
<span class="sd">        &gt;&gt;&gt;     return x</span>

<span class="sd">    The above example will be split into two stages.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">pippy</span><span class="o">.</span><span class="n">_pipe_split</span><span class="p">()</span></div>


<span class="k">class</span> <span class="nc">MultiUseParameterConfig</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">TRANSMIT</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">REPLICATE</span> <span class="o">=</span> <span class="mi">2</span>


<span class="n">MultiUseParamSpec</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiUseParameterConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">MultiUseParameterConfig</span><span class="p">]]</span>


<span class="k">class</span> <span class="nc">DetachExecutor</span><span class="p">(</span><span class="n">fx</span><span class="o">.</span><span class="n">Interpreter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Special interpreter to run the split_gm in testing that detaches all inputs to</span>
<span class="sd">    a module invocation. This is needed so that the values at the boundary are</span>
<span class="sd">    leaf modules in autograd execution.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">garbage_collect_values</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">garbage_collect_values</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">garbage_collect_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_remap</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">initial_env</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># type: ignore[override]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_remap</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">initial_env</span><span class="o">=</span><span class="n">initial_env</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">detach_tensors</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">a</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_remap</span><span class="p">:</span>
                    <span class="n">new_val</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">value_remap</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_val</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_remap</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">a</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        def dont_traverse_size(a):</span>
<span class="sd">            return type(a) != torch.Size</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">args</span> <span class="o">=</span> <span class="n">map_aggregate</span><span class="p">(</span>
            <span class="n">args</span><span class="p">,</span>
            <span class="n">detach_tensors</span><span class="p">,</span>  <span class="c1"># dont_traverse_size</span>
        <span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">map_aggregate</span><span class="p">(</span>
            <span class="n">kwargs</span><span class="p">,</span>
            <span class="n">detach_tensors</span><span class="p">,</span>  <span class="c1"># dont_traverse_size</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_module</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># HACK to reroute saved input tensors to point to the detach()ed version</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="n">stage_backward</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;input_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_remap</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;input_values&quot;</span><span class="p">]</span>
            <span class="p">]</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_NodeReference</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">class</span> <span class="nc">_LinearNodeList</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">serialize_node_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_list</span><span class="p">:</span>
            <span class="n">node_args</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">map_arg</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">_NodeReference</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type,return-value]</span>
            <span class="n">node_kwargs</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">map_arg</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">_NodeReference</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type,return-value]</span>
            <span class="n">serialize_node</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span>
                <span class="n">graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="n">name</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">op</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span>
                <span class="n">target</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="n">node_args</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">node_kwargs</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="n">return_type</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">type</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">serialize_node</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">serialize_node_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">serialize_node</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

        <span class="n">ref_str_to_node</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">def</span> <span class="nf">ref_to_node</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">_NodeReference</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">ref_str_to_node</span><span class="p">[</span><span class="n">arg</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">arg</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">serialize_node_list</span><span class="p">:</span>
            <span class="n">node_args</span> <span class="o">=</span> <span class="n">map_aggregate</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">ref_to_node</span><span class="p">)</span>
            <span class="n">node_kwargs</span> <span class="o">=</span> <span class="n">map_aggregate</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">ref_to_node</span><span class="p">)</span>
            <span class="n">deser_node</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span>
                <span class="n">op</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span>
                <span class="n">target</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="n">node_args</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="n">node_kwargs</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="n">name</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">type_expr</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">type</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ref_str_to_node</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">deser_node</span>

        <span class="k">return</span> <span class="n">graph</span>


<span class="k">def</span> <span class="nf">_direct_serialization_deserialize</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">nodes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom `__reduce__` method for serialization.</span>
<span class="sd">    DO AS I SAY -- NOT AS I DO. This violates the principle that</span>
<span class="sd">    GraphModules serialize via code export &amp; re-tracing. We allow</span>
<span class="sd">    for this here because **PIPE STAGES SHOULD NOT BE PERSISTED</span>
<span class="sd">    TO DISK -- THIS IS ONLY FOR TRANSMISSION VIA RPC**. Persisting</span>
<span class="sd">    these instances to disk will expose internal implementation</span>
<span class="sd">    details of `fx.Graph` and related data structures and is</span>
<span class="sd">    NOT advised.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">class</span> <span class="nc">DummyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">body</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">body</span><span class="p">)</span>

    <span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyModule</span><span class="p">(</span><span class="n">body</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="n">nodes</span><span class="o">.</span><span class="n">to_graph</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">_direct_serialization_reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">serialization_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
    <span class="n">serialization_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_graph&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">_direct_serialization_deserialize</span><span class="p">,</span>
        <span class="p">(</span><span class="n">serialization_dict</span><span class="p">,</span> <span class="n">_LinearNodeList</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">)),</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_modify_graph_op_device</span><span class="p">(</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">new_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Modify the device argument of all &quot;call_function&quot; nodes in the graph.  This</span>
<span class="sd">    is useful for moving the graph to a different device. In particular for</span>
<span class="sd">    generator ops, like torch.ones.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">modified</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;device&quot;</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">kwargs</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">new_device</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Changing device of Node </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;device&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">new_device</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G004</span>
                <span class="p">)</span>
                <span class="n">node</span><span class="o">.</span><span class="n">update_kwarg</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">,</span> <span class="n">new_device</span><span class="p">)</span>
                <span class="n">modified</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
            <span class="c1"># Recursively modify &quot;device&quot; in submodules</span>
            <span class="n">submod</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="n">_modify_graph_op_device</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">new_device</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">InterpreterModule</span><span class="p">):</span>
                <span class="c1"># If unflattening has been performed, we need to access its graph module by `.graph_module`</span>
                <span class="n">_modify_graph_op_device</span><span class="p">(</span><span class="n">submod</span><span class="o">.</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">new_device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Skipping device modification for submodule </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2"> because it is a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">submod</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G004</span>
                <span class="p">)</span>

    <span class="k">if</span> <span class="n">modified</span><span class="p">:</span>
        <span class="n">gm</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>


<div class="viewcode-block" id="Pipe"><a class="viewcode-back" href="../../../../distributed.pipelining.html#torch.distributed.pipelining.Pipe">[docs]</a><span class="k">class</span> <span class="nc">Pipe</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">split_gm</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
        <span class="n">num_stages</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">has_loss_and_backward</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">loss_spec</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># TODO: is there a way not to hard wire init?</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span> <span class="o">=</span> <span class="n">split_gm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">executor</span><span class="p">:</span> <span class="n">DetachExecutor</span> <span class="o">=</span> <span class="n">DetachExecutor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">num_stages</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_loss_and_backward</span> <span class="o">=</span> <span class="n">has_loss_and_backward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_spec</span> <span class="o">=</span> <span class="n">loss_spec</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">split_gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;call_module&quot;</span><span class="p">,</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">}</span>
                <span class="ow">or</span> <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="s2">&quot;call_method&quot;</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">stage_backward</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
                <span class="o">==</span> <span class="p">(</span><span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">_null_coalesce_accumulate</span><span class="p">)</span>
            <span class="p">),</span> <span class="n">node</span>

        <span class="c1"># Detect replicated parameters so we know that we have to do an additional allreduce</span>
        <span class="c1"># before applying the optimizer</span>
        <span class="c1">#</span>
        <span class="c1"># Note that this also handles the case where there were multiple calls to a single</span>
        <span class="c1"># module from different stages, regardless of whether that module invocation</span>
        <span class="c1"># was handled by the logic above.</span>

        <span class="c1"># Map parameter value to a dictionary that maps the user pipeline module</span>
        <span class="c1"># to the local qualname within that module</span>
        <span class="n">params_to_users</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">m_qualname</span><span class="p">,</span> <span class="n">mod</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">p_qualname</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="n">params_to_users</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="p">{})</span>
                <span class="n">params_to_users</span><span class="p">[</span><span class="n">param</span><span class="p">][</span><span class="n">m_qualname</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_qualname</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">replicated_params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">use_mapping</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">use_mapping</span> <span class="ow">in</span> <span class="n">params_to_users</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">use_mapping</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="p">]</span>

        <span class="c1"># We must break the aliasing relationship between the replicated parameters for correct</span>
        <span class="c1"># numerics in reference runs. If we do not do this, the autograd tape in separate stages</span>
        <span class="c1"># will have a reference to the same tensor value and will erroneously apply gradient</span>
        <span class="c1"># updates multiple times. Therefore, for each replicated parameter set, we deepcopy the</span>
        <span class="c1"># values so that we have separate instances.</span>
        <span class="k">for</span> <span class="n">param_mapping</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">replicated_params</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">submod_name</span><span class="p">,</span> <span class="n">param_qualname</span> <span class="ow">in</span> <span class="n">param_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">submod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="p">,</span> <span class="n">submod_name</span><span class="p">)</span>
                <span class="n">atoms</span> <span class="o">=</span> <span class="n">param_qualname</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">atom</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">submod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">atom</span><span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>

        <span class="k">def</span> <span class="nf">throw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;To run pipeline locally, invoke the Pipe object directly, not `split_gm`&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">throw</span>

        <span class="c1"># Make submodules use custom direct-serialized GraphModule</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;submod_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">submod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="n">submod</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">__reduce__</span> <span class="o">=</span> <span class="n">_direct_serialization_reduce</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">executor_args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="n">Parameter</span><span class="p">(</span>
                                <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                <span class="n">Parameter</span><span class="o">.</span><span class="n">POSITIONAL_OR_KEYWORD</span><span class="p">,</span>
                                <span class="n">default</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">parameter_kind</span> <span class="o">=</span> <span class="n">Parameter</span><span class="o">.</span><span class="n">POSITIONAL_OR_KEYWORD</span>
                        <span class="n">param_name</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span>
                        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;**&quot;</span><span class="p">):</span>
                            <span class="n">parameter_kind</span> <span class="o">=</span> <span class="n">Parameter</span><span class="o">.</span><span class="n">VAR_KEYWORD</span>  <span class="c1"># type: ignore[assignment]</span>
                            <span class="n">param_name</span> <span class="o">=</span> <span class="n">param_name</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
                        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">):</span>
                            <span class="n">parameter_kind</span> <span class="o">=</span> <span class="n">Parameter</span><span class="o">.</span><span class="n">VAR_POSITIONAL</span>  <span class="c1"># type: ignore[assignment]</span>
                            <span class="n">param_name</span> <span class="o">=</span> <span class="n">param_name</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
                        <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Parameter</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">parameter_kind</span><span class="p">))</span>
            <span class="n">signature</span> <span class="o">=</span> <span class="n">Signature</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
            <span class="n">ba</span> <span class="o">=</span> <span class="n">signature</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">ba</span><span class="o">.</span><span class="n">apply_defaults</span><span class="p">()</span>
            <span class="n">executor_args</span> <span class="o">=</span> <span class="n">ba</span><span class="o">.</span><span class="n">arguments</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">executor</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">executor_args</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">get_stage_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a stage module corresponding to `stage_idx` of the `pipe`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">stage_idx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">stage_idx</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid stage index </span><span class="si">{</span><span class="n">stage_idx</span><span class="si">}</span><span class="s2">!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;submod_</span><span class="si">{</span><span class="n">stage_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_number_and_count_forward_stages</span><span class="p">(</span><span class="n">gm</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
        <span class="n">num_stages</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">found_idxs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;submod_&quot;</span><span class="p">):</span>
                <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;stage_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;submod_&quot;</span><span class="p">)</span> <span class="p">:])</span>
                <span class="n">found_idxs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;stage_idx&quot;</span><span class="p">])</span>
                <span class="n">num_stages</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># this assert will fail if a split point is inserted before the first layer, which creates empty first submodule</span>
        <span class="c1"># Update: the following assert may fail against some torch versions &gt;=</span>
        <span class="c1"># 2.2.0, as:</span>
        <span class="c1"># submod_0, submod_1, submod_2, ...</span>
        <span class="c1"># may be named as</span>
        <span class="c1"># submod_0, submod_2, submod_4, ...</span>
        <span class="c1"># TODO: investigate</span>
        <span class="c1"># assert all(i in found_idxs for i in range(num_stages))</span>

        <span class="k">return</span> <span class="n">num_stages</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_from_traced</span><span class="p">(</span>
        <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">exported_program</span><span class="p">:</span> <span class="n">ExportedProgram</span><span class="p">,</span>
        <span class="n">multi_use_param_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiUseParamSpec</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_loss_value_spec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">split_policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Additionally, the ``output_loss_value_spec`` value can be specified to disambiguate</span>
<span class="sd">        which value in the output of `forward` is the loss value on which PiPPy should apply</span>
<span class="sd">        backpropagation. For example, if your ``forward`` returns a tuple ``(loss, model_out)``,</span>
<span class="sd">        you can specify ``output_loss_value_spec=(True, False)``. Or, if your ``forward`` returns</span>
<span class="sd">        a dict ``{&#39;loss&#39;: loss_value, &#39;model_out&#39;: model_out}``, you can specify</span>
<span class="sd">        ``output_loss_value_spec={&#39;loss&#39;: True, &#39;model_out&#39;: False}``</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">traced</span> <span class="o">=</span> <span class="n">exported_program</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">split_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Auto-splitting model&quot;</span><span class="p">)</span>
            <span class="n">traced</span> <span class="o">=</span> <span class="n">split_policy</span><span class="p">(</span><span class="n">traced</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">traced</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="c1"># Deduplicate `get_attr` nodes that refer to the same parameter . Downstream code for moving</span>
        <span class="c1"># parameters relies on the invariant that parameter accesses happen once. This is not necessarily</span>
        <span class="c1"># the case (especially with custom tracers), so fix that up here.</span>
        <span class="n">get_attr_nodes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">traced</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;get_attr&quot;</span><span class="p">:</span>
                <span class="n">get_attr_nodes</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">node</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">get_attr_nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">]</span> <span class="o">!=</span> <span class="n">node</span><span class="p">:</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">replace_all_uses_with</span><span class="p">(</span><span class="n">get_attr_nodes</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">])</span>
                    <span class="n">traced</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="c1"># avoid looking at next node by keeping track of previous pipe_split</span>
        <span class="n">prev_pipe_split_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">pipe_split_nodes_to_erase</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">traced</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="n">pipe_split</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">prev_pipe_split_idx</span> <span class="o">==</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">pipe_split_nodes_to_erase</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="n">prev_pipe_split_idx</span> <span class="o">=</span> <span class="n">i</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">pipe_split_nodes_to_erase</span><span class="p">:</span>
            <span class="n">traced</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="n">traced</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

        <span class="n">part_idx</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">def</span> <span class="nf">split_callback</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">):</span>
            <span class="k">nonlocal</span> <span class="n">part_idx</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
                <span class="s2">&quot;call_function&quot;</span><span class="p">,</span>
                <span class="n">aten_pipe_split_alias</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found pipe_split </span><span class="si">{</span><span class="n">part_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># noqa: G004</span>
                <span class="n">part_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="n">part_idx</span>

        <span class="c1"># TODO: what does split do with module invocations? does it move the modules</span>
        <span class="c1"># into the submodules?</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">split_module</span><span class="p">(</span><span class="n">traced</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">split_callback</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="c1"># a (custom) tracer can produce dead code like orphan get_attr nodes</span>
        <span class="n">split</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>

        <span class="c1"># peephole to remove pipe_split</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">split</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">submodule</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
                        <span class="s2">&quot;call_function&quot;</span><span class="p">,</span>
                        <span class="n">aten_pipe_split_alias</span><span class="p">,</span>
                    <span class="p">):</span>
                        <span class="n">submodule</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="n">submodule</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">split</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="n">new_submod</span> <span class="o">=</span> <span class="n">_outline_submodules</span><span class="p">(</span><span class="n">submodule</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
                <span class="c1"># Replace old submod</span>
                <span class="n">split</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">new_submod</span><span class="p">)</span>

        <span class="c1"># TODO: backport this into split_module</span>
        <span class="k">def</span> <span class="nf">delete_user_reference</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">user</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Delete reference of `node` from `user`&#39;s arg list.</span>
<span class="sd">            Args:</span>
<span class="sd">                - node: a `get_attr` node at root.</span>
<span class="sd">                - user: a submodule node that uses `node`.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">use_idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="k">if</span> <span class="n">arg</span> <span class="o">==</span> <span class="n">node</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">use_idxs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="n">args_copy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
            <span class="n">args_copy</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">use_idxs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">user</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args_copy</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Deleted </span><span class="si">{</span><span class="n">node</span><span class="si">}</span><span class="s2"> from user </span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">, arg index = </span><span class="si">{</span><span class="n">use_idxs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G004</span>
            <span class="p">)</span>

        <span class="c1"># A list of param referrals for deferred deletion.</span>
        <span class="c1"># To be accumulated in `move_param_to_callee`.</span>
        <span class="n">to_delete</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span> <span class="nf">_recursive_getattr_with_parent</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">fqn</span><span class="p">):</span>
            <span class="c1"># Returns getattr call given a nested FQN, and the last parent</span>
            <span class="n">atoms</span> <span class="o">=</span> <span class="n">fqn</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">atom</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">atom</span><span class="p">):</span>
                    <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
                <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">atom</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                <span class="k">return</span> <span class="n">mod</span><span class="p">,</span> <span class="kc">None</span>
            <span class="n">attr</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">mod</span><span class="p">,</span> <span class="n">attr</span>

        <span class="k">def</span> <span class="nf">move_param_to_callee</span><span class="p">(</span>
            <span class="n">root</span><span class="p">,</span>
            <span class="n">callee_name</span><span class="p">,</span>
            <span class="n">param_fqn</span><span class="p">,</span>
        <span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Move a parameter from the root module to a submodule.</span>
<span class="sd">            Args:</span>
<span class="sd">                root: The root module.</span>
<span class="sd">                callee_name: The name of the submodule to move the parameter to.</span>
<span class="sd">                param_fqn: The fully qualified name of the parameter to move.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="c1"># `atoms` is a list of strings representing the path to the</span>
            <span class="c1"># parameter in the original model</span>
            <span class="n">atoms</span> <span class="o">=</span> <span class="n">param_fqn</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="n">mod_itr</span><span class="p">,</span> <span class="n">param_val</span> <span class="o">=</span> <span class="n">_recursive_getattr_with_parent</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">param_fqn</span><span class="p">)</span>
            <span class="c1"># Check whether the parameter is a buffer or a parameter</span>
            <span class="n">is_buffer</span> <span class="o">=</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">mod_itr</span><span class="o">.</span><span class="n">_buffers</span>

            <span class="c1"># Check whether the parameter is a tensor</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">),</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected &#39;</span><span class="si">{</span><span class="n">param_fqn</span><span class="si">}</span><span class="s2">&#39; to be </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">param_val</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="o">+</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot; It might happen if module &#39;</span><span class="si">{</span><span class="n">param_fqn</span><span class="si">}</span><span class="s2">&#39; was passed to some &#39;leaf function&#39;&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(see https://pytorch.org/docs/stable/fx.html#fx.wrap). Please inspect &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;usages of &#39;</span><span class="si">{</span><span class="n">param_fqn</span><span class="si">}</span><span class="s2">&#39; in the traced graph.&quot;</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
                    <span class="k">else</span> <span class="s2">&quot;&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># Get submodule</span>
            <span class="n">callee</span> <span class="o">=</span> <span class="n">root</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">callee_name</span><span class="p">)</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="n">callee</span><span class="p">,</span> <span class="n">param_fqn</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Module </span><span class="si">{</span><span class="n">callee_name</span><span class="si">}</span><span class="s2"> already has a parameter named </span><span class="si">{</span><span class="n">param_fqn</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="c1"># Assign the parameter to the submodule</span>
            <span class="k">if</span> <span class="n">is_buffer</span><span class="p">:</span>
                <span class="n">_assign_attr</span><span class="p">(</span>
                    <span class="n">param_val</span><span class="p">,</span>
                    <span class="n">callee</span><span class="p">,</span>
                    <span class="n">param_fqn</span><span class="p">,</span>
                    <span class="n">attr_kind</span><span class="o">=</span><span class="n">_AttrKind</span><span class="o">.</span><span class="n">BUFFER</span><span class="p">,</span>
                    <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># TODO: handle non-persistent buffer</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_assign_attr</span><span class="p">(</span>
                    <span class="n">param_val</span><span class="p">,</span>
                    <span class="n">callee</span><span class="p">,</span>
                    <span class="n">param_fqn</span><span class="p">,</span>
                    <span class="n">attr_kind</span><span class="o">=</span><span class="n">_AttrKind</span><span class="o">.</span><span class="n">PARAMETER</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Moved parameter </span><span class="si">{</span><span class="n">param_fqn</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">callee_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># noqa: G004</span>

            <span class="c1"># Next step is to replace placeholder of submodule with a get_attr.</span>
            <span class="c1"># Those placeholders are created by `split_module` inside each</span>
            <span class="c1"># submodule.</span>
            <span class="c1"># Update: this step is now moved to `_sink_params` because</span>
            <span class="c1"># `_sink_params` can do it recursively (i.e. for modules inside</span>
            <span class="c1"># submodule)</span>

            <span class="n">to_delete</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">mod_itr</span><span class="p">,</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="c1"># Get the list of all parameters in the root module</span>
        <span class="n">attr_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;get_attr&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">attr_nodes</span><span class="p">:</span>
            <span class="c1"># Check whether the parameter is used in only one submodule</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">users</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Parameter </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2"> used in multiple stages: </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">users</span><span class="si">}</span><span class="s2">.&quot;</span>  <span class="c1"># noqa: G004</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">users</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span>
                <span class="c1"># Move parameter into submodule</span>
                <span class="n">move_param_to_callee</span><span class="p">(</span>
                    <span class="n">split</span><span class="p">,</span>
                    <span class="n">user</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># [aliasing] store tensor id -&gt; list of FQNs, built from state dict</span>
        <span class="c1"># Also assign non-persistent buffers</span>
        <span class="n">id_to_fqns</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">set</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">keep_vars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">id_to_fqns</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">tensor</span><span class="p">)]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
            <span class="n">id_to_fqns</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">tensor</span><span class="p">)]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span>

        <span class="c1"># After moving the params to their corresponding hierarchies, we also</span>
        <span class="c1"># need to move the `get_attr` nodes from the root of the graph to those</span>
        <span class="c1"># hierarchies.</span>
        <span class="c1"># [aliasing] use id -&gt; fqn mapping to list out all valid FQNs</span>
        <span class="n">inputs_to_state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attr_nodes</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">tensor</span> <span class="o">=</span> <span class="n">_recursive_getattr_with_parent</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">attr</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
            <span class="n">fqns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">id_to_fqns</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">tensor</span><span class="p">)])</span>
            <span class="k">if</span> <span class="n">fqns</span><span class="p">:</span>
                <span class="n">inputs_to_state</span><span class="p">[</span><span class="n">attr</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">fqns</span>
            <span class="k">elif</span> <span class="n">attr</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">exported_program</span><span class="o">.</span><span class="n">constants</span><span class="p">:</span>  <span class="c1"># lifted constants</span>
                <span class="n">inputs_to_state</span><span class="p">[</span><span class="n">attr</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">attr</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>

        <span class="c1"># [aliasing] for each submodule split, assign attributes on FQNs that may be used.</span>
        <span class="c1"># We determine this based on whether or not the FQN attribute parent exists.</span>
        <span class="c1"># i.e. if the last submodule exists, assign the attribute.</span>
        <span class="n">added_attributes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">keep_vars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">submod</span> <span class="ow">in</span> <span class="n">split</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                    <span class="n">parent</span><span class="p">,</span> <span class="n">child</span> <span class="o">=</span> <span class="n">_recursive_getattr_with_parent</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">fqn</span><span class="p">)</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">parent</span> <span class="ow">and</span> <span class="n">child</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="p">):</span>  <span class="c1"># parent exists, attribute doesn&#39;t -&gt; assign</span>
                        <span class="n">added_attributes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">fqn</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tensor</span><span class="p">)</span>

        <span class="c1"># Deferral deletion: Remove the original attributes (to params) from the</span>
        <span class="c1"># root GraphModule</span>
        <span class="k">for</span> <span class="n">mod_itr</span><span class="p">,</span> <span class="n">last_atom</span> <span class="ow">in</span> <span class="n">to_delete</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="nb">delattr</span><span class="p">(</span><span class="n">mod_itr</span><span class="p">,</span> <span class="n">last_atom</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="c1"># This is expected if the parameter is used in multiple stages</span>
                <span class="k">pass</span>

        <span class="c1"># This is done by (1) `_sink_params` at each submodule;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">submod</span> <span class="ow">in</span> <span class="n">split</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="n">_sink_params</span><span class="p">(</span><span class="n">submod</span><span class="p">,</span> <span class="n">inputs_to_state</span><span class="p">,</span> <span class="p">[])</span>
                <span class="n">submod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">lint</span><span class="p">()</span>
                <span class="n">submod</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

        <span class="c1"># [aliasing] This step is not super necessary, but helps reduce parameter usage/memory.</span>
        <span class="c1"># After _sink_params() routine has run, clean up unused attributes that we previously added.</span>
        <span class="c1"># Determine this based on the get_attr nodes - if not used, remove it.</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">attributes</span> <span class="ow">in</span> <span class="n">added_attributes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">submod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
            <span class="n">unused_attributes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">attributes</span><span class="p">)</span>
            <span class="c1"># track used attributes in the submodule, running DFS on subgraph hierarchy</span>
            <span class="n">stack</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">submod</span><span class="p">)]</span>  <span class="c1"># (scope, submodule)</span>
            <span class="k">while</span> <span class="n">stack</span><span class="p">:</span>
                <span class="n">scope</span><span class="p">,</span> <span class="n">_mod</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_mod</span><span class="p">,</span> <span class="p">(</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">InterpreterModule</span><span class="p">)):</span>
                    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">_mod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;get_attr&quot;</span><span class="p">:</span>
                            <span class="c1"># get_attr might get access deeper level attribute</span>
                            <span class="n">fqn</span> <span class="o">=</span> <span class="n">scope</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="k">if</span> <span class="n">scope</span> <span class="k">else</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span>
                            <span class="n">unused_attributes</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_submod</span> <span class="ow">in</span> <span class="n">_mod</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">scope</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">_name</span> <span class="k">if</span> <span class="n">scope</span> <span class="k">else</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_submod</span><span class="p">))</span>
            <span class="c1"># delete unused attributes</span>
            <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">unused_attributes</span><span class="p">:</span>
                <span class="n">mod_itr</span><span class="p">,</span> <span class="n">atoms</span> <span class="o">=</span> <span class="n">submod</span><span class="p">,</span> <span class="n">attr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">atom</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">mod_itr</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod_itr</span><span class="p">,</span> <span class="n">atom</span><span class="p">)</span>
                <span class="nb">delattr</span><span class="p">(</span><span class="n">mod_itr</span><span class="p">,</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">attr_nodes</span><span class="p">:</span>
            <span class="c1"># And (2): remove `get_attr` node from submod&#39;s arg list</span>
            <span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">users</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span>
                <span class="n">delete_user_reference</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">user</span><span class="p">)</span>
            <span class="c1"># And (3): remove the `get_attr` node from the root graph.</span>
            <span class="n">split</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="n">split</span><span class="o">.</span><span class="n">delete_all_unused_submodules</span><span class="p">()</span>
        <span class="n">split</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">lint</span><span class="p">()</span>
        <span class="n">split</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

        <span class="n">num_stages</span> <span class="o">=</span> <span class="n">Pipe</span><span class="o">.</span><span class="n">_number_and_count_forward_stages</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>

        <span class="n">has_loss_and_backward</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">generated_loss_spec</span> <span class="o">=</span> <span class="n">output_loss_value_spec</span>

        <span class="k">if</span> <span class="n">output_loss_value_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_node</span><span class="p">,</span> <span class="n">output_node</span><span class="p">,</span> <span class="n">generated_loss_spec</span> <span class="o">=</span> <span class="n">_find_loss_output</span><span class="p">(</span>
                <span class="n">mod</span><span class="p">,</span> <span class="n">split</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">output_loss_value_spec</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">loss_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_insert_stage_symbolic_backward</span><span class="p">(</span>
                    <span class="n">split</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span>
                    <span class="n">loss_node</span><span class="p">,</span>
                    <span class="n">output_node</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">split</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>
                <span class="n">has_loss_and_backward</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Pipeline is in training mode, backward pass generated&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Did not find any loss value according to </span><span class="si">{</span><span class="n">output_loss_value_spec</span><span class="si">=}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Pipeline is in inference mode, backward pass not generated&quot;</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Full pipe model:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># noqa: G004</span>

        <span class="k">return</span> <span class="n">Pipe</span><span class="p">(</span>
            <span class="n">split</span><span class="p">,</span>
            <span class="n">num_stages</span><span class="p">,</span>
            <span class="n">has_loss_and_backward</span><span class="p">,</span>
            <span class="n">generated_loss_spec</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">print_readable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Print the pipe in a human-readable format.</span>
<span class="sd">        This will print both the root pipe and each stage module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_trace_with_export</span><span class="p">(</span>
        <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">example_args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">example_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Tracing model ...&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
                <span class="n">mod</span><span class="p">,</span>
                <span class="n">example_args</span><span class="p">,</span>
                <span class="n">example_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;It seems that we cannot capture your model as a full graph. &quot;</span>
                <span class="s2">&quot;Typical reasons include graph breaks, data/shape-dependent &quot;</span>
                <span class="s2">&quot;control flow, or missing meta kernels for custom operators. &quot;</span>
                <span class="s2">&quot;You can use our manual pipeline interfaces, or try to fix the &quot;</span>
                <span class="s2">&quot;graph breaks, see https://pytorch.org/docs/stable/export.html&quot;</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="k">return</span> <span class="n">ep</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_tracing</span><span class="p">(</span>
        <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">example_args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">example_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">split_policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">],</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># If a param will be used in multiple pipeline stages, we default the strategy to REPLICATE&#39;ing the param across</span>
        <span class="c1"># stages instead of TRANSMIT&#39;ting it</span>
        <span class="n">multi_use_param_spec</span> <span class="o">=</span> <span class="n">MultiUseParameterConfig</span><span class="o">.</span><span class="n">REPLICATE</span>

        <span class="c1"># Figure out which output is loss from output_chunk_spec</span>
        <span class="n">output_loss_value_spec</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Deprecated</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        if output_chunk_spec is not None:</span>
<span class="sd">            output_loss_value_spec = map_aggregate(</span>
<span class="sd">                output_chunk_spec, lambda v: isinstance(v, _LossReducer)</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Trace with export</span>
        <span class="n">exported_program</span> <span class="o">=</span> <span class="n">Pipe</span><span class="o">.</span><span class="n">_trace_with_export</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span>
            <span class="n">example_args</span><span class="p">,</span>
            <span class="n">example_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipe</span><span class="o">.</span><span class="n">_from_traced</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span>
            <span class="n">exported_program</span><span class="p">,</span>
            <span class="n">multi_use_param_spec</span><span class="p">,</span>
            <span class="n">output_loss_value_spec</span><span class="o">=</span><span class="n">output_loss_value_spec</span><span class="p">,</span>
            <span class="n">split_policy</span><span class="o">=</span><span class="n">split_policy</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Users want the first pipeline stage to accept kwargs if the original</span>
        <span class="c1"># program does. This is controlled by the `_codegen` field of the graph,</span>
        <span class="c1"># so we make a copy here. Note: we only want the input spec and not the</span>
        <span class="c1"># output spec, because the output spec is for the last stage. Maybe a</span>
        <span class="c1"># TODO? Not sure yet.</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">split_gm</span>
        <span class="n">traced</span> <span class="o">=</span> <span class="n">exported_program</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
        <span class="n">submod0</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">split</span><span class="o">.</span><span class="n">children</span><span class="p">()))</span>
        <span class="n">submod0_sign</span> <span class="o">=</span> <span class="n">signature</span><span class="p">(</span><span class="n">submod0</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>
        <span class="n">model_sign</span> <span class="o">=</span> <span class="n">signature</span><span class="p">(</span><span class="n">traced</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_sign</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">submod0_sign</span><span class="o">.</span><span class="n">parameters</span><span class="p">):</span>
            <span class="c1"># We don&#39;t change the signature of the first stage if it takes</span>
            <span class="c1"># different number of args than original model</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Original model takes </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">model_sign</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span><span class="si">}</span><span class="s2"> args but the &quot;</span>  <span class="c1"># noqa: G004</span>
                <span class="sa">f</span><span class="s2">&quot;first pipeline stage takes </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">submod0_sign</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="s2">&quot;Please provide args to respective pipeline stages.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Support kwargs for the first stage</span>
            <span class="n">submod0</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_codegen</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">traced</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_codegen</span><span class="p">)</span>
            <span class="c1"># `_replace` is actually not &quot;private&quot; or internal. based on this doc:</span>
            <span class="c1"># To prevent conflicts with field names, the method and attribute names</span>
            <span class="c1"># start with an underscore</span>
            <span class="n">submod0</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_codegen</span><span class="o">.</span><span class="n">pytree_info</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">submod0</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_codegen</span><span class="o">.</span><span class="n">pytree_info</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">out_spec</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">submod0</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">pipe</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">info</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PipeInfo</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get information about the pipe.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        PipeInfo</span>
<span class="sd">            A dataclass containing information about the pipe.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">PipeInfo</span><span class="p">(</span>
            <span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">split_gm</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span>
            <span class="n">num_stages</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_stages</span><span class="p">,</span>
            <span class="n">has_loss_and_backward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">has_loss_and_backward</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_stage</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">stage_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_PipelineStage</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a `PipelineStage` given a stage index and distributed group.</span>
<span class="sd">        The `PipelineStage` can run with `PipelineSchedule`s.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Find stage module</span>
        <span class="n">stage_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_stage_module</span><span class="p">(</span><span class="n">stage_index</span><span class="p">)</span>

        <span class="c1"># Move ops argument to device</span>
        <span class="c1"># Today PT2 tracer does not treat `x.device` as a symbolic device;</span>
        <span class="c1"># instead, the device of tracing time got burned into the generated</span>
        <span class="c1"># code.  Here we provide a workaround for users to manually modify the</span>
        <span class="c1"># &quot;device&quot; kwarg of operations. Such operation may include:</span>
        <span class="c1"># `torch.ones`, `torch.zeros`, `torch.rand`, etc.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stage_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
            <span class="n">_modify_graph_op_device</span><span class="p">(</span><span class="n">stage_module</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected a `torch.fx.GraphModule` but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">stage_module</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G004</span>
            <span class="p">)</span>

        <span class="c1"># Detach pipe info</span>
        <span class="c1"># Note: be careful what&#39;s included in `pipe_info`. We don&#39;t want to keep</span>
        <span class="c1"># a reference to `Pipe` or `Pipe.split_gm` which stops python from</span>
        <span class="c1"># recycling them. When python recycles them, other stage modules (which</span>
        <span class="c1"># are irrelevant to current rank) can be automatically freed.</span>
        <span class="n">pipe_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">_PipelineStage</span><span class="p">(</span><span class="n">stage_module</span><span class="p">,</span> <span class="n">stage_index</span><span class="p">,</span> <span class="n">pipe_info</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="SplitPoint"><a class="viewcode-back" href="../../../../distributed.pipelining.html#torch.distributed.pipelining.SplitPoint">[docs]</a><span class="k">class</span> <span class="nc">SplitPoint</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">BEGINNING</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">END</span> <span class="o">=</span> <span class="mi">2</span></div>


<span class="c1"># For backward compatibility, we kept the PipeSplitWrapper class because `class</span>
<span class="c1"># SplitPoint` used to be defined in this class.</span>
<span class="k">class</span> <span class="nc">PipeSplitWrapper</span><span class="p">:</span>
    <span class="c1"># Create a class alias for BC</span>
    <span class="n">SplitPoint</span> <span class="o">=</span> <span class="n">SplitPoint</span>


<span class="k">def</span> <span class="nf">_split_before_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">pipe_split</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_orig_forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_split_after_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_orig_forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">pipe_split</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">annotate_split_points</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">spec</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SplitPoint</span><span class="p">]):</span>
    <span class="c1"># TODO: make this implementation out-of-place?</span>
    <span class="k">for</span> <span class="n">qualname</span><span class="p">,</span> <span class="n">split_type</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">atoms</span> <span class="o">=</span> <span class="n">qualname</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
        <span class="n">predecessor_module</span> <span class="o">=</span> <span class="n">mod</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">atom</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">atoms</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">predecessor_module</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">predecessor_module</span><span class="p">,</span> <span class="n">atom</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Specified target </span><span class="si">{</span><span class="n">qualname</span><span class="si">}</span><span class="s2"> referenced &quot;</span>
                    <span class="sa">f</span><span class="s1">&#39;nonexistent module </span><span class="si">{</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">atoms</span><span class="p">[:</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="n">mod_to_wrap</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">predecessor_module</span><span class="p">,</span> <span class="n">atoms</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mod_to_wrap</span><span class="o">.</span><span class="n">_orig_forward</span> <span class="o">=</span> <span class="n">mod_to_wrap</span><span class="o">.</span><span class="n">forward</span>
        <span class="k">if</span> <span class="n">split_type</span> <span class="o">==</span> <span class="n">SplitPoint</span><span class="o">.</span><span class="n">BEGINNING</span><span class="p">:</span>
            <span class="n">mod_to_wrap</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">_split_before_forward</span><span class="p">,</span> <span class="n">mod_to_wrap</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">split_type</span> <span class="o">==</span> <span class="n">SplitPoint</span><span class="o">.</span><span class="n">END</span><span class="p">:</span>
            <span class="n">mod_to_wrap</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">_split_after_forward</span><span class="p">,</span> <span class="n">mod_to_wrap</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown split point type.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="pipeline"><a class="viewcode-back" href="../../../../distributed.pipelining.html#torch.distributed.pipelining.pipeline">[docs]</a><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">mb_args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">mb_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">split_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SplitPoint</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">split_policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">],</span> <span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Pipe</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split a module based on a specification.</span>

<span class="sd">    See `Pipe` for more details.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    module:</span>
<span class="sd">        The module to be splitted.</span>
<span class="sd">    mb_args:</span>
<span class="sd">        Example positional inputs, in micro-batch form.</span>
<span class="sd">    mb_kwargs:</span>
<span class="sd">        Example keyword inputs, in micro-batch form. (default: `None`)</span>
<span class="sd">    split_spec:</span>
<span class="sd">        A dictionary using submodule names as split marker. (default: `None`)</span>
<span class="sd">    split_policy:</span>
<span class="sd">        The policy to use for splitting the module. (default: `None`)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A pipeline representation of class `Pipe`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">split_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">split_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot specify both `split_spec` and `split_policy`. Please use only one of them.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">split_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Annotate split points in the module based on user spec</span>
        <span class="n">annotate_split_points</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">split_spec</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Pipe</span><span class="o">.</span><span class="n">from_tracing</span><span class="p">(</span>
            <span class="n">mod</span><span class="o">=</span><span class="n">module</span><span class="p">,</span>
            <span class="n">example_args</span><span class="o">=</span><span class="n">mb_args</span><span class="p">,</span>
            <span class="n">example_kwargs</span><span class="o">=</span><span class="n">mb_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Use split policy</span>
        <span class="k">return</span> <span class="n">Pipe</span><span class="o">.</span><span class="n">from_tracing</span><span class="p">(</span>
            <span class="n">mod</span><span class="o">=</span><span class="n">module</span><span class="p">,</span>
            <span class="n">example_args</span><span class="o">=</span><span class="n">mb_args</span><span class="p">,</span>
            <span class="n">example_kwargs</span><span class="o">=</span><span class="n">mb_kwargs</span><span class="p">,</span>
            <span class="n">split_policy</span><span class="o">=</span><span class="n">split_policy</span><span class="p">,</span>
        <span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>