


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.attention.flex_attention &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/attention/flex_attention.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.5.0a0+git72f2b29 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/attention/flex_attention.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/get_start_xpu.html">Pytorch 2.4: Getting Started on Intel GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch_environment_variables.html">Torch Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../attention.html">torch.nn.attention</a> &gt;</li>
        
      <li>torch.nn.attention.flex_attention</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.attention.flex_attention</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-decorators</span>
<span class="c1"># mypy: allow-untyped-defs</span>
<span class="c1"># flake8: noqa C101</span>
<span class="sd">&quot;&quot;&quot;This module implements the user facing API for flex_attention in PyTorch.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._higher_order_ops.flex_attention</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">flex_attention</span> <span class="k">as</span> <span class="n">flex_attention_hop</span><span class="p">,</span>
    <span class="n">TransformGetItemToIndex</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch._higher_order_ops.utils</span> <span class="kn">import</span> <span class="n">_set_compilation_env</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.proxy_tensor</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_temp_remove_pre_dispatch_torch_function_mode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.nn.attention._utils</span> <span class="kn">import</span> <span class="n">_validate_sdpa_input</span>
<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_map_only</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;BlockMask&quot;</span><span class="p">,</span>
    <span class="s2">&quot;flex_attention&quot;</span><span class="p">,</span>
    <span class="s2">&quot;create_block_mask&quot;</span><span class="p">,</span>
    <span class="s2">&quot;create_mask&quot;</span><span class="p">,</span>
    <span class="s2">&quot;or_masks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;and_masks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;noop_mask&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">_score_mod_signature</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span>
<span class="n">_mask_mod_signature</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">_ModificationType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Enum for the type of modification function.</span>
<span class="sd">    - SCORE_MOD: score_mod function which accepts a score as the first argument</span>
<span class="sd">    - mask_mod: mask function which does not accept a score and is only used for generating</span>
<span class="sd">    block mask</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SCORE_MOD</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">MASK_MOD</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">UNKNOWN</span> <span class="o">=</span> <span class="mi">3</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">assume_constant_result</span>
<span class="k">def</span> <span class="nf">_get_mod_type</span><span class="p">(</span><span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_ModificationType</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the type of modification function.</span>
<span class="sd">    This function inspects the number of positional arguments of the function to determine</span>
<span class="sd">    the type of modification function. If the function has 5 positional arguments, it is</span>
<span class="sd">    considered as a score_mod function. If the function has 4 positional arguments, it is</span>
<span class="sd">    considered as a mask function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_positional_args</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
        <span class="mi">1</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">default</span> <span class="o">==</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">num_positional_args</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">num_positional_args</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="k">if</span> <span class="n">num_positional_args</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_ModificationType</span><span class="o">.</span><span class="n">SCORE_MOD</span>
    <span class="k">elif</span> <span class="n">num_positional_args</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_ModificationType</span><span class="o">.</span><span class="n">MASK_MOD</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_ModificationType</span><span class="o">.</span><span class="n">UNKNOWN</span>


<span class="c1"># Need to define it here so that Dynamo doesn&#39;t skip it</span>
<span class="k">def</span> <span class="nf">_vmap_for_bhqkv</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prefix</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">suffix</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">out_dims</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">group_dim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Used to vmap both score_mods and mask_mods over 4-dimensional/5-dimension inputs.</span>
<span class="sd">    Mapping over the [b, hq, q_idx, kv_idx] or [b, hkv, g, q_idx, kv_idx] dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (callable): The function to vmap.</span>
<span class="sd">        prefix (tuple): The prefix of the vmap. For score mod functions,</span>
<span class="sd">                        this should be set to (0,). For mask_mods = ()</span>
<span class="sd">        suffix (tuple): We need to add (0,) if gradOut is being mapped over,</span>
<span class="sd">                        and (None,) * len(other_buffers).</span>
<span class="sd">        out_dims (tuple): For forward cases, keep this as the default 0 since</span>
<span class="sd">                          we are only returning 1 output. For backwards, the joint</span>
<span class="sd">                          graph returns grads for B, H, Q_idx, KV_idx and other_buffers,</span>
<span class="sd">                          so we set this to (0, None, None, None, None) + (None,) * len(other_buffers).</span>

<span class="sd">    Returns:</span>
<span class="sd">        callable: The vmapped function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We vamp a function 4 times, broadcasting the [b, h, q_idx, kv_idx] dimensions</span>
    <span class="n">dimensions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">group_dim</span><span class="p">:</span>
        <span class="n">dimensions</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="p">]</span>

    <span class="n">dimensions</span> <span class="o">+=</span> <span class="p">[</span>
        <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">for</span> <span class="n">dims</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">:</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_dims</span><span class="o">=</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">dims</span> <span class="o">+</span> <span class="n">suffix</span><span class="p">,</span> <span class="n">out_dims</span><span class="o">=</span><span class="n">out_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fn</span>


<span class="k">def</span> <span class="nf">_identity</span><span class="p">(</span>
    <span class="n">score</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">token_q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">token_kv</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">score</span>


<div class="viewcode-block" id="noop_mask"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.noop_mask">[docs]</a><span class="k">def</span> <span class="nf">noop_mask</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">token_q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">token_kv</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a noop mask_mod&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span></div>


<span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">128</span>


<span class="k">def</span> <span class="nf">_ordered_to_dense</span><span class="p">(</span><span class="n">num_blocks_in_row</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">):</span>
    <span class="n">num_rows</span> <span class="o">=</span> <span class="n">col_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">num_cols</span> <span class="o">=</span> <span class="n">col_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">batch_dims</span> <span class="o">=</span> <span class="n">num_blocks_in_row</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">num_blocks_in_row</span><span class="o">.</span><span class="n">device</span>

    <span class="k">def</span> <span class="nf">create_dense_one</span><span class="p">(</span><span class="n">kv_num_blocks</span><span class="p">,</span> <span class="n">kv_indices</span><span class="p">):</span>
        <span class="n">dense_mask</span> <span class="o">=</span> <span class="n">kv_indices</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">num_cols</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="n">row_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">col_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">index_mask</span> <span class="o">=</span> <span class="n">col_range</span> <span class="o">&lt;</span> <span class="n">kv_num_blocks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># We write to one spot &quot;out of bounds&quot;</span>
        <span class="n">valid_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">index_mask</span><span class="p">,</span> <span class="n">kv_indices</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">)</span>

        <span class="c1"># set the values in &#39;a&#39; to 1 where the indices are valid</span>
        <span class="n">dense_mask</span><span class="p">[</span><span class="n">row_indices</span><span class="p">,</span> <span class="n">valid_indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">dense_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_cols</span><span class="p">]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

    <span class="n">create_dense_batched</span> <span class="o">=</span> <span class="n">create_dense_one</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)):</span>
        <span class="n">create_dense_batched</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">create_dense_batched</span><span class="p">,</span> <span class="n">in_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">create_dense_batched</span><span class="p">(</span><span class="n">num_blocks_in_row</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_dense_to_ordered</span><span class="p">(</span><span class="n">dense_mask</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="n">dense_mask</span> <span class="o">=</span> <span class="n">dense_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">num_blocks_in_row</span> <span class="o">=</span> <span class="n">dense_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">col_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dense_mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">num_blocks_in_row</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
        <span class="n">col_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_transpose_ordered</span><span class="p">(</span><span class="n">num_blocks_in_row</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">):</span>
    <span class="n">dense</span> <span class="o">=</span> <span class="n">_ordered_to_dense</span><span class="p">(</span><span class="n">num_blocks_in_row</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_dense_to_ordered</span><span class="p">(</span><span class="n">dense</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>


<div class="viewcode-block" id="BlockMask"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask">[docs]</a><span class="k">class</span> <span class="nc">BlockMask</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    BlockMask is our format for representing a block-sparse attention mask.</span>
<span class="sd">    It is somewhat of a cross in-between BCSR and a non-sparse format.</span>

<span class="sd">    Basics</span>
<span class="sd">    ------</span>
<span class="sd">    A block-sparse mask means that instead of representing the sparsity of</span>
<span class="sd">    individual elements in the mask, a KV_BLOCK_SIZE x Q_BLOCK_SIZE block is</span>
<span class="sd">    considered sparse only if every element within that block is sparse.</span>
<span class="sd">    This aligns well with hardware, which generally expects to perform</span>
<span class="sd">    contiguous loads and computation.</span>

<span class="sd">    This format is primarily optimized for 1. simplicity, and 2. kernel</span>
<span class="sd">    efficiency. Notably, it is *not* optimized for size, as this mask is always</span>
<span class="sd">    reduced by a factor of KV_BLOCK_SIZE * Q_BLOCK_SIZE. If the size is a</span>
<span class="sd">    concern, the tensors can be reduced in size by increasing the block size.</span>

<span class="sd">    The essentials of our format are:</span>

<span class="sd">    num_blocks_in_row: Tensor[ROWS]:</span>
<span class="sd">    Describes the number of blocks present in each row.</span>

<span class="sd">    col_indices: Tensor[ROWS, MAX_BLOCKS_IN_COL]:</span>
<span class="sd">    `col_indices[i]` is the sequence of block positions for row i. The values of</span>
<span class="sd">    this row after `col_indices[i][num_blocks_in_row[i]]` are undefined.</span>

<span class="sd">    For example, to reconstruct the original tensor from this format:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        dense_mask = torch.zeros(ROWS, COLS)</span>
<span class="sd">        for row in range(ROWS):</span>
<span class="sd">            for block_idx in range(num_blocks_in_row[row]):</span>
<span class="sd">                dense_mask[row, col_indices[row, block_idx]] = 1</span>

<span class="sd">    Notably, this format makes it easier to implement a reduction along the</span>
<span class="sd">    *rows* of the mask.</span>

<span class="sd">    Details</span>
<span class="sd">    -------</span>
<span class="sd">    The basics of our format require only kv_num_blocks and kv_indices. But, we</span>
<span class="sd">    have up to 8 tensors on this object. This represents 4 pairs:</span>

<span class="sd">    1. (kv_num_blocks, kv_indices): Used for the forwards pass of attention, as</span>
<span class="sd">    we reduce along the KV dimension.</span>

<span class="sd">    2. [OPTIONAL] (full_kv_num_blocks, full_kv_indices): This is optional and</span>
<span class="sd">    purely an optimization. As it turns out, applying masking to every block</span>
<span class="sd">    is quite expensive! If we specifically know which blocks are &quot;full&quot; and</span>
<span class="sd">    don&#39;t require masking at all, then we can skip applying mask_mod to these</span>
<span class="sd">    blocks. This requires the user to split out a separate mask_mod from the</span>
<span class="sd">    score_mod. For causal masks, this is about a 15% speedup.</span>

<span class="sd">    3. [GENERATED] (q_num_blocks, q_indices): Required for the backwards pass,</span>
<span class="sd">    as computing dKV requires iterating along the mask along the Q dimension. These are autogenerated from 1.</span>

<span class="sd">    4. [GENERATED] (full_q_num_blocks, full_q_indices): Same as above, but for</span>
<span class="sd">    the backwards pass. These are autogenerated from 2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kv_num_blocks</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">kv_indices</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">full_kv_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">full_kv_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">q_num_blocks</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">q_indices</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">full_q_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">full_q_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
    <span class="n">mask_mod</span><span class="p">:</span> <span class="n">_mask_mod_signature</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">kv_num_blocks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">kv_indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">full_kv_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_kv_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">q_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">q_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_q_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_q_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
        <span class="n">mask_mod</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_mask_mod_signature</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">kv_indices</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;BlockMask must have at least 2 dimensions&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kv_num_blocks</span> <span class="o">=</span> <span class="n">kv_num_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span> <span class="o">=</span> <span class="n">kv_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span> <span class="o">=</span> <span class="n">full_kv_num_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_indices</span> <span class="o">=</span> <span class="n">full_kv_indices</span>

        <span class="c1"># Set q_num_blocks and q_indices if provided, otherwise generate them</span>
        <span class="k">if</span> <span class="n">q_num_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">q_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_num_blocks</span> <span class="o">=</span> <span class="n">q_num_blocks</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_indices</span> <span class="o">=</span> <span class="n">q_indices</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_num_blocks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_indices</span> <span class="o">=</span> <span class="n">_transpose_ordered</span><span class="p">(</span>
                <span class="n">kv_num_blocks</span><span class="p">,</span> <span class="n">kv_indices</span>
            <span class="p">)</span>

        <span class="c1"># Set full_q_num_blocks and full_q_indices if provided, otherwise generate them if full_kv tensors are available</span>
        <span class="k">if</span> <span class="n">full_q_num_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">full_q_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_q_num_blocks</span> <span class="o">=</span> <span class="n">full_q_num_blocks</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_q_indices</span> <span class="o">=</span> <span class="n">full_q_indices</span>
        <span class="k">elif</span> <span class="n">full_kv_num_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">full_kv_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_q_num_blocks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_q_indices</span> <span class="o">=</span> <span class="n">_transpose_ordered</span><span class="p">(</span>
                <span class="n">full_kv_num_blocks</span><span class="p">,</span> <span class="n">full_kv_indices</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_q_num_blocks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_q_indices</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mask_mod</span> <span class="o">=</span> <span class="n">mask_mod</span> <span class="k">if</span> <span class="n">mask_mod</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">noop_mask</span>

<div class="viewcode-block" id="BlockMask.as_tuple"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.as_tuple">[docs]</a>    <span class="k">def</span> <span class="nf">as_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flatten</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a tuple of the attributes of the BlockMask.</span>

<span class="sd">        Args:</span>
<span class="sd">            flatten (bool): If True, it will flatten the tuple of (KV_BLOCK_SIZE, Q_BLOCK_SIZE)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">flatten</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">,)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kv_num_blocks</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_num_blocks</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_q_num_blocks</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_q_indices</span><span class="p">,</span>
            <span class="o">*</span><span class="n">block_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mask_mod</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;BlockMask(shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, sparsity=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sparsity</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%, </span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">mask_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">mask_str</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">)&quot;</span>
        <span class="k">return</span> <span class="n">s</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockMask&quot;</span><span class="p">:</span>
        <span class="n">new_kv_num_blocks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_num_blocks</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">new_kv_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">new_kv_num_blocks_full</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">new_kv_indices_full</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">BlockMask</span><span class="p">(</span>
            <span class="n">new_kv_num_blocks</span><span class="p">,</span>
            <span class="n">new_kv_indices</span><span class="p">,</span>
            <span class="n">full_kv_num_blocks</span><span class="o">=</span><span class="n">new_kv_num_blocks_full</span><span class="p">,</span>
            <span class="n">full_kv_indices</span><span class="o">=</span><span class="n">new_kv_indices_full</span><span class="p">,</span>
            <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">,</span>
            <span class="n">mask_mod</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_mod</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;BlockMask(</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    kv_num_blocks=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_num_blocks</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    kv_indices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    full_kv_num_blocks=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    full_kv_indices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">full_kv_indices</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">full_kv_indices</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    q_num_blocks=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">q_num_blocks</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    q_indices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">q_indices</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    full_q_num_blocks=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">full_q_num_blocks</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">full_q_num_blocks</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    full_q_indices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">full_q_indices</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">full_q_indices</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    BLOCK_SIZE=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    sparsity=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sparsity</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    mask_mod=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_mod</span><span class="o">.</span><span class="vm">__name__</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_mod</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;__name__&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">mask_mod</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;)&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the shape of the mask.&quot;&quot;&quot;</span>
        <span class="o">*</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">q_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">q_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">kv_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="p">[</span><span class="n">q_length</span><span class="p">,</span> <span class="n">kv_length</span><span class="p">])</span>

<div class="viewcode-block" id="BlockMask.numel"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.numel">[docs]</a>    <span class="k">def</span> <span class="nf">numel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the number of elements (not accounting for sparsity) in the mask.&quot;&quot;&quot;</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">def</span> <span class="nf">_prod</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">_prod</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockMask.sparsity"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.sparsity">[docs]</a>    <span class="k">def</span> <span class="nf">sparsity</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes the percentage of blocks that are sparse (i.e. not computed)&quot;&quot;&quot;</span>
        <span class="n">total_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">computed_blocks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_num_blocks</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">computed_blocks</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="n">computed_size</span> <span class="o">=</span> <span class="n">computed_blocks</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">BLOCK_SIZE</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">dense_ratio</span> <span class="o">=</span> <span class="n">computed_size</span> <span class="o">/</span> <span class="n">total_size</span>
        <span class="k">return</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dense_ratio</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockMask.to_dense"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.to_dense">[docs]</a>    <span class="k">def</span> <span class="nf">to_dense</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a dense block that is equivalent to the block mask.&quot;&quot;&quot;</span>
        <span class="n">partial_dense</span> <span class="o">=</span> <span class="n">_ordered_to_dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_num_blocks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_indices</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">partial_dense</span> <span class="o">|</span> <span class="n">_ordered_to_dense</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_num_blocks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_kv_indices</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">partial_dense</span></div>

<div class="viewcode-block" id="BlockMask.to_string"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.to_string">[docs]</a>    <span class="k">def</span> <span class="nf">to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">limit</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a string representation of the block mask. Quite nifty.</span>

<span class="sd">        If grid_size is None, prints out an uncompressed version. Warning, it can be quite big!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dense_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
        <span class="o">*</span><span class="n">batch_dims</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">,</span> <span class="n">num_cols</span> <span class="o">=</span> <span class="n">dense_mask</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grid_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">max_rows</span> <span class="o">=</span> <span class="n">grid_size</span>
            <span class="n">max_cols</span> <span class="o">=</span> <span class="n">grid_size</span>
        <span class="k">elif</span> <span class="n">grid_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">max_rows</span> <span class="o">=</span> <span class="n">num_rows</span>
            <span class="n">max_cols</span> <span class="o">=</span> <span class="n">num_cols</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_rows</span><span class="p">,</span> <span class="n">max_cols</span> <span class="o">=</span> <span class="n">grid_size</span>

        <span class="k">def</span> <span class="nf">create_block_vis</span><span class="p">(</span><span class="o">*</span><span class="n">batch_idx</span><span class="p">):</span>
            <span class="n">descriptors</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">descriptors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">batch_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="n">vis</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">descriptors</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

            <span class="k">def</span> <span class="nf">summarize_section</span><span class="p">(</span><span class="n">section</span><span class="p">):</span>
                <span class="n">percentage</span> <span class="o">=</span> <span class="n">section</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">percentage</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">return</span> <span class="s2">&quot;█&quot;</span>
                <span class="k">elif</span> <span class="n">percentage</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">return</span> <span class="s2">&quot; &quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="s2">&quot;░&quot;</span>

            <span class="k">def</span> <span class="nf">cdiv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">//</span> <span class="n">b</span>

            <span class="n">row_step</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cdiv</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">max_rows</span><span class="p">))</span>
            <span class="n">col_step</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cdiv</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="n">max_cols</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">,</span> <span class="n">row_step</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">,</span> <span class="n">col_step</span><span class="p">):</span>
                    <span class="n">cur_mask</span> <span class="o">=</span> <span class="n">dense_mask</span>
                    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch_idx</span><span class="p">:</span>
                        <span class="n">cur_mask</span> <span class="o">=</span> <span class="n">cur_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="n">char</span> <span class="o">=</span> <span class="n">summarize_section</span><span class="p">(</span>
                        <span class="n">cur_mask</span><span class="p">[</span><span class="n">r</span> <span class="p">:</span> <span class="n">r</span> <span class="o">+</span> <span class="n">row_step</span><span class="p">,</span> <span class="n">c</span> <span class="p">:</span> <span class="n">c</span> <span class="o">+</span> <span class="n">col_step</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="n">vis</span> <span class="o">+=</span> <span class="n">char</span> <span class="o">*</span> <span class="mi">2</span>
                <span class="n">vis</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="k">return</span> <span class="n">vis</span>

        <span class="n">total_vis</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">batch_dims</span><span class="p">])</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">limit</span><span class="p">:</span>
                <span class="n">total_vis</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
                <span class="n">total_vis</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;To print out more, set BlockMask.to_string(limit=N)&quot;</span><span class="p">)</span>
                <span class="n">total_vis</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="s2">&quot;You can also index (BlockMask[batch, head]) to choose a specific batch or head&quot;</span>
                <span class="p">)</span>
                <span class="k">break</span>
            <span class="n">block_vis</span> <span class="o">=</span> <span class="n">create_block_vis</span><span class="p">(</span><span class="o">*</span><span class="n">batch_idx</span><span class="p">)</span>
            <span class="n">total_vis</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block_vis</span><span class="p">)</span>

        <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">total_vis</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockMask.to"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Moves the BlockMask to the specified device.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (torch.device or str): The target device to move the BlockMask to.</span>
<span class="sd">                Can be a torch.device object or a string (e.g., &#39;cpu&#39;, &#39;cuda:0&#39;).</span>

<span class="sd">        Returns:</span>
<span class="sd">            BlockMask: A new BlockMask instance with all tensor components moved</span>
<span class="sd">            to the specified device.</span>

<span class="sd">        Note:</span>
<span class="sd">            This method does not modify the original BlockMask in-place.</span>
<span class="sd">            Instead, it returns a new BlockMask instance where invidual tensor attributes</span>
<span class="sd">            may or may not be moved to the specified device, depending on their</span>
<span class="sd">            current device placement.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mapped_attributes</span> <span class="o">=</span> <span class="n">tree_map_only</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">as_tuple</span><span class="p">(</span><span class="n">flatten</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">BlockMask</span><span class="p">(</span><span class="o">*</span><span class="n">mapped_attributes</span><span class="p">)</span></div></div>


<span class="k">def</span> <span class="nf">_broadcast_to_dim</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="k">while</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">_round_up_to_multiple</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">multiple</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">multiple</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple</span> <span class="o">*</span> <span class="n">multiple</span>


<span class="k">def</span> <span class="nf">_convert_mask_to_block_mask</span><span class="p">(</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">KV_BLOCK_SIZE</span><span class="o">=</span><span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
    <span class="n">Q_BLOCK_SIZE</span><span class="o">=</span><span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
    <span class="n">separate_full_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">_broadcast_to_dim</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">KV</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">Q</span> <span class="o">%</span> <span class="n">Q_BLOCK_SIZE</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">KV</span> <span class="o">%</span> <span class="n">KV_BLOCK_SIZE</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span> <span class="o">//</span> <span class="n">Q_BLOCK_SIZE</span><span class="p">,</span> <span class="n">Q_BLOCK_SIZE</span><span class="p">,</span> <span class="n">KV</span> <span class="o">//</span> <span class="n">KV_BLOCK_SIZE</span><span class="p">,</span> <span class="n">KV_BLOCK_SIZE</span>
    <span class="p">)</span>  <span class="c1"># [B, H, Q//Q_BLOCK_SIZE, Q_BLOCK_SIZE, KV//KV_BLOCK_SIZE, KV_BLOCK_SIZE]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span>
        <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span>
    <span class="p">)</span>  <span class="c1"># [B, H, Q//Q_BLOCK_SIZE, KV//KV_BLOCK_SIZE, Q_BLOCK_SIZE, KV_BLOCK_SIZE]</span>
    <span class="n">mask_block_sum</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>  <span class="c1"># [B, H, Q//Q_BLOCK_SIZE, KV//KV_BLOCK_SIZE]</span>
    <span class="k">if</span> <span class="n">separate_full_blocks</span><span class="p">:</span>
        <span class="n">full_block_sum</span> <span class="o">=</span> <span class="n">Q_BLOCK_SIZE</span> <span class="o">*</span> <span class="n">KV_BLOCK_SIZE</span>
        <span class="n">full_blocks</span> <span class="o">=</span> <span class="n">mask_block_sum</span> <span class="o">==</span> <span class="n">full_block_sum</span>
        <span class="n">partial_blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask_block_sum</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">mask_block_sum</span> <span class="o">&lt;</span> <span class="n">full_block_sum</span><span class="p">)</span>
        <span class="n">partial_blocks</span> <span class="o">=</span> <span class="n">partial_blocks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="n">full_blocks</span> <span class="o">=</span> <span class="n">full_blocks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">partial_blocks</span><span class="p">,</span> <span class="n">full_blocks</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">partial_blocks</span> <span class="o">=</span> <span class="n">mask_block_sum</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">partial_blocks</span> <span class="o">=</span> <span class="n">partial_blocks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">partial_blocks</span><span class="p">,</span> <span class="kc">None</span>


<div class="viewcode-block" id="or_masks"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.or_masks">[docs]</a><span class="k">def</span> <span class="nf">or_masks</span><span class="p">(</span><span class="o">*</span><span class="n">mask_mods</span><span class="p">:</span> <span class="n">_mask_mod_signature</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_mask_mod_signature</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a mask_mod that&#39;s the union of provided mask_mods&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">callable</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">mask_mods</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All inputs should be callable mask_mods: </span><span class="si">{</span><span class="n">mask_mods</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">or_mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">mask</span> <span class="ow">in</span> <span class="n">mask_mods</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">result</span> <span class="o">|</span> <span class="n">mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">return</span> <span class="n">or_mask</span></div>


<div class="viewcode-block" id="and_masks"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.and_masks">[docs]</a><span class="k">def</span> <span class="nf">and_masks</span><span class="p">(</span><span class="o">*</span><span class="n">mask_mods</span><span class="p">:</span> <span class="n">_mask_mod_signature</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_mask_mod_signature</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a mask_mod that&#39;s the intersection of provided mask_mods&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">callable</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">mask_mods</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All inputs should be callable mask_mods: </span><span class="si">{</span><span class="n">mask_mods</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">and_mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">mask</span> <span class="ow">in</span> <span class="n">mask_mods</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">result</span> <span class="o">&amp;</span> <span class="n">mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">return</span> <span class="n">and_mask</span></div>


<span class="k">def</span> <span class="nf">_convert_block_mask_to_mask</span><span class="p">(</span>
    <span class="n">block_mask</span><span class="p">,</span>
    <span class="n">KV_BLOCK_SIZE</span><span class="o">=</span><span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
    <span class="n">Q_BLOCK_SIZE</span><span class="o">=</span><span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">block_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">KV</span> <span class="o">=</span> <span class="n">block_mask</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">block_mask</span> <span class="o">=</span> <span class="n">block_mask</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">Q_BLOCK_SIZE</span><span class="p">,</span> <span class="n">KV_BLOCK_SIZE</span><span class="p">,</span> <span class="o">*</span><span class="n">block_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">block_mask</span> <span class="o">=</span> <span class="n">block_mask</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span> <span class="o">*</span> <span class="n">Q_BLOCK_SIZE</span><span class="p">,</span> <span class="n">KV</span> <span class="o">*</span> <span class="n">KV_BLOCK_SIZE</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">block_mask</span>


<span class="k">def</span> <span class="nf">_create_sparse_block_from_block_mask</span><span class="p">(</span>
    <span class="n">block_mask</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">mask_mod</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
    <span class="n">KV_BLOCK_SIZE</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
    <span class="n">Q_BLOCK_SIZE</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BlockMask</span><span class="p">:</span>
    <span class="n">partial_blocks</span><span class="p">,</span> <span class="n">full_blocks</span> <span class="o">=</span> <span class="n">block_mask</span>

    <span class="n">partial_bm</span> <span class="o">=</span> <span class="n">_dense_to_ordered</span><span class="p">(</span><span class="n">partial_blocks</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">full_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">full_bm</span> <span class="o">=</span> <span class="n">_dense_to_ordered</span><span class="p">(</span><span class="n">full_blocks</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">full_bm</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">BlockMask</span><span class="p">(</span>  <span class="c1"># type: ignore[call-arg]</span>
        <span class="n">partial_bm</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">partial_bm</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">full_bm</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">full_bm</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="p">(</span><span class="n">KV_BLOCK_SIZE</span><span class="p">,</span> <span class="n">Q_BLOCK_SIZE</span><span class="p">),</span>
        <span class="n">mask_mod</span><span class="o">=</span><span class="n">mask_mod</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="create_mask"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.create_mask">[docs]</a><span class="k">def</span> <span class="nf">create_mask</span><span class="p">(</span>
    <span class="n">mod_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_score_mod_signature</span><span class="p">,</span> <span class="n">_mask_mod_signature</span><span class="p">],</span>
    <span class="n">B</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_LEN</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">KV_LEN</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
    <span class="n">_compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This function creates a mask tensor from a mod_fn function.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod_fn (Union[_score_mod_signature, _mask_mod_signature]): Function to modify attention scores.</span>
<span class="sd">        B (int): Batch size.</span>
<span class="sd">        H (int): Number of query heads.</span>
<span class="sd">        Q_LEN (int): Sequence length of query.</span>
<span class="sd">        KV_LEN (int): Sequence length of key/value.</span>
<span class="sd">        device (str): Device to run the mask creation on.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mask (Tensor): A mask tensor with shape (B, H, M, N).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Q_LEN</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">KV_LEN</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># TODO: fix this</span>
    <span class="c1"># Lack instantiation support for __torch_function__ mode support under compile</span>
    <span class="k">if</span> <span class="n">_compile</span><span class="p">:</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">TransformGetItemToIndex</span><span class="p">()</span>  <span class="c1"># type: ignore[assignment]</span>
    <span class="n">mod_type</span> <span class="o">=</span> <span class="n">_get_mod_type</span><span class="p">(</span><span class="n">mod_fn</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mod_type</span> <span class="o">==</span> <span class="n">_ModificationType</span><span class="o">.</span><span class="n">SCORE_MOD</span><span class="p">:</span>
            <span class="n">score_mod</span> <span class="o">=</span> <span class="n">mod_fn</span>
            <span class="n">score_mod</span> <span class="o">=</span> <span class="n">_vmap_for_bhqkv</span><span class="p">(</span><span class="n">score_mod</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>  <span class="c1"># first input is score</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">score_mod</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q_LEN</span><span class="p">,</span> <span class="n">KV_LEN</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isneginf</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">mask</span>
        <span class="k">elif</span> <span class="n">mod_type</span> <span class="o">==</span> <span class="n">_ModificationType</span><span class="o">.</span><span class="n">MASK_MOD</span><span class="p">:</span>
            <span class="n">mask_mod</span> <span class="o">=</span> <span class="n">mod_fn</span>
            <span class="n">mask_mod</span> <span class="o">=</span> <span class="n">_vmap_for_bhqkv</span><span class="p">(</span><span class="n">mask_mod</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="p">())</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask_mod</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">mask</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span></div>


<span class="k">def</span> <span class="nf">_create_block_mask_inner</span><span class="p">(</span>
    <span class="n">mask_mod</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">B</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_LEN</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">KV_LEN</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">KV_BLOCK_SIZE</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_BLOCK_SIZE</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Work around for being unable to instantiate __torch_function__ mode under compile.</span>
<span class="sd">    `create_block_mask` will compile this inner function and wrap the call to this</span>
<span class="sd">    with the __torch_function__ mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask_tensor</span> <span class="o">=</span> <span class="n">create_mask</span><span class="p">(</span><span class="n">mask_mod</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q_LEN</span><span class="p">,</span> <span class="n">KV_LEN</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">partial_block_mask</span><span class="p">,</span> <span class="n">full_block_mask</span> <span class="o">=</span> <span class="n">_convert_mask_to_block_mask</span><span class="p">(</span>
        <span class="n">mask_tensor</span><span class="p">,</span>
        <span class="n">KV_BLOCK_SIZE</span><span class="o">=</span><span class="n">KV_BLOCK_SIZE</span><span class="p">,</span>
        <span class="n">Q_BLOCK_SIZE</span><span class="o">=</span><span class="n">Q_BLOCK_SIZE</span><span class="p">,</span>
        <span class="n">separate_full_blocks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_create_sparse_block_from_block_mask</span><span class="p">(</span>
        <span class="p">(</span><span class="n">partial_block_mask</span><span class="p">,</span> <span class="n">full_block_mask</span><span class="p">),</span> <span class="n">mask_mod</span>
    <span class="p">)</span>


<div class="viewcode-block" id="create_block_mask"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.create_block_mask">[docs]</a><span class="k">def</span> <span class="nf">create_block_mask</span><span class="p">(</span>
    <span class="n">mask_mod</span><span class="p">:</span> <span class="n">_mask_mod_signature</span><span class="p">,</span>
    <span class="n">B</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">H</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">Q_LEN</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">KV_LEN</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
    <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">_DEFAULT_SPARSE_BLOCK_SIZE</span><span class="p">,</span>
    <span class="n">_compile</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BlockMask</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This function creates a block mask tuple from a mask_mod function.</span>

<span class="sd">    Args:</span>
<span class="sd">        mask_mod (Callable): mask_mod function. This is a callable that defines the</span>
<span class="sd">            masking pattern for the attention mechanism. It takes four arguments:</span>
<span class="sd">            b (batch size), h (number of heads), q_idx (query index), and kv_idx (key/value index).</span>
<span class="sd">            It should return a boolean tensor indicating which attention connections are allowed (True)</span>
<span class="sd">            or masked out (False).</span>
<span class="sd">        B (int): Batch size.</span>
<span class="sd">        H (int): Number of query heads.</span>
<span class="sd">        Q_LEN (int): Sequence length of query.</span>
<span class="sd">        KV_LEN (int): Sequence length of key/value.</span>
<span class="sd">        device (str): Device to run the mask creation on.</span>
<span class="sd">        KV_BLOCK_SIZE (int): Block size of block mask for each query.</span>
<span class="sd">        Q_BLOCK_SIZE (int): Block size of block mask for each key/value.</span>
<span class="sd">        _compile (bool): Whether to compile the mask creation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        BlockMask:  A BlockMask object that contains the block mask information.</span>

<span class="sd">    Example Usage:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            def causal_mask(b, h, q_idx, kv_idx):</span>
<span class="sd">                return q_idx &gt;= kv_idx</span>

<span class="sd">            block_mask = create_block_mask(causal_mask, 1, 1, 8192, 8192, device=&quot;cuda&quot;)</span>
<span class="sd">            query = torch.randn(1, 1, 8192, 64, device=&quot;cuda&quot;, dtype=torch.float16)</span>
<span class="sd">            key = torch.randn(1, 1, 8192, 64, device=&quot;cuda&quot;, dtype=torch.float16)</span>
<span class="sd">            value = torch.randn(1, 1, 8192, 64, device=&quot;cuda&quot;, dtype=torch.float16)</span>
<span class="sd">            output = flex_attention(query, key, value, block_mask=block_mask)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mod_type</span> <span class="o">=</span> <span class="n">_get_mod_type</span><span class="p">(</span><span class="n">mask_mod</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">mod_type</span> <span class="o">==</span> <span class="n">_ModificationType</span><span class="o">.</span><span class="n">MASK_MOD</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;create-block_mask requires a mask_mod function! Got </span><span class="si">{</span><span class="n">mask_mod</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">inner_func</span> <span class="o">=</span> <span class="n">_create_block_mask_inner</span>
    <span class="k">if</span> <span class="n">B</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">B</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">H</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">H</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">Q_BLOCK_SIZE</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span>
        <span class="n">KV_BLOCK_SIZE</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Q_BLOCK_SIZE</span><span class="p">,</span> <span class="n">KV_BLOCK_SIZE</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span>

    <span class="k">if</span> <span class="n">Q_LEN</span> <span class="o">&lt;</span> <span class="mi">128</span><span class="p">:</span>
        <span class="n">Q_BLOCK_SIZE</span> <span class="o">=</span> <span class="n">Q_LEN</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Q_LEN</span> <span class="o">=</span> <span class="n">_round_up_to_multiple</span><span class="p">(</span><span class="n">Q_LEN</span><span class="p">,</span> <span class="n">Q_BLOCK_SIZE</span><span class="p">)</span>
    <span class="n">KV_LEN</span> <span class="o">=</span> <span class="n">_round_up_to_multiple</span><span class="p">(</span><span class="n">KV_LEN</span><span class="p">,</span> <span class="n">KV_BLOCK_SIZE</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_compile</span><span class="p">:</span>
        <span class="n">inner_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">inner_func</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">TransformGetItemToIndex</span><span class="p">():</span>
        <span class="n">block_mask</span> <span class="o">=</span> <span class="n">inner_func</span><span class="p">(</span>
            <span class="n">mask_mod</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q_LEN</span><span class="p">,</span> <span class="n">KV_LEN</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">KV_BLOCK_SIZE</span><span class="p">,</span> <span class="n">Q_BLOCK_SIZE</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">block_mask</span></div>


<span class="k">def</span> <span class="nf">_create_empty_block_mask</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BlockMask</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Default block mask for flex attention.</span>
<span class="sd">    If users don&#39;t specify any block sparse mask info, we create this</span>
<span class="sd">    empty block sparse mask. Which creates a BlockMask with 1 block that is the full length</span>
<span class="sd">    of the query and key tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">device</span>
    <span class="n">kv_len</span> <span class="o">=</span> <span class="n">_round_up_to_multiple</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">128</span><span class="p">)</span>
    <span class="n">q_len</span> <span class="o">=</span> <span class="n">_round_up_to_multiple</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">128</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">BlockMask</span><span class="p">(</span>
        <span class="n">kv_num_blocks</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
        <span class="n">kv_indices</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
        <span class="n">full_kv_num_blocks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">full_kv_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="p">(</span><span class="n">kv_len</span><span class="p">,</span> <span class="n">q_len</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_apply_kernel_options</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">kernel_options</span><span class="p">):</span>
    <span class="n">kernel_options</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kernel_options</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kernel_options</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;ROWS_GUARANTEED_SAFE&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kernel_options</span><span class="p">:</span>
        <span class="n">kernel_options</span><span class="p">[</span><span class="s2">&quot;ROWS_GUARANTEED_SAFE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="s2">&quot;PRESCALE_QK&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kernel_options</span><span class="p">:</span>
        <span class="n">kernel_options</span><span class="p">[</span><span class="s2">&quot;PRESCALE_QK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If foward kernel needs to return logsumexp is decided by this rule internally.</span>
    <span class="k">assert</span> <span class="s2">&quot;OUTPUT_LOGSUMEXP&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kernel_options</span>
    <span class="n">any_inputs_require_grad</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">query</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">key</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">value</span><span class="o">.</span><span class="n">requires_grad</span>
    <span class="p">)</span>
    <span class="n">output_logsumexp</span> <span class="o">=</span> <span class="n">any_inputs_require_grad</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
    <span class="n">kernel_options</span><span class="p">[</span><span class="s2">&quot;OUTPUT_LOGSUMEXP&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_logsumexp</span>

    <span class="k">return</span> <span class="n">kernel_options</span>


<div class="viewcode-block" id="flex_attention"><a class="viewcode-back" href="../../../../nn.attention.flex_attention.html#torch.nn.attention.flex_attention.flex_attention">[docs]</a><span class="k">def</span> <span class="nf">flex_attention</span><span class="p">(</span>
    <span class="n">query</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">key</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">score_mod</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_score_mod_signature</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">block_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BlockMask</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">enable_gqa</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kernel_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This function implements scaled dot product attention with an arbitrary attention score modification function.</span>

<span class="sd">    This function computes the scaled dot product attention between query, key, and value tensors with a user-defined</span>
<span class="sd">    attention score modification function. The attention score modification function will be applied after the attention</span>
<span class="sd">    scores have been calculated between the query and key tensors. The attention scores are calculated as follows:</span>

<span class="sd">    The ``score_mod`` function should have the following signature:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def score_mod(</span>
<span class="sd">            score: Tensor,</span>
<span class="sd">            batch: Tensor,</span>
<span class="sd">            head: Tensor,</span>
<span class="sd">            q_idx: Tensor,</span>
<span class="sd">            k_idx: Tensor</span>
<span class="sd">        ) -&gt; Tensor:</span>

<span class="sd">    Where:</span>
<span class="sd">        - ``score``: A scalar tensor representing the attention score,</span>
<span class="sd">          with the same data type and device as the query, key, and value tensors.</span>
<span class="sd">        - ``batch``, ``head``, ``q_idx``, ``k_idx``: Scalar tensors indicating</span>
<span class="sd">          the batch index, query head index, query index, and key/value index, respectively.</span>
<span class="sd">          These should have the ``torch.int`` data type and be located on the same device as the score tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        query (Tensor): Query tensor; shape :math:`(B, Hq, L, E)`.</span>
<span class="sd">        key (Tensor): Key tensor; shape :math:`(B, Hkv, S, E)`.</span>
<span class="sd">        value (Tensor): Value tensor; shape :math:`(B, Hkv, S, Ev)`.</span>
<span class="sd">        score_mod (Optional[Callable]): Function to modify attention scores. By default no score_mod is applied.</span>
<span class="sd">        block_mask (Optional[BlockMask]): BlockMask object that controls the blocksparsity pattern of the attention.</span>
<span class="sd">        scale (Optional[float]): Scaling factor applied prior to softmax. If none, the default value is set to :math:`\frac{1}{\sqrt{E}}`.</span>
<span class="sd">        enable_gqa (bool): If set to True, enables Grouped Query Attention (GQA) and broadcasts key/value heads to query heads.</span>
<span class="sd">        kernel_options (Optional[Dict[str, Any]]): Options to pass into the Triton kernels.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor): Attention output; shape :math:`(B, Hq, L, Ev)`.</span>

<span class="sd">    Shape legend:</span>
<span class="sd">        - :math:`N: \text{Batch size} ... : \text{Any number of other batch dimensions (optional)}`</span>
<span class="sd">        - :math:`S: \text{Source sequence length}`</span>
<span class="sd">        - :math:`L: \text{Target sequence length}`</span>
<span class="sd">        - :math:`E: \text{Embedding dimension of the query and key}`</span>
<span class="sd">        - :math:`Ev: \text{Embedding dimension of the value}`</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `torch.nn.attention.flex_attention` is a prototype feature in PyTorch.</span>
<span class="sd">        Please look forward to a more stable implementation in a future version of PyTorch.</span>
<span class="sd">        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Some basic input validation</span>
    <span class="n">_validate_sdpa_input</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">key</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">value</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;NYI: query, key, and value must be 4D tensors&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">32</span><span class="p">:</span>  <span class="c1"># use Attention Kernel</span>
        <span class="k">if</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">128</span> <span class="ow">and</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">128</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;NYI: S must be &lt;128 or a multiple of 128&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">128</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;NYI: L must be a multiple of 128&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">enable_gqa</span><span class="p">)</span> <span class="ow">and</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span> <span class="o">!=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expect query and key/value to have the same number of heads &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but got Hq=</span><span class="si">{</span><span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2"> and Hkv=</span><span class="si">{</span><span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Try setting enable_gqa=True for GQA.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">enable_gqa</span><span class="p">:</span>
        <span class="n">Hq</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">Hkv</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">Hq</span> <span class="o">%</span> <span class="n">Hkv</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expect number of query heads to be a multiple of kv heads for GQA &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but got Hq=</span><span class="si">{</span><span class="n">Hq</span><span class="si">}</span><span class="s2"> and Hkv=</span><span class="si">{</span><span class="n">Hkv</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">score_mod</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">score_mod</span> <span class="o">=</span> <span class="n">_identity</span>
    <span class="k">if</span> <span class="n">block_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">block_mask</span> <span class="o">=</span> <span class="n">_create_empty_block_mask</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">kernel_options</span> <span class="o">=</span> <span class="n">_apply_kernel_options</span><span class="p">(</span>
        <span class="n">query</span><span class="p">,</span>
        <span class="n">key</span><span class="p">,</span>
        <span class="n">value</span><span class="p">,</span>
        <span class="n">kernel_options</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_dynamo_compiling</span><span class="p">():</span>
        <span class="c1"># mark head_dim and number of heads to be static</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">]:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">mark_static</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">mark_static</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">flex_attention_hop</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">score_mod</span><span class="p">,</span> <span class="n">block_mask</span><span class="o">.</span><span class="n">as_tuple</span><span class="p">(),</span> <span class="n">scale</span><span class="p">,</span> <span class="n">kernel_options</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">is_dynamo_supported</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;flex_attention requires dynamo support&quot;</span><span class="p">)</span>

    <span class="c1"># Dynamo is expecting a callable with &quot;__code__&quot; attribute.</span>
    <span class="c1"># We cannot directly pass hop to it. So we wrap it in a dummy function.</span>
    <span class="k">def</span> <span class="nf">_flex_attention_hop_wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">flex_attention_hop</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">_set_compilation_env</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">disable_cache_limit</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">_temp_remove_pre_dispatch_torch_function_mode</span><span class="p">():</span>
                <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
                    <span class="n">_flex_attention_hop_wrapper</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)(</span>
                    <span class="n">query</span><span class="p">,</span>
                    <span class="n">key</span><span class="p">,</span>
                    <span class="n">value</span><span class="p">,</span>
                    <span class="n">score_mod</span><span class="p">,</span>
                    <span class="n">block_mask</span><span class="o">.</span><span class="n">as_tuple</span><span class="p">(),</span>
                    <span class="n">scale</span><span class="p">,</span>
                    <span class="n">kernel_options</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">out</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>